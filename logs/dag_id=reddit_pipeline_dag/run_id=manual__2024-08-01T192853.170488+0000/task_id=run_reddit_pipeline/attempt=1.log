[2024-08-01T19:28:53.958+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: reddit_pipeline_dag.run_reddit_pipeline manual__2024-08-01T19:28:53.170488+00:00 [queued]>
[2024-08-01T19:28:53.970+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: reddit_pipeline_dag.run_reddit_pipeline manual__2024-08-01T19:28:53.170488+00:00 [queued]>
[2024-08-01T19:28:53.971+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-08-01T19:28:53.988+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): run_reddit_pipeline> on 2024-08-01 19:28:53.170488+00:00
[2024-08-01T19:28:53.995+0000] {standard_task_runner.py:57} INFO - Started process 1248 to run task
[2024-08-01T19:28:54.000+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'reddit_pipeline_dag', 'run_reddit_pipeline', 'manual__2024-08-01T19:28:53.170488+00:00', '--job-id', '615', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmp4a735j76']
[2024-08-01T19:28:54.001+0000] {standard_task_runner.py:85} INFO - Job 615: Subtask run_reddit_pipeline
[2024-08-01T19:28:54.058+0000] {task_command.py:416} INFO - Running <TaskInstance: reddit_pipeline_dag.run_reddit_pipeline manual__2024-08-01T19:28:53.170488+00:00 [running]> on host 8eab068d030e
[2024-08-01T19:28:54.154+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='reddit_pipeline_dag' AIRFLOW_CTX_TASK_ID='run_reddit_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2024-08-01T19:28:53.170488+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-08-01T19:28:53.170488+00:00'
[2024-08-01T19:28:56.641+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_cpgujsw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Sr. Data Engineer vs excel guy', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 112, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehlebx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'ups': 671, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 671, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/gsQD1-dl33Q3VrpjrUDVQwJoL4sCsaJVW2Ohf3abjCw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722530678.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/0q52otpzz2gd1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?auto=webp&s=78052c43548c654fc81a9adb77f63dd0f9d6899c', 'width': 1280, 'height': 1032}, 'resolutions': [{'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=108&crop=smart&auto=webp&s=e286ad6e83cae85030f5266b568978f00a37c1d8', 'width': 108, 'height': 87}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=216&crop=smart&auto=webp&s=0f06013ac76c092fee7192f28158ac005fba9b92', 'width': 216, 'height': 174}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=320&crop=smart&auto=webp&s=13ffccdb8e675c8ab83561abf96e82e0b55315ec', 'width': 320, 'height': 258}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=640&crop=smart&auto=webp&s=29fd194d4ce3861cb0a9b3fdbbdcf12ae43f6c00', 'width': 640, 'height': 516}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=960&crop=smart&auto=webp&s=f4bf838cca30717c170959578e820570bb7a225e', 'width': 960, 'height': 774}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=1080&crop=smart&auto=webp&s=434466fba7a0d8922e5e504acd8145fcafeb11b0', 'width': 1080, 'height': 870}], 'variants': {}, 'id': '_JRN-jGXF5utzwuwX68y-x_cy0ojwr6YdOqWfIWbHRk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1ehlebx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nobilix'), 'discussion_type': None, 'num_comments': 31, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehlebx/sr_data_engineer_vs_excel_guy/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/0q52otpzz2gd1.png', 'subreddit_subscribers': 201108, 'created_utc': 1722530678.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.642+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm experiencing difficulties finding work as a DE. I thought I have a good shot at getting at least some calls, but I've quite literally gotten 0 in over 100 applications. I'm fairly experienced in Python, SQL, PySpark, Tableau, Airflow, and data modeling. I've done work critical to building and supporting multi million dollar operations at scale. From what I see, with regards to technical skills I'm missing dbt and I'm lacking system design experience.\n\nThis is moreso directed to seniors and hiring managers - what do you look for in applicants?\n\nEdit: looking for senior DE roles with 8 YoE as an analyst/DE", 'author_fullname': 't2_14fnbt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What separates the average DE from a desirable DE in this market?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh05ya', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 100, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 100, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722473017.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722464231.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m experiencing difficulties finding work as a DE. I thought I have a good shot at getting at least some calls, but I&#39;ve quite literally gotten 0 in over 100 applications. I&#39;m fairly experienced in Python, SQL, PySpark, Tableau, Airflow, and data modeling. I&#39;ve done work critical to building and supporting multi million dollar operations at scale. From what I see, with regards to technical skills I&#39;m missing dbt and I&#39;m lacking system design experience.</p>\n\n<p>This is moreso directed to seniors and hiring managers - what do you look for in applicants?</p>\n\n<p>Edit: looking for senior DE roles with 8 YoE as an analyst/DE</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1eh05ya', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='git0ffmylawnm8'), 'discussion_type': None, 'num_comments': 51, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh05ya/what_separates_the_average_de_from_a_desirable_de/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh05ya/what_separates_the_average_de_from_a_desirable_de/', 'subreddit_subscribers': 201108, 'created_utc': 1722464231.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.644+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone. Currently, I am facing some difficulties in choosing a database. I work at a small company, and we have a project to create a database where molecular biologists can upload data and query other users' data. Due to the nature of molecular biology data, we need a high write throughput (each upload contains about 4 million rows). Therefore, we chose Cassandra because of its fast write speed (tested on our server at 10 million rows / 140s).\n\nHowever, the current issue is that Cassandra does not have an open-source solution for exporting an API for the frontend to query. If we have to code the backend REST API ourselves, it will be very tiring and time-consuming. I am looking for another database that can do this. I am considering HBase as an alternative solution. Is it really stable? Is there any combo like Directus + Postgres? Please give me your opinions.", 'author_fullname': 't2_wdxjgcmct', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Which database should I choose for a large database?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh7aux', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 46, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 46, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722484782.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone. Currently, I am facing some difficulties in choosing a database. I work at a small company, and we have a project to create a database where molecular biologists can upload data and query other users&#39; data. Due to the nature of molecular biology data, we need a high write throughput (each upload contains about 4 million rows). Therefore, we chose Cassandra because of its fast write speed (tested on our server at 10 million rows / 140s).</p>\n\n<p>However, the current issue is that Cassandra does not have an open-source solution for exporting an API for the frontend to query. If we have to code the backend REST API ourselves, it will be very tiring and time-consuming. I am looking for another database that can do this. I am considering HBase as an alternative solution. Is it really stable? Is there any combo like Directus + Postgres? Please give me your opinions.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eh7aux', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Practical_Slip6791'), 'discussion_type': None, 'num_comments': 53, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh7aux/which_database_should_i_choose_for_a_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh7aux/which_database_should_i_choose_for_a_large/', 'subreddit_subscribers': 201108, 'created_utc': 1722484782.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.644+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_uf0vdzs2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is this book worth reading ? pls give some reviews if anyone of you has read it', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehcd6q', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'ups': 33, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 33, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/5tKAaAqTzscuVmSjd5AfoUEgpmwP-6kx7INs3RhX9-g.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722504485.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/yy1fq9i2u0gd1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?auto=webp&s=39f9efa35d898b970780f80dd027f791f156db2c', 'width': 400, 'height': 525}, 'resolutions': [{'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?width=108&crop=smart&auto=webp&s=c54ed9477564b6b2e27a487358db7e29798fd477', 'width': 108, 'height': 141}, {'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?width=216&crop=smart&auto=webp&s=84cfabe3a6dc19440f6e38c8d24fd1aa3887da0b', 'width': 216, 'height': 283}, {'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?width=320&crop=smart&auto=webp&s=1d9fac121b7d3701f58816d38310320d9f78bf9f', 'width': 320, 'height': 420}], 'variants': {}, 'id': 'tJVjpuGR-P7USSNJa0AIrWRXK9CsJSxZpFgLD63emDs'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehcd6q', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='OpenWeb5282'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehcd6q/is_this_book_worth_reading_pls_give_some_reviews/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/yy1fq9i2u0gd1.jpeg', 'subreddit_subscribers': 201108, 'created_utc': 1722504485.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.645+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone, i have a question about what is the best way to load data from S3 to snowflake and truncate the data after the ingestion is successful.. \nIt will be a continues job runs every 4 hours and data is staged to s3 using lambda function \n\nI don't want to use snowpipe because it's not truncating the data so what is the best approach? ", 'author_fullname': 't2_dm09aec1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'S3 to snowflake', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egxl1n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 22, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 22, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722457932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone, i have a question about what is the best way to load data from S3 to snowflake and truncate the data after the ingestion is successful.. \nIt will be a continues job runs every 4 hours and data is staged to s3 using lambda function </p>\n\n<p>I don&#39;t want to use snowpipe because it&#39;s not truncating the data so what is the best approach? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egxl1n', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mountain-Luck7673'), 'discussion_type': None, 'num_comments': 20, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egxl1n/s3_to_snowflake/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egxl1n/s3_to_snowflake/', 'subreddit_subscribers': 201108, 'created_utc': 1722457932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.645+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'll start: Blob", 'author_fullname': 't2_xwr1wuedn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What are data engineering tools/systems that have a funny name?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh4dyn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722475832.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ll start: Blob</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1eh4dyn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='inner-musician-5457'), 'discussion_type': None, 'num_comments': 33, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh4dyn/what_are_data_engineering_toolssystems_that_have/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh4dyn/what_are_data_engineering_toolssystems_that_have/', 'subreddit_subscribers': 201108, 'created_utc': 1722475832.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.646+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Is it common for people to use it in production? I’ve seen most people mention it for local processing but I think it would be interesting to set it up in a server but I’m not sure if this a normal thing people do. \nIf you happen to do so, please enlighten me', 'author_fullname': 't2_6b3yqctf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DuckDB in production ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh4hrh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722476145.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Is it common for people to use it in production? I’ve seen most people mention it for local processing but I think it would be interesting to set it up in a server but I’m not sure if this a normal thing people do. \nIf you happen to do so, please enlighten me</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eh4hrh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Snoo_70708'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh4hrh/duckdb_in_production/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh4hrh/duckdb_in_production/', 'subreddit_subscribers': 201108, 'created_utc': 1722476145.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.646+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'How many stages were involved in the process? What sort of questions were you asked?', 'author_fullname': 't2_2zfz8duf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "What is the easiest experience you've had applying for a data engineer job?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehhr08', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 16, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722521674.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>How many stages were involved in the process? What sort of questions were you asked?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehhr08', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Novel-Tree7557'), 'discussion_type': None, 'num_comments': 28, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehhr08/what_is_the_easiest_experience_youve_had_applying/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehhr08/what_is_the_easiest_experience_youve_had_applying/', 'subreddit_subscribers': 201108, 'created_utc': 1722521674.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.647+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am a junior big data engineer in Poland, in a few days i will be assigned to a project with the following stack (Nifi, Spark, Hive-impala, ozzie) but the nature of the project that is Nifi take the most weight of the project and there is a third party developing spark and i have concerns regarding that i might be tool oriented rather than developer, i need an advice that shall i try to join project that i develop spark myself or reviewing those third party scripts and optimize them would be enough. ', 'author_fullname': 't2_v0uh4ksy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is Nifi that Important ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehc2rt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722503274.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am a junior big data engineer in Poland, in a few days i will be assigned to a project with the following stack (Nifi, Spark, Hive-impala, ozzie) but the nature of the project that is Nifi take the most weight of the project and there is a third party developing spark and i have concerns regarding that i might be tool oriented rather than developer, i need an advice that shall i try to join project that i develop spark myself or reviewing those third party scripts and optimize them would be enough. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehc2rt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ancient_Quiet_790'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehc2rt/is_nifi_that_important/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehc2rt/is_nifi_that_important/', 'subreddit_subscribers': 201108, 'created_utc': 1722503274.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.647+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi, I'm starting to dip my toes into the data engineering world and have been working a little with Spark. I see that the polars library is quite popular for speed and had some questions. Is it a good idea to use Polars on Spark or will it interfere with Spark's parallelism? From what I've been reading Polars seems excellent for parrellelsing stuff on your machine, but I'm unclear how it works with the distributed side of Spark? Any insight on this and related issues would be super helpful.\n\nFor context, I'm working on a project where a bunch of parquet files are read into spark, the data then undergoes some cleaning up and transfroming before being merged into a bunch of delta tables.", 'author_fullname': 't2_3mfqrrfi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it a good idea to use Polars with Pyspark?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehbgfl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722500700.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, I&#39;m starting to dip my toes into the data engineering world and have been working a little with Spark. I see that the polars library is quite popular for speed and had some questions. Is it a good idea to use Polars on Spark or will it interfere with Spark&#39;s parallelism? From what I&#39;ve been reading Polars seems excellent for parrellelsing stuff on your machine, but I&#39;m unclear how it works with the distributed side of Spark? Any insight on this and related issues would be super helpful.</p>\n\n<p>For context, I&#39;m working on a project where a bunch of parquet files are read into spark, the data then undergoes some cleaning up and transfroming before being merged into a bunch of delta tables.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehbgfl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Schlooooompy'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehbgfl/is_it_a_good_idea_to_use_polars_with_pyspark/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehbgfl/is_it_a_good_idea_to_use_polars_with_pyspark/', 'subreddit_subscribers': 201108, 'created_utc': 1722500700.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.648+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_nwnnprhup', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Amazon’s Exabyte-Scale Migration from Apache Spark to Ray on Amazon EC2', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 69, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egxnsu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/aTbLuIA4dHP37zZy3uWN2Dm52xB4VI0JxXq85gYhWNg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722458117.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'aws.amazon.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?auto=webp&s=c091425bd8a7d95084f2541baf5f04b1dd8b9bf0', 'width': 1260, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=108&crop=smart&auto=webp&s=8a26f3d3fd2aeea9de73c0c7220835232d5976b6', 'width': 108, 'height': 53}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=216&crop=smart&auto=webp&s=e39b4aba108b51327eb053f885afe27cb111f431', 'width': 216, 'height': 107}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=320&crop=smart&auto=webp&s=56eb833818025a22de90be17152afee64113dc77', 'width': 320, 'height': 159}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=640&crop=smart&auto=webp&s=79bcfb494edffafb5688d0200121f9d16b447ecd', 'width': 640, 'height': 318}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=960&crop=smart&auto=webp&s=b66a98643aa46e923827085af5551a0998db1a91', 'width': 960, 'height': 478}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=1080&crop=smart&auto=webp&s=77f6646519ede21b9ebf076fe6dc25c82bbbe3b6', 'width': 1080, 'height': 538}], 'variants': {}, 'id': 'DYcK-LguRnr1f1_eAOPF3C2IotmgYet-W6HqkdAt5dU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1egxnsu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='saaggy_peneer'), 'discussion_type': None, 'num_comments': 2, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egxnsu/amazons_exabytescale_migration_from_apache_spark/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/', 'subreddit_subscribers': 201108, 'created_utc': 1722458117.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.648+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'From what little understanding I have around these two, I thought one of their big features was the elimination for needing to be a multi-cloud environment? For example, if a retail company has vendors using different clouds, a big reason you would pick Snowflake/Databricks was because it essentially "translated" the data you were receiving/sending out. I\'m likely misremembering this, but I could have sworn this gave them the ability to free themselves from needing multiple clouds to accommodate to their vendors...your help/insight is greatly appreciated.', 'author_fullname': 't2_3jryrs5n', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dumb question about Databricks/Snowflake', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egw5ns', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722454432.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>From what little understanding I have around these two, I thought one of their big features was the elimination for needing to be a multi-cloud environment? For example, if a retail company has vendors using different clouds, a big reason you would pick Snowflake/Databricks was because it essentially &quot;translated&quot; the data you were receiving/sending out. I&#39;m likely misremembering this, but I could have sworn this gave them the ability to free themselves from needing multiple clouds to accommodate to their vendors...your help/insight is greatly appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egw5ns', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pickmeup0103'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egw5ns/dumb_question_about_databrickssnowflake/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egw5ns/dumb_question_about_databrickssnowflake/', 'subreddit_subscribers': 201108, 'created_utc': 1722454432.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.649+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am currently working in data analytics and am trying to move towards the data engineering role. \n\nMy current skillset:\nI use SQL heavily at work and am starting to learn python. I also have a homelab where I run multiple services so I am a little familiar with Linux as well as networking. At least enough to figure things out with google searches (usually). We use snowflake at work.\n\nMy project idea:\nI would like to do a project where I gather my bank transactions from an API and move that data through some kind of pipeline to eventually analyze and create dashboards in power bi. (I have successfully been able to pull this data with teller.io in python). The data here is not important, I’m more asking about the pipeline related stuff.\n\nMy question/ requested advice:\nWith this project I want to try to maximize my learning of real world skills that a data engineer might use, so I don’t necessarily just want to drop the data in one database and call it good. It might not make sense logistically, but if it means I learn more skills I am happy pushing the data through multiple systems to get experience. For example I’ve thought about pushing the raw api response to an S3 bucket, grabbing it out of there and parsing it, then transforming and moving it to another DB for consumption. Or something like that. I just don’t know all the possibilities or what the best possibilities are for learning the craft. Anyone have any advice on what I might do to maximize my learning from this project as a semi-beginner?\n\nNotes:\nI understand bank transactions are sensitive data, I’m not going to be using my real bank. \n\nI’d like this to be cheap if possible, but if there’s a commonly used product in the field (like snowflake for example) I might be willing to spend some money if it’s reasonable\n\nApologies for my ignorance on the data engineering lingo if I misused any words and thanks for your time in reading this\n\n', 'author_fullname': 't2_t9cvlrkt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Learning Data Engineering (Project Advice)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh11bc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722466462.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am currently working in data analytics and am trying to move towards the data engineering role. </p>\n\n<p>My current skillset:\nI use SQL heavily at work and am starting to learn python. I also have a homelab where I run multiple services so I am a little familiar with Linux as well as networking. At least enough to figure things out with google searches (usually). We use snowflake at work.</p>\n\n<p>My project idea:\nI would like to do a project where I gather my bank transactions from an API and move that data through some kind of pipeline to eventually analyze and create dashboards in power bi. (I have successfully been able to pull this data with teller.io in python). The data here is not important, I’m more asking about the pipeline related stuff.</p>\n\n<p>My question/ requested advice:\nWith this project I want to try to maximize my learning of real world skills that a data engineer might use, so I don’t necessarily just want to drop the data in one database and call it good. It might not make sense logistically, but if it means I learn more skills I am happy pushing the data through multiple systems to get experience. For example I’ve thought about pushing the raw api response to an S3 bucket, grabbing it out of there and parsing it, then transforming and moving it to another DB for consumption. Or something like that. I just don’t know all the possibilities or what the best possibilities are for learning the craft. Anyone have any advice on what I might do to maximize my learning from this project as a semi-beginner?</p>\n\n<p>Notes:\nI understand bank transactions are sensitive data, I’m not going to be using my real bank. </p>\n\n<p>I’d like this to be cheap if possible, but if there’s a commonly used product in the field (like snowflake for example) I might be willing to spend some money if it’s reasonable</p>\n\n<p>Apologies for my ignorance on the data engineering lingo if I misused any words and thanks for your time in reading this</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eh11bc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Spiritual-Battle5723'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh11bc/learning_data_engineering_project_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh11bc/learning_data_engineering_project_advice/', 'subreddit_subscribers': 201108, 'created_utc': 1722466462.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.649+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello thanks for taking the time.\n\nI\'m a solo developer for an employer who runs a small agency-- basically him and his industry expertise along with me and one other employee. He\'s brought me on full time because he wants to build out some data services he can offer to his clients with hopes of being able to move away from the traditional agency people / project hand holding strategy into something more scalable-- basically selling curated datasets and expert analysis with some light consulting on the side.\n\nI\'m building everything in GCP and pulling from two data sources. Right now the pipeline is running on airflow pulling data from the first API, storing that raw JSON in GCS, as well as performing some transformations on the data to get it to fit our desired schema for BigQuery, saving the transformed JSON, and then running a batch load operation to put the data into BigQuery. Then a request is built off of that data for the second data source (think "top ten" or something like that), and the same process is undergone on the second data source.\n\nThe process so far is relatively slow, given the API rate limits but also some slowness in processing. I am wary of over-optimizing or over-engineering since I am a single engineer team, and we have other projects ongoing-- however I am curious if someone can answer as to whether it would make sense to perform the data transformations from the raw JSON to the formatted JSON using Spark. Partly this is because I\'m interested in learning Spark for its own sake (using the Python API), so there is that. I have of course consulted ChatGPT but it tends to just tell me some version of what I might want to hear i.e. yes you could speed it up with Spark. But what I really want to ask from the human developer perspective is does it make any sense to do it this way or is this over-engineering or inelegant? I\'m ok making a bit of an effort to learn the new skill and jerry-rig it to work with my pipeline, like I said I do selfishly just want to apply the tool so that i can put it on my cv, but is this even a useful application of Spark or would I be better of just learning it on some kind of hobby project?\n\nEDIT: Reading the replies and reflecting, sounds like the answer is "no", Spark doesn\'t really make sense here. The main bottleneck is definitely all of the network-bound operations, and looking back the transformations I\'m making are minimal, and I\'ve optimized the code pretty much as well as I can using Python (list comprehension :\'-D). To be honest, the best way to speed this up will really just be to increase the worker count in Cloud Composer, still a long way for me to go there. I will however consider using an ELT approach next time for this kind of thing, as that was definitely where my mind was going with thinking of introducing Spark. Ultimately I\'m still thinking about a way to gain some experience using Spark without myself having to pay to host PB of data! I was hoping I could make it work for this but I\'m sure i\'ll find some opportunity. Thanks for the feedback everyone.', 'author_fullname': 't2_r0324by', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Will Spark speed up my pipeline, Airflow, BigQuery', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehlg5u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722538452.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722530795.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello thanks for taking the time.</p>\n\n<p>I&#39;m a solo developer for an employer who runs a small agency-- basically him and his industry expertise along with me and one other employee. He&#39;s brought me on full time because he wants to build out some data services he can offer to his clients with hopes of being able to move away from the traditional agency people / project hand holding strategy into something more scalable-- basically selling curated datasets and expert analysis with some light consulting on the side.</p>\n\n<p>I&#39;m building everything in GCP and pulling from two data sources. Right now the pipeline is running on airflow pulling data from the first API, storing that raw JSON in GCS, as well as performing some transformations on the data to get it to fit our desired schema for BigQuery, saving the transformed JSON, and then running a batch load operation to put the data into BigQuery. Then a request is built off of that data for the second data source (think &quot;top ten&quot; or something like that), and the same process is undergone on the second data source.</p>\n\n<p>The process so far is relatively slow, given the API rate limits but also some slowness in processing. I am wary of over-optimizing or over-engineering since I am a single engineer team, and we have other projects ongoing-- however I am curious if someone can answer as to whether it would make sense to perform the data transformations from the raw JSON to the formatted JSON using Spark. Partly this is because I&#39;m interested in learning Spark for its own sake (using the Python API), so there is that. I have of course consulted ChatGPT but it tends to just tell me some version of what I might want to hear i.e. yes you could speed it up with Spark. But what I really want to ask from the human developer perspective is does it make any sense to do it this way or is this over-engineering or inelegant? I&#39;m ok making a bit of an effort to learn the new skill and jerry-rig it to work with my pipeline, like I said I do selfishly just want to apply the tool so that i can put it on my cv, but is this even a useful application of Spark or would I be better of just learning it on some kind of hobby project?</p>\n\n<p>EDIT: Reading the replies and reflecting, sounds like the answer is &quot;no&quot;, Spark doesn&#39;t really make sense here. The main bottleneck is definitely all of the network-bound operations, and looking back the transformations I&#39;m making are minimal, and I&#39;ve optimized the code pretty much as well as I can using Python (list comprehension :&#39;-D). To be honest, the best way to speed this up will really just be to increase the worker count in Cloud Composer, still a long way for me to go there. I will however consider using an ELT approach next time for this kind of thing, as that was definitely where my mind was going with thinking of introducing Spark. Ultimately I&#39;m still thinking about a way to gain some experience using Spark without myself having to pay to host PB of data! I was hoping I could make it work for this but I&#39;m sure i&#39;ll find some opportunity. Thanks for the feedback everyone.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ehlg5u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nem03'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehlg5u/will_spark_speed_up_my_pipeline_airflow_bigquery/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehlg5u/will_spark_speed_up_my_pipeline_airflow_bigquery/', 'subreddit_subscribers': 201108, 'created_utc': 1722530795.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.650+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)', 'author_fullname': 't2_6l4z3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Monthly General Discussion - Aug 2024', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehkasi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722528031.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.</p>\n\n<p>Examples:</p>\n\n<ul>\n<li>What are you working on this month?</li>\n<li>What was something you accomplished?</li>\n<li>What was something you learned recently?</li>\n<li>What is something frustrating you currently?</li>\n</ul>\n\n<p>As always, sub rules apply. Please be respectful and stay curious.</p>\n\n<p><strong>Community Links:</strong></p>\n\n<ul>\n<li><a href="https://dataengineeringcommunity.substack.com/">Monthly newsletter</a></li>\n<li><a href="https://dataengineering.wiki/Community/Events">Data Engineering Events</a></li>\n<li><a href="https://dataengineering.wiki/Community/Meetups">Data Engineering Meetups</a></li>\n<li><a href="https://dataengineering.wiki/Community/Get+Involved">Get involved in the community</a></li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?auto=webp&s=38495eca7177fe7952a28f7cb7ec78b3735d7ce7', 'width': 920, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=108&crop=smart&auto=webp&s=7182d4e8f2ab9018d01089cac5deea3bb6adee65', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=216&crop=smart&auto=webp&s=3b48371d41aeeaed44e3c9bd50975ea3612d6e21', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=320&crop=smart&auto=webp&s=01226e541c733f4f617393c51347c7276a330624', 'width': 320, 'height': 166}, {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=640&crop=smart&auto=webp&s=bdc1a1eebb35c952b90cf0f903aa3c71ca25c607', 'width': 640, 'height': 333}], 'variants': {}, 'id': 'xl1dl_PUt0lnmjNhPzrvZZDAwfJuW9dYKPmR729jqfA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehkasi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AutoModerator'), 'discussion_type': None, 'num_comments': 3, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehkasi/monthly_general_discussion_aug_2024/', 'parent_whitelist_status': 'all_ads', 'stickied': True, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehkasi/monthly_general_discussion_aug_2024/', 'subreddit_subscribers': 201108, 'created_utc': 1722528031.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.650+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Greetings from the data science subreddit. What are some good resources to help a data scientist learn best practices for database design and schemas? I don’t necessarily need an absolutely beginners guide, but something I can reference so I don’t make any glaring mistakes while developing a *** db for my team to use.\n', 'author_fullname': 't2_1nhqot00', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data engineering primer for a data scientist', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egzott', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722463078.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Greetings from the data science subreddit. What are some good resources to help a data scientist learn best practices for database design and schemas? I don’t necessarily need an absolutely beginners guide, but something I can reference so I don’t make any glaring mistakes while developing a *** db for my team to use.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1egzott', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AngryDuckling1'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egzott/data_engineering_primer_for_a_data_scientist/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egzott/data_engineering_primer_for_a_data_scientist/', 'subreddit_subscribers': 201108, 'created_utc': 1722463078.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.651+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have a client that has reports on Google Analytics & Google Ads that wants me to set up a datapipe to a s3 bucket to use to create dashboards with Tableau. The reason they want to export the data is because they have several metrics & KPIs that are coming from other sources that they want to integrate the data from Google with. \n\nI suggested that they create a Google Cloud account to uptilize the API services, but they don’t want to do that. I’ve been researching deeply into the Google documentation, but can’t find a solution. \n\nIf anyone has built a similar pipeline let me know please.', 'author_fullname': 't2_jg7lw4rm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it possible to automate exporting raw data from Google Analytics without using an API call?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 61, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ehnq6m', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/qh0ysEiOE9gSbtMlwdqtDHVXPJEKNWsv2KLU9vfEij4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722536297.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a client that has reports on Google Analytics &amp; Google Ads that wants me to set up a datapipe to a s3 bucket to use to create dashboards with Tableau. The reason they want to export the data is because they have several metrics &amp; KPIs that are coming from other sources that they want to integrate the data from Google with. </p>\n\n<p>I suggested that they create a Google Cloud account to uptilize the API services, but they don’t want to do that. I’ve been researching deeply into the Google documentation, but can’t find a solution. </p>\n\n<p>If anyone has built a similar pipeline let me know please.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/1p8mlmhsg3gd1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?auto=webp&s=ba697843f672eb5631e007a121e77b8bbe46285a', 'width': 2577, 'height': 1125}, 'resolutions': [{'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?width=108&crop=smart&auto=webp&s=66dd7b753be8401972c88299d50b8e89f2cdfd13', 'width': 108, 'height': 47}, {'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?width=216&crop=smart&auto=webp&s=088492583f23a66515fc1ca2e1b213a62f2cd58b', 'width': 216, 'height': 94}, {'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?width=320&crop=smart&auto=webp&s=93e6c54ffd06ad24cdbba343b34807a4be84169e', 'width': 320, 'height': 139}, {'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?width=640&crop=smart&auto=webp&s=0c038f42d2de8ed0e2ea5c67b6a51a36b816e904', 'width': 640, 'height': 279}, {'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?width=960&crop=smart&auto=webp&s=c28febf0c206560f36464f63e7f3aa7776ce002e', 'width': 960, 'height': 419}, {'url': 'https://preview.redd.it/1p8mlmhsg3gd1.jpeg?width=1080&crop=smart&auto=webp&s=dad9bda0fb0fc894a014db93e7e92a3fb8ffa598', 'width': 1080, 'height': 471}], 'variants': {}, 'id': 'u3F-Z6Y8Np_KEsh1DOwaJLeUyzo1A53dJPwFQn-Hjwk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ehnq6m', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Quick123Fox'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehnq6m/is_it_possible_to_automate_exporting_raw_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/1p8mlmhsg3gd1.jpeg', 'subreddit_subscribers': 201108, 'created_utc': 1722536297.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.652+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I hope everyone using cdc for incremental load has come up with the problem of binlog file expiration. What is the best way to handle these kind of expiration so that the pipeline extracts the data actually from where it has left. \n\nI guess this problem is more based on the database side rather than on the extractor side. ', 'author_fullname': 't2_aenh4mw5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Pipeline Failure due to bin-log expiry', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehcxt6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722506760.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I hope everyone using cdc for incremental load has come up with the problem of binlog file expiration. What is the best way to handle these kind of expiration so that the pipeline extracts the data actually from where it has left. </p>\n\n<p>I guess this problem is more based on the database side rather than on the extractor side. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehcxt6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Suspicious_Peanut282'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehcxt6/pipeline_failure_due_to_binlog_expiry/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehcxt6/pipeline_failure_due_to_binlog_expiry/', 'subreddit_subscribers': 201108, 'created_utc': 1722506760.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.652+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'What is considered a good practice for distributed streaming/processing?\n\nLike is it okay to rely on cloud services like MSK, EMR or Kinesis to manage/run jobs? Or is there any other alternatives? Which apache platform should one use?\n\nAny sharing/tips would be very helpful🙏🏻🙏🏻', 'author_fullname': 't2_fta9agm0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Industry practice for distributed streaming/processing?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehawol', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722498401.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>What is considered a good practice for distributed streaming/processing?</p>\n\n<p>Like is it okay to rely on cloud services like MSK, EMR or Kinesis to manage/run jobs? Or is there any other alternatives? Which apache platform should one use?</p>\n\n<p>Any sharing/tips would be very helpful🙏🏻🙏🏻</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehawol', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='luqmancrit69'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehawol/industry_practice_for_distributed/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehawol/industry_practice_for_distributed/', 'subreddit_subscribers': 201108, 'created_utc': 1722498401.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.652+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi guys.\n\nI'm currently deploying the Clickhouse on kubernetes.\n\nAs you might know, there are several way to deploy Clickhouse on kubernetes such as bitnami helm chart or Altinity clickhouse operator.\n\n  \nI just want to know which method is more popular and convenient way to deploy Clickhouse on kubernetes.\n\nIf is there any one who has those experiences, please let me know.", 'author_fullname': 't2_tjyzhoaio', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Clickhouse on kubernetes', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh5fox', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.68, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722478884.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys.</p>\n\n<p>I&#39;m currently deploying the Clickhouse on kubernetes.</p>\n\n<p>As you might know, there are several way to deploy Clickhouse on kubernetes such as bitnami helm chart or Altinity clickhouse operator.</p>\n\n<p>I just want to know which method is more popular and convenient way to deploy Clickhouse on kubernetes.</p>\n\n<p>If is there any one who has those experiences, please let me know.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eh5fox', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='PrimaryConsistent262'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh5fox/clickhouse_on_kubernetes/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh5fox/clickhouse_on_kubernetes/', 'subreddit_subscribers': 201108, 'created_utc': 1722478884.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.653+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello,\n\nI will be working on a project soon for a marketing agency that generates leads for local businesses and tracks the leads that convert in an Excel sheet. Each business has its own Excel sheet, which they manually update when a customer converts.\n\nThe agency has two issues: \n\n1. Because the leads are updated manually, there are often errors due to minor changes made to the sheet by the companies.\n\n2. The agency wants to scale and will start adding more companies, so this manual process might not be sustainable.\n\nHere's my proposed solution:\n\n1. Create a web application using Flask that will allow the companies to update the database with just a few clicks when clients convert. This will limit the manipulation of the Excel file and reduce manual entry errors.\n\n2. Move from Google Sheets to a more robust database system like MySQL or PostgreSQL with multiple tables to support scalability.\n\nI am looking for more ideas on how to deal with this.\n\nThank you!", 'author_fullname': 't2_a9lj3zfh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Ideas for this project ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh3rmn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722474046.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,</p>\n\n<p>I will be working on a project soon for a marketing agency that generates leads for local businesses and tracks the leads that convert in an Excel sheet. Each business has its own Excel sheet, which they manually update when a customer converts.</p>\n\n<p>The agency has two issues: </p>\n\n<ol>\n<li><p>Because the leads are updated manually, there are often errors due to minor changes made to the sheet by the companies.</p></li>\n<li><p>The agency wants to scale and will start adding more companies, so this manual process might not be sustainable.</p></li>\n</ol>\n\n<p>Here&#39;s my proposed solution:</p>\n\n<ol>\n<li><p>Create a web application using Flask that will allow the companies to update the database with just a few clicks when clients convert. This will limit the manipulation of the Excel file and reduce manual entry errors.</p></li>\n<li><p>Move from Google Sheets to a more robust database system like MySQL or PostgreSQL with multiple tables to support scalability.</p></li>\n</ol>\n\n<p>I am looking for more ideas on how to deal with this.</p>\n\n<p>Thank you!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eh3rmn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Key_Perspective6112'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh3rmn/ideas_for_this_project/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh3rmn/ideas_for_this_project/', 'subreddit_subscribers': 201108, 'created_utc': 1722474046.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.653+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'When DuckDB is running in a resource limited environment such as AWS Lambda function or kubernetes pods with resource limits. What are the tools and ways to: \n1. Profile and optimize resource usage, specially memory.\n2. Limit resource usage by DuckDB.\n\nAnd lastly, what are some tools or ways DuckDB provides to look into query plans and optimize queries?', 'author_fullname': 't2_15f5dlzj1s', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to profile and resource limit duckdb', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh1gkr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722467584.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>When DuckDB is running in a resource limited environment such as AWS Lambda function or kubernetes pods with resource limits. What are the tools and ways to: \n1. Profile and optimize resource usage, specially memory.\n2. Limit resource usage by DuckDB.</p>\n\n<p>And lastly, what are some tools or ways DuckDB provides to look into query plans and optimize queries?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eh1gkr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SAsad01'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh1gkr/how_to_profile_and_resource_limit_duckdb/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh1gkr/how_to_profile_and_resource_limit_duckdb/', 'subreddit_subscribers': 201108, 'created_utc': 1722467584.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.654+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\nI joined a new company as a data engineer (with \\~5 years of experience in Data & Software eng.) 2 weeks ago and was immediately introduced to [AWS SDLF v1.x](https://github.com/awslabs/aws-serverless-data-lake-framework) as the central data lake solution (managed an internal cloud team).  Migration to v2 going to happen until start of Q4 this year. Seems like I will be working with it regularly from now on and write pipelines using Glue / Lambda. \n\nCurrently I am trying to understand the core principles and the way it works under the hood. Compared to previous architectures , this AWS serverless service mess is way more complex and very fragmented. \n\nHave you worked with SDLF before and what were the main pain points and pitfalls to avoid ? Any tips ? Any positive experiences? Cant find any useful reports about this framework so I am using my chance to ask you here.', 'author_fullname': 't2_tidh3aji8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone experience with AWS SDLF ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ehnlry', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722536004.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>I joined a new company as a data engineer (with ~5 years of experience in Data &amp; Software eng.) 2 weeks ago and was immediately introduced to <a href="https://github.com/awslabs/aws-serverless-data-lake-framework">AWS SDLF v1.x</a> as the central data lake solution (managed an internal cloud team).  Migration to v2 going to happen until start of Q4 this year. Seems like I will be working with it regularly from now on and write pipelines using Glue / Lambda. </p>\n\n<p>Currently I am trying to understand the core principles and the way it works under the hood. Compared to previous architectures , this AWS serverless service mess is way more complex and very fragmented. </p>\n\n<p>Have you worked with SDLF before and what were the main pain points and pitfalls to avoid ? Any tips ? Any positive experiences? Cant find any useful reports about this framework so I am using my chance to ask you here.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/sNiNgmZYLrPKuFvIstV4Wa1-Zs2IrSNl0Gp3axAFPkM.jpg?auto=webp&s=d33bc51c4073f055dee76081bf5b44bc9b5e4cb0', 'width': 271, 'height': 231}, 'resolutions': [{'url': 'https://external-preview.redd.it/sNiNgmZYLrPKuFvIstV4Wa1-Zs2IrSNl0Gp3axAFPkM.jpg?width=108&crop=smart&auto=webp&s=b24339707a9b31fc87a1e7b4b9d9bae9d0d9b1fa', 'width': 108, 'height': 92}, {'url': 'https://external-preview.redd.it/sNiNgmZYLrPKuFvIstV4Wa1-Zs2IrSNl0Gp3axAFPkM.jpg?width=216&crop=smart&auto=webp&s=42a6385108d310e47e470158e451283c179a8f77', 'width': 216, 'height': 184}], 'variants': {}, 'id': 'zLbuJh-0ClB2AfFKu1ga5zVcLtehBAKH287694Eb62Q'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehnlry', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LokeReven'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehnlry/anyone_experience_with_aws_sdlf/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehnlry/anyone_experience_with_aws_sdlf/', 'subreddit_subscribers': 201108, 'created_utc': 1722536004.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.654+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'It seems like the data system as a whole is mostly maintained by Epic. If anybody is currently working in the data engineer space in a healthcare company, how has your experience been? Please mention the name of the company too if possible. (Can be past or present )', 'author_fullname': 't2_ryxoz6of4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there growth and learning for data engineers in hospital/healthcare companies that use mainly Epic db/dw? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ehn5o1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722534933.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>It seems like the data system as a whole is mostly maintained by Epic. If anybody is currently working in the data engineer space in a healthcare company, how has your experience been? Please mention the name of the company too if possible. (Can be past or present )</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehn5o1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='UnfairDiscount8331'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehn5o1/is_there_growth_and_learning_for_data_engineers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehn5o1/is_there_growth_and_learning_for_data_engineers/', 'subreddit_subscribers': 201108, 'created_utc': 1722534933.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.655+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "In your workplace, what's more valued? Asking both the analytics and DE subreddit.\n\nI work in hospital finance and accounting. We have a dedicated database team, we use mostly Python, SQL, and VBA. I'm easily the most skilled in programming, but I have almost 0 SME compared to my coworkers who have been here much longer. Because of this, I'm unable to contribute to high impact projects, instead I work on automation and troubleshooting our pipelines/scripts (which still requires some degreee of SME to understand the issue). \n\nMy manager knows this and he's good with answering any questions I have, so I'm hopeful about developing my SME in the relativey niche field of hospital finance for years to come.\n\nHow will this impact a potential job hunt down the line if I want to stay in hospital finance? We don't use modern cloud or ETL solutions, but rather we use Windows task scheduler, batch scripts, and homegrown ETL with Python, SQL, and VBA. ", 'author_fullname': 't2_tt7gml0lp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Technical vs Subject Matter skills', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehmfq7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722533184.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In your workplace, what&#39;s more valued? Asking both the analytics and DE subreddit.</p>\n\n<p>I work in hospital finance and accounting. We have a dedicated database team, we use mostly Python, SQL, and VBA. I&#39;m easily the most skilled in programming, but I have almost 0 SME compared to my coworkers who have been here much longer. Because of this, I&#39;m unable to contribute to high impact projects, instead I work on automation and troubleshooting our pipelines/scripts (which still requires some degreee of SME to understand the issue). </p>\n\n<p>My manager knows this and he&#39;s good with answering any questions I have, so I&#39;m hopeful about developing my SME in the relativey niche field of hospital finance for years to come.</p>\n\n<p>How will this impact a potential job hunt down the line if I want to stay in hospital finance? We don&#39;t use modern cloud or ETL solutions, but rather we use Windows task scheduler, batch scripts, and homegrown ETL with Python, SQL, and VBA. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ehmfq7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='date_uh'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehmfq7/technical_vs_subject_matter_skills/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehmfq7/technical_vs_subject_matter_skills/', 'subreddit_subscribers': 201108, 'created_utc': 1722533184.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.655+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_975og', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How Airbyte 1.0 orchestrates data movement jobs', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehhmzs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/TO3vcpMPfDwDUkc_2QHemVInqgzIGm0of54AYCEu-Sc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722521389.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'airbyte.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://airbyte.com/blog/introducing-workloads-how-airbyte-1-0-orchestrates-data-movement-jobs', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?auto=webp&s=730a7ec8de61b8d3c75922f081c47cec180dd7a9', 'width': 2400, 'height': 1256}, 'resolutions': [{'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?width=108&crop=smart&auto=webp&s=430101264a3c505b21fdedcd801b3900c77857d8', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?width=216&crop=smart&auto=webp&s=7504c8c2cbbb6d70c530b4db19b6a21119a5822b', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?width=320&crop=smart&auto=webp&s=3191cfc72b3aa01a4bf8f242e0a094b7db76a370', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?width=640&crop=smart&auto=webp&s=5d7a56b74afcf526aa62c5a1722fb9ee8da31b46', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?width=960&crop=smart&auto=webp&s=8ca9f9821c4ee00932cc97224c4e44551e0221d6', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/p863PabJAGumZJjcapigSVhK5HvD_tm0kXgAbyc3CV4.jpg?width=1080&crop=smart&auto=webp&s=ce4893d6c5134a226cf19a8b7c736cdc0e03740c', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'gy-h9gSyMpDSuh4jexeHFaX2ioukaoh8ZTVxPyfG4Go'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ehhmzs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='arimbr'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehhmzs/how_airbyte_10_orchestrates_data_movement_jobs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://airbyte.com/blog/introducing-workloads-how-airbyte-1-0-orchestrates-data-movement-jobs', 'subreddit_subscribers': 201108, 'created_utc': 1722521389.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.656+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\n  \nSo ever since azure data factory introduced the new salesforce connector im having lots of troubling every day ingesting data from Salesforce into my adls. Sometimes its a connection issue, sometimes data is incorrect, sometimes it cannot retrieve an object.\n\n  \nIs anyone else experiencing this issue of has experienced it as well?\n\n  \nIf yes, what did you do to solve this?', 'author_fullname': 't2_2cd0q7u5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is anyone else having lots of troubles with the new salesforce connector in data factory?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehaxa6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722498468.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>So ever since azure data factory introduced the new salesforce connector im having lots of troubling every day ingesting data from Salesforce into my adls. Sometimes its a connection issue, sometimes data is incorrect, sometimes it cannot retrieve an object.</p>\n\n<p>Is anyone else experiencing this issue of has experienced it as well?</p>\n\n<p>If yes, what did you do to solve this?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ehaxa6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='kbic93'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehaxa6/is_anyone_else_having_lots_of_troubles_with_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehaxa6/is_anyone_else_having_lots_of_troubles_with_the/', 'subreddit_subscribers': 201108, 'created_utc': 1722498468.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.656+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey fellas, anyone doing this set up that could give me any lead? \n\nI was looking to have DMS dumping data into s3 then once data loaded in stage snowflake make the appropriate processing to handle duplicates and the logic that comes with the DMS record (the type of update), or is it better do it using a spark job in AWS. Do not have much idea how to handle schema changes, any help very much appreciated ', 'author_fullname': 't2_42yrzhea', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'RDS MySQL -> Snowflake CDC', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh8z70', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722490684.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey fellas, anyone doing this set up that could give me any lead? </p>\n\n<p>I was looking to have DMS dumping data into s3 then once data loaded in stage snowflake make the appropriate processing to handle duplicates and the logic that comes with the DMS record (the type of update), or is it better do it using a spark job in AWS. Do not have much idea how to handle schema changes, any help very much appreciated </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eh8z70', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='josejo9423'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh8z70/rds_mysql_snowflake_cdc/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh8z70/rds_mysql_snowflake_cdc/', 'subreddit_subscribers': 201108, 'created_utc': 1722490684.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.657+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone! Are there any tools available for extracting data from a large PDF (Over 900 Pages) and converting it into a CSV file? I am aware that there are numerous tools for this purpose, and it may seem simple. However, it is crucial that the extracted data maintains the CSI Master Format division structure to ensure compatibility with our existing data tables. This is the specific data in question.\xa0[RSMeans Building Construction Cost Data 2014 : Free Download, Borrow, and Streaming : Internet Archive](https://archive.org/details/RSMeansBuildingConstructionCostData2014/page/n29/mode/2up).', 'author_fullname': 't2_r1z02lhy9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'dumb question-Mining historical cost data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh0av7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722464587.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone! Are there any tools available for extracting data from a large PDF (Over 900 Pages) and converting it into a CSV file? I am aware that there are numerous tools for this purpose, and it may seem simple. However, it is crucial that the extracted data maintains the CSI Master Format division structure to ensure compatibility with our existing data tables. This is the specific data in question.\xa0<a href="https://archive.org/details/RSMeansBuildingConstructionCostData2014/page/n29/mode/2up">RSMeans Building Construction Cost Data 2014 : Free Download, Borrow, and Streaming : Internet Archive</a>.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/aiZIMDuaY-pjcaKP1hqV19qftZkvDYr4FFFM60g9bZY.jpg?auto=webp&s=7f985a641ffbef85b09c54296d4931bfea301a83', 'width': 180, 'height': 239}, 'resolutions': [{'url': 'https://external-preview.redd.it/aiZIMDuaY-pjcaKP1hqV19qftZkvDYr4FFFM60g9bZY.jpg?width=108&crop=smart&auto=webp&s=5057d2bce77f1677b497321eb36aebb2b7cb433d', 'width': 108, 'height': 143}], 'variants': {}, 'id': 'oj0MGuwlTROPvQJpV6nVoB91mBt6_G22HeCKFuyUzQk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eh0av7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SheepherderFamous510'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh0av7/dumb_questionmining_historical_cost_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh0av7/dumb_questionmining_historical_cost_data/', 'subreddit_subscribers': 201108, 'created_utc': 1722464587.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.657+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "How platform and security engineering teams can leverage data encryption to improve security standards, simplify infrastructure architecture, and enhance developer velocity. We created Keyper([https://jarrid.xyz/keyper](https://jarrid.xyz/keyper)) to make data encryption as simple as possible and we'd really love to learn about platform engineer's thoughts on this.\xa0\n\n[https://jarrid.xyz/articles/2024-07-30-simplify-infrastructure-with-security](https://jarrid.xyz/articles/2024-07-30-simplify-infrastructure-with-security)", 'author_fullname': 't2_v14p918y', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How Data Encryption Can Simplify Infrastructure Architecture', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egzdoy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722462307.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>How platform and security engineering teams can leverage data encryption to improve security standards, simplify infrastructure architecture, and enhance developer velocity. We created Keyper(<a href="https://jarrid.xyz/keyper">https://jarrid.xyz/keyper</a>) to make data encryption as simple as possible and we&#39;d really love to learn about platform engineer&#39;s thoughts on this.\xa0</p>\n\n<p><a href="https://jarrid.xyz/articles/2024-07-30-simplify-infrastructure-with-security">https://jarrid.xyz/articles/2024-07-30-simplify-infrastructure-with-security</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?auto=webp&s=86ae654160601c40351764d57c5b0a2ba5dc9e7c', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?width=108&crop=smart&auto=webp&s=873f35bf9472ca188183961245801ed0588ed441', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?width=216&crop=smart&auto=webp&s=f215e6d27ad64fcd81e161d59f4d2987ff78077d', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?width=320&crop=smart&auto=webp&s=80a7f02454265c59eb25e873e82566ed35ba3af9', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?width=640&crop=smart&auto=webp&s=2d685f47a8b89b685764c43d7a543b86de8a8e48', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?width=960&crop=smart&auto=webp&s=b114466df098cd3ea5f907f13bac0ccf0144370e', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/izMYfn4eX1y9WeA2M4IieLSYj1kAgKfhjR0lGZw_-Pw.jpg?width=1080&crop=smart&auto=webp&s=513afb35073e6c2843ce5d96362710cd738fc999', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'VbJM7NbFKKjFejgYoRVOdqF3gh7QdTC9ps9t-aBRY0w'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1egzdoy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='CharmingOwl4972'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egzdoy/how_data_encryption_can_simplify_infrastructure/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egzdoy/how_data_encryption_can_simplify_infrastructure/', 'subreddit_subscribers': 201108, 'created_utc': 1722462307.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.658+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My database is mysql and my application is a php laravel internal application (not consumer facing)\n\nI have a large database of components that are grouped by series.\n\nexamples are the AB series, the R2H series... Within each series, some components can be altered to become other components. This is governed by the part number.\n\nexample: There are part numbers in the AB series AB12.01-4HU AB22.01-4HU AB08.01-4HU AB12.01-2HF AB22.01-6TR AB08.01-4HL\n\nfor a given part, as long as this prefix is the same AB\\_\\_.\\_\\_, the letters on the end can transform. U can become F, R and L, but not the other way around. R can become L and L can become R I have this mapped in an array indexed on the position of the part number after the prefix:\n\n        'AB__.__-' = [\n            0 = [\n            2 = [2, 4],\n            4 = [4],\n            6 = [6]\n          ],\n          1 = [\n          'H' = ['H']\n          ],\n          2 = [\n            'U' = ['U', 'F', 'R', 'L'],\n            'F' = ['F'],\n            'R' = ['R', 'L'],\n            'L' = ['L', 'R']\n          ]\n        ]\n\nThe assembler I built takes components we have, grouped by prefix, and then iterates through each letter position and adds possible components to a buildable table.\n\nEvery day I run this assembler on the components present in the database, to build a list of the components currently buildable. This is computationally expensive and I wonder if there is a better way of doing things.\xa0**Also, there are some configurations which do not neatly fit into this system and would benefit from being able to manually add some configurations. Additionally, there are some components which require the presence of TWO or MORE base components, and this current setup doesn't allow for that. I have code written that does this but it's even worse.**\n\nI know that I could run all of these calculations just one time and store the possible combinations so that given a component I could retrieve all components buildable by that component, but I am unsure of the best table structure. any insight or advice would be helpful.\n\nA table structure I am thinking of could be: components table: id, part\\_number, series\n\nbuildable\\_components = base\\_component\\_id, buildable\\_component\\_id, build\\_type {'manual' || 'autobuilt'}\n\nand then if I make changes to the configuration I could run the builder one time to rebuild the database and leave the manual entries alone.\n\nThis doesn't solve the multiple base models needed issue though\n\nThank You", 'author_fullname': 't2_31lxam17', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How can I optimize the storage and retrieval of buildable component combinations in a large database?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egxazo', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722457247.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My database is mysql and my application is a php laravel internal application (not consumer facing)</p>\n\n<p>I have a large database of components that are grouped by series.</p>\n\n<p>examples are the AB series, the R2H series... Within each series, some components can be altered to become other components. This is governed by the part number.</p>\n\n<p>example: There are part numbers in the AB series AB12.01-4HU AB22.01-4HU AB08.01-4HU AB12.01-2HF AB22.01-6TR AB08.01-4HL</p>\n\n<p>for a given part, as long as this prefix is the same AB__.__, the letters on the end can transform. U can become F, R and L, but not the other way around. R can become L and L can become R I have this mapped in an array indexed on the position of the part number after the prefix:</p>\n\n<pre><code>    &#39;AB__.__-&#39; = [\n        0 = [\n        2 = [2, 4],\n        4 = [4],\n        6 = [6]\n      ],\n      1 = [\n      &#39;H&#39; = [&#39;H&#39;]\n      ],\n      2 = [\n        &#39;U&#39; = [&#39;U&#39;, &#39;F&#39;, &#39;R&#39;, &#39;L&#39;],\n        &#39;F&#39; = [&#39;F&#39;],\n        &#39;R&#39; = [&#39;R&#39;, &#39;L&#39;],\n        &#39;L&#39; = [&#39;L&#39;, &#39;R&#39;]\n      ]\n    ]\n</code></pre>\n\n<p>The assembler I built takes components we have, grouped by prefix, and then iterates through each letter position and adds possible components to a buildable table.</p>\n\n<p>Every day I run this assembler on the components present in the database, to build a list of the components currently buildable. This is computationally expensive and I wonder if there is a better way of doing things.\xa0<strong>Also, there are some configurations which do not neatly fit into this system and would benefit from being able to manually add some configurations. Additionally, there are some components which require the presence of TWO or MORE base components, and this current setup doesn&#39;t allow for that. I have code written that does this but it&#39;s even worse.</strong></p>\n\n<p>I know that I could run all of these calculations just one time and store the possible combinations so that given a component I could retrieve all components buildable by that component, but I am unsure of the best table structure. any insight or advice would be helpful.</p>\n\n<p>A table structure I am thinking of could be: components table: id, part_number, series</p>\n\n<p>buildable_components = base_component_id, buildable_component_id, build_type {&#39;manual&#39; || &#39;autobuilt&#39;}</p>\n\n<p>and then if I make changes to the configuration I could run the builder one time to rebuild the database and leave the manual entries alone.</p>\n\n<p>This doesn&#39;t solve the multiple base models needed issue though</p>\n\n<p>Thank You</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egxazo', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='shepshep7'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egxazo/how_can_i_optimize_the_storage_and_retrieval_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egxazo/how_can_i_optimize_the_storage_and_retrieval_of/', 'subreddit_subscribers': 201108, 'created_utc': 1722457247.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.658+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "`EcsRunLauncher` will spawn a separate fargate task per job. There are (i think) at-least 2 disadvantages to this: (1) Job start takes a really long time (as much as 2 mins for us compared to say docker runs locally which are instantaneous) (2) Sounds a bit expensive running new task with its own vcpu and memory allocation for every job-run.\n\nI was thinking of something along the lines of running all jobs in a single long-living task and use fargate policies to autoscale that depending on vcpu/memory metrics. I think if we use the `DefaultRunLauncher` instead of `EcsRunLauncher` then this is what would happen?\n\n2) If yes, then are the runs (instigated through `dagit-ui` or `sensors` or `schedules`) run in the same task as the usercode/`gRPC` task? The documentation here: [https://docs.dagster.io/deployment/run-launcher#](https://docs.dagster.io/deployment/run-launcher#) seems to suggest that:\n\n>The simplest run launcher is the built-in run launcher,\xa0[`DefaultRunLauncher`](https://docs.dagster.io/deployment/run-launcher#). This run launcher spawns a new process per run on the same node as the job's code location\n\n3) If yes, again is it OK to scale this task horizontally based on `Fargate` metrics for the service the task is a part of? Eg. I can put the service with `gRPC` task container (ie the container serving the user code repo to `dagit`) behind an `Application Load Balancer` and scale that service up or down depending on Fargate metrics.\n\nDoes all this sound correct and reasonable? Any alternatives (we don't want to use EC2s)", 'author_fullname': 't2_zgel5pd7k', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Alternate scaling strategy with Dagster Job runs on ECS/Fargate', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egx3tn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722456759.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><code>EcsRunLauncher</code> will spawn a separate fargate task per job. There are (i think) at-least 2 disadvantages to this: (1) Job start takes a really long time (as much as 2 mins for us compared to say docker runs locally which are instantaneous) (2) Sounds a bit expensive running new task with its own vcpu and memory allocation for every job-run.</p>\n\n<p>I was thinking of something along the lines of running all jobs in a single long-living task and use fargate policies to autoscale that depending on vcpu/memory metrics. I think if we use the <code>DefaultRunLauncher</code> instead of <code>EcsRunLauncher</code> then this is what would happen?</p>\n\n<p>2) If yes, then are the runs (instigated through <code>dagit-ui</code> or <code>sensors</code> or <code>schedules</code>) run in the same task as the usercode/<code>gRPC</code> task? The documentation here: <a href="https://docs.dagster.io/deployment/run-launcher#">https://docs.dagster.io/deployment/run-launcher#</a> seems to suggest that:</p>\n\n<blockquote>\n<p>The simplest run launcher is the built-in run launcher,\xa0<a href="https://docs.dagster.io/deployment/run-launcher#"><code>DefaultRunLauncher</code></a>. This run launcher spawns a new process per run on the same node as the job&#39;s code location</p>\n</blockquote>\n\n<p>3) If yes, again is it OK to scale this task horizontally based on <code>Fargate</code> metrics for the service the task is a part of? Eg. I can put the service with <code>gRPC</code> task container (ie the container serving the user code repo to <code>dagit</code>) behind an <code>Application Load Balancer</code> and scale that service up or down depending on Fargate metrics.</p>\n\n<p>Does all this sound correct and reasonable? Any alternatives (we don&#39;t want to use EC2s)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?auto=webp&s=d9eec3bfbfc3fd565643229f8a84c399bc1fe73b', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=108&crop=smart&auto=webp&s=499be57a5b257c149c9417e04fb72b227c3fc6bc', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=216&crop=smart&auto=webp&s=41c03559a31ecfba73ee7ff44bd6293216bcd5f6', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=320&crop=smart&auto=webp&s=f1f3696c3a4f7025280b0a98110eadfe130536c4', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=640&crop=smart&auto=webp&s=a12aeb324dd791c5dbd277a3131545efcfa5b3fe', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=960&crop=smart&auto=webp&s=bceae394da1ca70512dad8f5ec6ce1e142472e86', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=1080&crop=smart&auto=webp&s=29800a80de6287c32b91b05133f7c8e4a2d57eb2', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'gUU0nwKC8lE8qWjp6y6pjjOolTxbCTTaSrkKGe3Kq6A'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1egx3tn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dick-the-prick'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egx3tn/alternate_scaling_strategy_with_dagster_job_runs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egx3tn/alternate_scaling_strategy_with_dagster_job_runs/', 'subreddit_subscribers': 201108, 'created_utc': 1722456759.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.659+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi All,\n\nAfter graduating in May in Computer Science, I finally landed my first job as a Data Engineer I (I started focusing on DE my senior year)for a mid size tech company.\n\nI’m super excited, but also nervous of course. Does anyone have any advice on what to expect my first year of working? Any helpful tips you wish you had known when first joining? \n\nAppreciate any and all advice, thanks!', 'author_fullname': 't2_e3fhaijbf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What to Expect', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ehotxh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722538988.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi All,</p>\n\n<p>After graduating in May in Computer Science, I finally landed my first job as a Data Engineer I (I started focusing on DE my senior year)for a mid size tech company.</p>\n\n<p>I’m super excited, but also nervous of course. Does anyone have any advice on what to expect my first year of working? Any helpful tips you wish you had known when first joining? </p>\n\n<p>Appreciate any and all advice, thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ehotxh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Standard_Penalty5182'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehotxh/what_to_expect/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehotxh/what_to_expect/', 'subreddit_subscribers': 201108, 'created_utc': 1722538988.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.660+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bfd1877c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'How will you prefer to design the system  , given there are multiple dataset with 70 % overlap of the data coverage in facts and dimensions. The datasets are restricted at the hierarchy. But at some point dataset b will fade away, and only data set A will prevail as gold standards. \n\nFor now I am planning to have a mapping key for both systems based on attributes. [ this should take care of de duplication]\n\nThen when time comes I use the flag to delete or archieve the dataset B [ this to me is enterprise driven approach]\n\nApproach 2: I do a union and use a flag [ truncate  load], given that I will always get 48 months of data on monthly basis.', 'author_fullname': 't2_qinvsb2g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Multi Vendor dataset- single product', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ehnidp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722535770.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>How will you prefer to design the system  , given there are multiple dataset with 70 % overlap of the data coverage in facts and dimensions. The datasets are restricted at the hierarchy. But at some point dataset b will fade away, and only data set A will prevail as gold standards. </p>\n\n<p>For now I am planning to have a mapping key for both systems based on attributes. [ this should take care of de duplication]</p>\n\n<p>Then when time comes I use the flag to delete or archieve the dataset B [ this to me is enterprise driven approach]</p>\n\n<p>Approach 2: I do a union and use a flag [ truncate  load], given that I will always get 48 months of data on monthly basis.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehnidp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cida1205'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehnidp/multi_vendor_dataset_single_product/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehnidp/multi_vendor_dataset_single_product/', 'subreddit_subscribers': 201108, 'created_utc': 1722535770.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T19:28:56.661+0000] {python.py:194} INFO - Done. Returned value was: None
[2024-08-01T19:28:56.670+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=reddit_pipeline_dag, task_id=run_reddit_pipeline, execution_date=20240801T192853, start_date=20240801T192853, end_date=20240801T192856
[2024-08-01T19:28:56.707+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-08-01T19:28:56.730+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
