[2024-08-01T20:09:55.970+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline2.reddit_extraction manual__2024-08-01T20:09:54.918408+00:00 [queued]>
[2024-08-01T20:09:55.983+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline2.reddit_extraction manual__2024-08-01T20:09:54.918408+00:00 [queued]>
[2024-08-01T20:09:55.984+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-08-01T20:09:56.136+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-08-01 20:09:54.918408+00:00
[2024-08-01T20:09:56.143+0000] {standard_task_runner.py:57} INFO - Started process 1258 to run task
[2024-08-01T20:09:56.146+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline2', 'reddit_extraction', 'manual__2024-08-01T20:09:54.918408+00:00', '--job-id', '620', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpwznuf46y']
[2024-08-01T20:09:56.147+0000] {standard_task_runner.py:85} INFO - Job 620: Subtask reddit_extraction
[2024-08-01T20:09:56.322+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline2.reddit_extraction manual__2024-08-01T20:09:54.918408+00:00 [running]> on host 8eab068d030e
[2024-08-01T20:09:56.701+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline2' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-08-01T20:09:54.918408+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-08-01T20:09:54.918408+00:00'
[2024-08-01T20:09:56.704+0000] {logging_mixin.py:151} INFO - connected to reddit!
[2024-08-01T20:09:59.769+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_k0kr23gm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Describe your perfect date', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 102, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecrvy2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'ups': 823, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 823, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/lKikOtk6NoAolGbyxGDpAoRsu4vbHS1YrTwDxMI-r5E.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722010026.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/ckxrp0z9zved1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/ckxrp0z9zved1.png?auto=webp&s=2f04e37b4115a24ed303ba6052e1341c8dda78cd', 'width': 1920, 'height': 1400}, 'resolutions': [{'url': 'https://preview.redd.it/ckxrp0z9zved1.png?width=108&crop=smart&auto=webp&s=e3fc4b353b89664c88a6ce84ff8f626a4f5c2eb6', 'width': 108, 'height': 78}, {'url': 'https://preview.redd.it/ckxrp0z9zved1.png?width=216&crop=smart&auto=webp&s=ec4838f14fcb36b8e53d83c86088224e9bdcf4dc', 'width': 216, 'height': 157}, {'url': 'https://preview.redd.it/ckxrp0z9zved1.png?width=320&crop=smart&auto=webp&s=ed6a0e0b21f0d135891451e138f4c4d101951cb2', 'width': 320, 'height': 233}, {'url': 'https://preview.redd.it/ckxrp0z9zved1.png?width=640&crop=smart&auto=webp&s=8fce5aed6dfd2b6c84811f4eb3780d130fb4b0b1', 'width': 640, 'height': 466}, {'url': 'https://preview.redd.it/ckxrp0z9zved1.png?width=960&crop=smart&auto=webp&s=dc37e5201e77310fcd38b79524b1db56ac9987d3', 'width': 960, 'height': 700}, {'url': 'https://preview.redd.it/ckxrp0z9zved1.png?width=1080&crop=smart&auto=webp&s=e3d8a53b8250e7003f33044b9e4b4e83e6c3e568', 'width': 1080, 'height': 787}], 'variants': {}, 'id': 'axIooBPhYnqcCsOKDce9Hwde082UkIMTlu4QPX8IwfI'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1ecrvy2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SelectStarData'), 'discussion_type': None, 'num_comments': 56, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecrvy2/describe_your_perfect_date/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/ckxrp0z9zved1.png', 'subreddit_subscribers': 201126, 'created_utc': 1722010026.0, 'num_crossposts': 2, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.769+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_cpgujsw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Sr. Data Engineer vs excel guy', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 112, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehlebx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'ups': 886, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 886, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/gsQD1-dl33Q3VrpjrUDVQwJoL4sCsaJVW2Ohf3abjCw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722530678.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/0q52otpzz2gd1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?auto=webp&s=78052c43548c654fc81a9adb77f63dd0f9d6899c', 'width': 1280, 'height': 1032}, 'resolutions': [{'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=108&crop=smart&auto=webp&s=e286ad6e83cae85030f5266b568978f00a37c1d8', 'width': 108, 'height': 87}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=216&crop=smart&auto=webp&s=0f06013ac76c092fee7192f28158ac005fba9b92', 'width': 216, 'height': 174}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=320&crop=smart&auto=webp&s=13ffccdb8e675c8ab83561abf96e82e0b55315ec', 'width': 320, 'height': 258}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=640&crop=smart&auto=webp&s=29fd194d4ce3861cb0a9b3fdbbdcf12ae43f6c00', 'width': 640, 'height': 516}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=960&crop=smart&auto=webp&s=f4bf838cca30717c170959578e820570bb7a225e', 'width': 960, 'height': 774}, {'url': 'https://preview.redd.it/0q52otpzz2gd1.png?width=1080&crop=smart&auto=webp&s=434466fba7a0d8922e5e504acd8145fcafeb11b0', 'width': 1080, 'height': 870}], 'variants': {}, 'id': '_JRN-jGXF5utzwuwX68y-x_cy0ojwr6YdOqWfIWbHRk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1ehlebx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nobilix'), 'discussion_type': None, 'num_comments': 43, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehlebx/sr_data_engineer_vs_excel_guy/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/0q52otpzz2gd1.png', 'subreddit_subscribers': 201126, 'created_utc': 1722530678.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.770+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've applied to about 200 jobs in the past 3 months. Have had about 25 reach out for initial stages(decent ratio), and made it to 5 final rounds. \n\nA lot of companies are asking for different things. \n\n* 4 times I've been asked to do JSON parsing in Python or SQL\n* 5 times I've been asked easy-medium leetcode SQL and Python questions on various data structure manipulation\n* I've been asked to create a trial databricks account and write production level Spark jobs from obscure data sources\n* I've been asked to create a database model in dbdiagrams that allows for users to dynamically create tables using triggered stored procedures\n* I've been grilled on dbt\n* I've been grilled on event driven architecture\n* I've been asked about random things like how cursors store information, linting, partitioning, multi-threading\n\nEvery company is taking their sweet time with decision making, and it's seriously so frustrating. \n\nAnyone else in the same boat?\n\n", 'author_fullname': 't2_fhmml14j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'This market is seriously wack', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efb5xr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 469, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 469, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722287781.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve applied to about 200 jobs in the past 3 months. Have had about 25 reach out for initial stages(decent ratio), and made it to 5 final rounds. </p>\n\n<p>A lot of companies are asking for different things. </p>\n\n<ul>\n<li>4 times I&#39;ve been asked to do JSON parsing in Python or SQL</li>\n<li>5 times I&#39;ve been asked easy-medium leetcode SQL and Python questions on various data structure manipulation</li>\n<li>I&#39;ve been asked to create a trial databricks account and write production level Spark jobs from obscure data sources</li>\n<li>I&#39;ve been asked to create a database model in dbdiagrams that allows for users to dynamically create tables using triggered stored procedures</li>\n<li>I&#39;ve been grilled on dbt</li>\n<li>I&#39;ve been grilled on event driven architecture</li>\n<li>I&#39;ve been asked about random things like how cursors store information, linting, partitioning, multi-threading</li>\n</ul>\n\n<p>Every company is taking their sweet time with decision making, and it&#39;s seriously so frustrating. </p>\n\n<p>Anyone else in the same boat?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1efb5xr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Capable-Jicama2155'), 'discussion_type': None, 'num_comments': 211, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efb5xr/this_market_is_seriously_wack/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1efb5xr/this_market_is_seriously_wack/', 'subreddit_subscribers': 201126, 'created_utc': 1722287781.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.771+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I almost learned R instead of python. At one point there was a real "debate" between which one was more useful for data work.  \n\nMongo DB was literally everywhere for awhile and you almost never hear about it anymore.\n\nWhat are some other formerly hot topics that have been relegated into "oh yeah, I remember that..."?  \n\nEDIT: Bonus HOT TAKE, which current DE topic do you think will end up being an afterthought? ', 'author_fullname': 't2_t1bnfnc4q', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Letâ€™s remember some data engineering fads', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efsgqf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 319, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 319, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722344815.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722343703.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I almost learned R instead of python. At one point there was a real &quot;debate&quot; between which one was more useful for data work.  </p>\n\n<p>Mongo DB was literally everywhere for awhile and you almost never hear about it anymore.</p>\n\n<p>What are some other formerly hot topics that have been relegated into &quot;oh yeah, I remember that...&quot;?  </p>\n\n<p>EDIT: Bonus HOT TAKE, which current DE topic do you think will end up being an afterthought? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1efsgqf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bjogc42069'), 'discussion_type': None, 'num_comments': 335, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efsgqf/lets_remember_some_data_engineering_fads/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1efsgqf/lets_remember_some_data_engineering_fads/', 'subreddit_subscribers': 201126, 'created_utc': 1722343703.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.771+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_k0kr23gm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Can I join you?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efy6al', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'ups': 304, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 304, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/0HQqkfipSwNRdnxkKnFQFK0Xin4jSCAE4UIGmFFT75M.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722357848.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/t9lrzf8toofd1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/t9lrzf8toofd1.png?auto=webp&s=89e93b1dea066f099222848325e37571625e6168', 'width': 500, 'height': 500}, 'resolutions': [{'url': 'https://preview.redd.it/t9lrzf8toofd1.png?width=108&crop=smart&auto=webp&s=a3e14290f67b4520f4b4321f4d6ef1d8d6e2a579', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/t9lrzf8toofd1.png?width=216&crop=smart&auto=webp&s=e4488ea63d6d49cb89134783e023bde4c3a2ab3d', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/t9lrzf8toofd1.png?width=320&crop=smart&auto=webp&s=29654c32366e3684539c08b15ed1bf1bb9f4036a', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'q9ZIgmgk7PCJxSLqFZZ-icGHCPGwQ41dQuHTg9o7Ip4'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1efy6al', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SelectStarData'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efy6al/can_i_join_you/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/t9lrzf8toofd1.png', 'subreddit_subscribers': 201126, 'created_utc': 1722357848.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.772+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I was recently hired as a senior data engineer, and it seems like they\'re pushing me to be the "go-to" person for Power BI within the company. This is surprising because the job description emphasized a strong background in Oracle, ETL, CI/CD pipelines, etc., which aligns with my experience. However, during the skill assessment stage of the recruitment, they focused heavily on my knowledge of Power BI, likely because of my previous role as a senior BI developer.\n\nDoes anyone else find this odd? Data engineering roles typically involve skills that require backend data processing, something that you can do with Python, Kafka, and Airflow, rather than focusing so much on a front-end system such as Power BI. Please let me know what you think.', 'author_fullname': 't2_6k1xiqlt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A data engineer doing Power BI stuff?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edezgh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 154, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 154, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722082449.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was recently hired as a senior data engineer, and it seems like they&#39;re pushing me to be the &quot;go-to&quot; person for Power BI within the company. This is surprising because the job description emphasized a strong background in Oracle, ETL, CI/CD pipelines, etc., which aligns with my experience. However, during the skill assessment stage of the recruitment, they focused heavily on my knowledge of Power BI, likely because of my previous role as a senior BI developer.</p>\n\n<p>Does anyone else find this odd? Data engineering roles typically involve skills that require backend data processing, something that you can do with Python, Kafka, and Airflow, rather than focusing so much on a front-end system such as Power BI. Please let me know what you think.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1edezgh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Shr1988'), 'discussion_type': None, 'num_comments': 81, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edezgh/a_data_engineer_doing_power_bi_stuff/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edezgh/a_data_engineer_doing_power_bi_stuff/', 'subreddit_subscribers': 201126, 'created_utc': 1722082449.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.773+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm experiencing difficulties finding work as a DE. I thought I have a good shot at getting at least some calls, but I've quite literally gotten 0 in over 100 applications. I'm fairly experienced in Python, SQL, PySpark, Tableau, Airflow, and data modeling. I've done work critical to building and supporting multi million dollar operations at scale. From what I see, with regards to technical skills I'm missing dbt and I'm lacking system design experience.\n\nThis is moreso directed to seniors and hiring managers - what do you look for in applicants?\n\nEdit: looking for senior DE roles with 8 YoE as an analyst/DE", 'author_fullname': 't2_14fnbt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What separates the average DE from a desirable DE in this market?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh05ya', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 98, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 98, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722473017.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722464231.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m experiencing difficulties finding work as a DE. I thought I have a good shot at getting at least some calls, but I&#39;ve quite literally gotten 0 in over 100 applications. I&#39;m fairly experienced in Python, SQL, PySpark, Tableau, Airflow, and data modeling. I&#39;ve done work critical to building and supporting multi million dollar operations at scale. From what I see, with regards to technical skills I&#39;m missing dbt and I&#39;m lacking system design experience.</p>\n\n<p>This is moreso directed to seniors and hiring managers - what do you look for in applicants?</p>\n\n<p>Edit: looking for senior DE roles with 8 YoE as an analyst/DE</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1eh05ya', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='git0ffmylawnm8'), 'discussion_type': None, 'num_comments': 51, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh05ya/what_separates_the_average_de_from_a_desirable_de/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh05ya/what_separates_the_average_de_from_a_desirable_de/', 'subreddit_subscribers': 201126, 'created_utc': 1722464231.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.775+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey everyone, I work as a data analyst for a behavioral health non profit with serious data issues. We're a pretty decent size organization, seeing over 3000 patients a year. I've been there four years, starting out as a data analyst and have been working to increase the use of data in leadership's day to day decisions. As the only technical person on staff besides the IT department - made up of only one person which is a whole other issue - part of my journey has been to shift towards data engineering as it lightens my analytics role considerably. \n\nHowever, due to very limited resources, significant data issues, and little interest in the data itself, I've been forced to do all the engineering in less than ideal ways. I'm curious to hear the communities feedback on my methodology. My process is cheap and simple. \n\n* A Windows batch file with Windows Task Scheduler running on a VM on a largely unutilized local server.\n* Python's Pandas library for data processing, feature engineering and any complex operation because it's simple to set up, very readable, while still powerful. \n* Power BI for the data modelling, data cataloging and, of course, the front dashboard display. \n\nChange management for leadership - a bunch of clinicians that were given management positions so they wouldn't go out and start their own practices - is a whole different story.  \n\nIf you're curious to hear more details about the dysfunction and my process, check out my article below:\n\n[Nonprofit Data Analytics - Dysfunction with No One to Blame.](https://stevesgroceries.substack.com/p/dysfunctional-nonprofit-data-analytics)", 'author_fullname': 't2_iy5z26ii', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering and Analytics for Nonprofits', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eecvln', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 94, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 94, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722188031.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone, I work as a data analyst for a behavioral health non profit with serious data issues. We&#39;re a pretty decent size organization, seeing over 3000 patients a year. I&#39;ve been there four years, starting out as a data analyst and have been working to increase the use of data in leadership&#39;s day to day decisions. As the only technical person on staff besides the IT department - made up of only one person which is a whole other issue - part of my journey has been to shift towards data engineering as it lightens my analytics role considerably. </p>\n\n<p>However, due to very limited resources, significant data issues, and little interest in the data itself, I&#39;ve been forced to do all the engineering in less than ideal ways. I&#39;m curious to hear the communities feedback on my methodology. My process is cheap and simple. </p>\n\n<ul>\n<li>A Windows batch file with Windows Task Scheduler running on a VM on a largely unutilized local server.</li>\n<li>Python&#39;s Pandas library for data processing, feature engineering and any complex operation because it&#39;s simple to set up, very readable, while still powerful. </li>\n<li>Power BI for the data modelling, data cataloging and, of course, the front dashboard display. </li>\n</ul>\n\n<p>Change management for leadership - a bunch of clinicians that were given management positions so they wouldn&#39;t go out and start their own practices - is a whole different story.  </p>\n\n<p>If you&#39;re curious to hear more details about the dysfunction and my process, check out my article below:</p>\n\n<p><a href="https://stevesgroceries.substack.com/p/dysfunctional-nonprofit-data-analytics">Nonprofit Data Analytics - Dysfunction with No One to Blame.</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?auto=webp&s=e844d2e424970c9b19007ebb63c3b14fccab1f38', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?width=108&crop=smart&auto=webp&s=8eb8fa60d74e7357212aaeb98ad7a5ebf999254a', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?width=216&crop=smart&auto=webp&s=6c872be40e57648d82dfb370e153b897ca7980b1', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?width=320&crop=smart&auto=webp&s=719e62e31260bb132d2e706378cd0a56d0c84283', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?width=640&crop=smart&auto=webp&s=9c5bbb21717f65c4ceb01c124a12b44b341a8ee3', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?width=960&crop=smart&auto=webp&s=4fa699a1ed9bc4643b6b504d90953a0bf553e887', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/otblovJJhrWgKR0LSTr_UNcivtpNZL-7S6tm55skLbA.jpg?width=1080&crop=smart&auto=webp&s=691bf51a44d65297d3709b21ff9291e3b71bfcd4', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'EMBlvbzqLCh09C1eXihgdmmKr7TLOMYThGZS4okyEMk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eecvln', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Distinct-Grocery-784'), 'discussion_type': None, 'num_comments': 39, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eecvln/data_engineering_and_analytics_for_nonprofits/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eecvln/data_engineering_and_analytics_for_nonprofits/', 'subreddit_subscribers': 201126, 'created_utc': 1722188031.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.775+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'As topic suggests - does anyone with data engineering background has started own business based on data engineering skills? If yes, what business is that?', 'author_fullname': 't2_15yv6m9r', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any data engineers who started own business?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef7u72', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 84, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 84, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722279923.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As topic suggests - does anyone with data engineering background has started own business based on data engineering skills? If yes, what business is that?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ef7u72', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mrscript_lt'), 'discussion_type': None, 'num_comments': 16, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef7u72/any_data_engineers_who_started_own_business/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef7u72/any_data_engineers_who_started_own_business/', 'subreddit_subscribers': 201126, 'created_utc': 1722279923.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.776+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm curious what others who also enjoy data modeling do for fun because perhaps I would enjoy it too!\n\nPersonally, I'm a sucker for grand strategy games like Stellaris, Crusader Kings, Total War, and can easily play 9 hours straight. Doesn't sound a lot like data modeling, but oddly it feels like it's scratching a similar itch.", 'author_fullname': 't2_339yllrj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What are some of your hobbies and interests outside of work?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg2hed', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 68, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 68, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722368201.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m curious what others who also enjoy data modeling do for fun because perhaps I would enjoy it too!</p>\n\n<p>Personally, I&#39;m a sucker for grand strategy games like Stellaris, Crusader Kings, Total War, and can easily play 9 hours straight. Doesn&#39;t sound a lot like data modeling, but oddly it feels like it&#39;s scratching a similar itch.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eg2hed', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ToroBall'), 'discussion_type': None, 'num_comments': 95, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg2hed/what_are_some_of_your_hobbies_and_interests/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg2hed/what_are_some_of_your_hobbies_and_interests/', 'subreddit_subscribers': 201126, 'created_utc': 1722368201.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.776+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Over the last three weeks, I've released an article per week that looks at Kimball data modelling.\n\nWeek 1: [Dimension Tables](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-1-0ed765574712?source=friends_link&sk=33dcb7916ce7845e46aa986fb56902f1)  \nWeek 2: [Fact Tables](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-2-4ee64be25cf3?source=friends_link&sk=94919b6ef2ce92b93fd06f4b869f22a2)\n\nThis is the final week of the mini-series, talking about the often misunderstood [Bridge Tables](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-3-048a12b061c9?source=friends_link&sk=690bd81b14333f5210bbde80b1582df6). I hope people find this interesting, and ideally helpful!\n\nAll three links are paywall bypassed.", 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Kimball Data Modelling - An overview in 3 parts', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eeqg4l', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 69, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 69, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722226387.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Over the last three weeks, I&#39;ve released an article per week that looks at Kimball data modelling.</p>\n\n<p>Week 1: <a href="https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-1-0ed765574712?source=friends_link&amp;sk=33dcb7916ce7845e46aa986fb56902f1">Dimension Tables</a><br/>\nWeek 2: <a href="https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-2-4ee64be25cf3?source=friends_link&amp;sk=94919b6ef2ce92b93fd06f4b869f22a2">Fact Tables</a></p>\n\n<p>This is the final week of the mini-series, talking about the often misunderstood <a href="https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-3-048a12b061c9?source=friends_link&amp;sk=690bd81b14333f5210bbde80b1582df6">Bridge Tables</a>. I hope people find this interesting, and ideally helpful!</p>\n\n<p>All three links are paywall bypassed.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?auto=webp&s=0fb879e8d455f480548a86132809a33a0c212f6b', 'width': 1200, 'height': 722}, 'resolutions': [{'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?width=108&crop=smart&auto=webp&s=7e98b00e1e57d6adecea167004916d6c07cf86e5', 'width': 108, 'height': 64}, {'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?width=216&crop=smart&auto=webp&s=df06bc46e02e21562148e9d93b86a28634e1c49e', 'width': 216, 'height': 129}, {'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?width=320&crop=smart&auto=webp&s=8eea53b47ca9013d3738853d88969468c461b4a0', 'width': 320, 'height': 192}, {'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?width=640&crop=smart&auto=webp&s=f901cabcc72b40445340a8baefa9b43b570130f3', 'width': 640, 'height': 385}, {'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?width=960&crop=smart&auto=webp&s=39e260c83968410da3c72fc01d15b6f297153944', 'width': 960, 'height': 577}, {'url': 'https://external-preview.redd.it/tNnRQdFgnbKREBpIkuALewgEyFug9-KGwyDwn1bq7gk.jpg?width=1080&crop=smart&auto=webp&s=3c6abf8e32da162589dfd69c000c28ee40bf9685', 'width': 1080, 'height': 649}], 'variants': {}, 'id': 'dJCcBFPz4iV0N0FWp5Wu2cK9M6XMLocvQ29bN-ioq-E'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1eeqg4l', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1eeqg4l/kimball_data_modelling_an_overview_in_3_parts/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eeqg4l/kimball_data_modelling_an_overview_in_3_parts/', 'subreddit_subscribers': 201126, 'created_utc': 1722226387.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.777+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I understand the usage of row number and the use of dense\\_rank(). The latter used for ties. However, I cannot understand where and when would anyone ever use rank() function over dense\\_rank(). I understand the difference of consecutive rank after duplicates with ties with dense\\_rank() and the skip with the other. However, where have you used it ever?', 'author_fullname': 't2_8ixxnnf4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why would anyone ever use rank() window function?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egkoxu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 69, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 69, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722425027.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I understand the usage of row number and the use of dense_rank(). The latter used for ties. However, I cannot understand where and when would anyone ever use rank() function over dense_rank(). I understand the difference of consecutive rank after duplicates with ties with dense_rank() and the skip with the other. However, where have you used it ever?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egkoxu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mozakaak'), 'discussion_type': None, 'num_comments': 69, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egkoxu/why_would_anyone_ever_use_rank_window_function/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egkoxu/why_would_anyone_ever_use_rank_window_function/', 'subreddit_subscribers': 201126, 'created_utc': 1722425027.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.777+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Any tips to be a best BI Analyst / Analytics engineer who canâ€™t be replaced? \n', 'author_fullname': 't2_mzakdcwgd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to become an irreplaceable Business Intelligence analyst / Analytics engineer?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eewlbs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 67, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 67, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722250705.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Any tips to be a best BI Analyst / Analytics engineer who canâ€™t be replaced? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eewlbs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='YouAre_TheOne'), 'discussion_type': None, 'num_comments': 80, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eewlbs/how_to_become_an_irreplaceable_business/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eewlbs/how_to_become_an_irreplaceable_business/', 'subreddit_subscribers': 201126, 'created_utc': 1722250705.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.778+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey everyone! I just launched my third data modeling challenge (think hackathon, but better) for all you data modeling experts out there. This time, the data being modeled is fascinating: User-generated Social Media Data!\n\nHere's the scoop:\n\n* Showcase your SQL, dbt, and analytics skills\n* Derive insights from real social media data (prepare for some interesting findings!)\n* Big prizes up for grabs: $3,000 for 1st place, $2,000 for 2nd, and $1,000 for 3rd!\n\nWhen you sign up, you'll get free access to some seriously cool tools:\n\n* Paradime (for SQL and dbt development)\n* MotherDuck (for storage and compute)\n* Hex (for data visualization and analytics)\n* A Git repository (for version control and challenge submission)\n\nYou'll have about 6 weeks to work on your project at your own pace. After that, a panel of judges will review the submissions and pick the top three winners based on the following criteria: Value of Insights, Quality of Insights, and Complexity of Insights.\n\nThis is a great opportunity to improve your data expertise, network with like-minded folks, add to your project portfolio, uncover fascinating insights from social media data, and of course, compete to win $3k!\n\nInterested in joining? Check out the challenge page here:\xa0[https://www.paradime.io/dbt-data-modeling-challenge](https://www.paradime.io/dbt-data-modeling-challenge)", 'author_fullname': 't2_nkrhcqia', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Just Launched: $6000 Social Media Data Challenge - Showcase Your Data Modeling Skills', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg2jy5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 61, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 61, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722368374.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone! I just launched my third data modeling challenge (think hackathon, but better) for all you data modeling experts out there. This time, the data being modeled is fascinating: User-generated Social Media Data!</p>\n\n<p>Here&#39;s the scoop:</p>\n\n<ul>\n<li>Showcase your SQL, dbt, and analytics skills</li>\n<li>Derive insights from real social media data (prepare for some interesting findings!)</li>\n<li>Big prizes up for grabs: $3,000 for 1st place, $2,000 for 2nd, and $1,000 for 3rd!</li>\n</ul>\n\n<p>When you sign up, you&#39;ll get free access to some seriously cool tools:</p>\n\n<ul>\n<li>Paradime (for SQL and dbt development)</li>\n<li>MotherDuck (for storage and compute)</li>\n<li>Hex (for data visualization and analytics)</li>\n<li>A Git repository (for version control and challenge submission)</li>\n</ul>\n\n<p>You&#39;ll have about 6 weeks to work on your project at your own pace. After that, a panel of judges will review the submissions and pick the top three winners based on the following criteria: Value of Insights, Quality of Insights, and Complexity of Insights.</p>\n\n<p>This is a great opportunity to improve your data expertise, network with like-minded folks, add to your project portfolio, uncover fascinating insights from social media data, and of course, compete to win $3k!</p>\n\n<p>Interested in joining? Check out the challenge page here:\xa0<a href="https://www.paradime.io/dbt-data-modeling-challenge">https://www.paradime.io/dbt-data-modeling-challenge</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?auto=webp&s=796e6f1ee051d5868a36be436b72cbafc9860b7d', 'width': 2560, 'height': 1440}, 'resolutions': [{'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?width=108&crop=smart&auto=webp&s=38239707e331ae18e5716e6a2574b9ab2f0ec4f9', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?width=216&crop=smart&auto=webp&s=712bfda9ea4401474e6bddabb50d2110d35dd882', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?width=320&crop=smart&auto=webp&s=378f91ffa3cc553880f5c2abfd4d91e3be7bc950', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?width=640&crop=smart&auto=webp&s=c05355e5c66314dfab0701039c8a444cda6c07a2', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?width=960&crop=smart&auto=webp&s=008be126c8a6422729c49fbbe199779e1ac7d9d6', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/J0kAP25YsKsU_ABgCl_LH_ZJkK_4TD49IlEhnW2c7Sk.jpg?width=1080&crop=smart&auto=webp&s=2c6453e9888e24194e9da5e533d1c6ad6de0522e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'zYzxPSPFI7Pc7VDrjcAIA56nfaBMPWdyj4-cK3Vi58U'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1eg2jy5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JParkerRogers'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg2jy5/just_launched_6000_social_media_data_challenge/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg2jy5/just_launched_6000_social_media_data_challenge/', 'subreddit_subscribers': 201126, 'created_utc': 1722368374.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.778+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I this this thread https://old.reddit.com/r/cscareerquestions/comments/1e5kjzj/whole_team_let_go_to_hire_offshore_employees/?sort=top and thought that there should be IT jobs that will not or cannot be offshored or maybe it is less likely. \n\nIs there offshoring happening in data engineering too? If yes, how offshoring is/will affect data engineering?', 'author_fullname': 't2_g3q7oxhre', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there offshoring happening in data engineering too? If yes, how offshoring is/will affect for data engineering?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eel3ln', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 59, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 59, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722209765.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I this this thread <a href="https://old.reddit.com/r/cscareerquestions/comments/1e5kjzj/whole_team_let_go_to_hire_offshore_employees/?sort=top">https://old.reddit.com/r/cscareerquestions/comments/1e5kjzj/whole_team_let_go_to_hire_offshore_employees/?sort=top</a> and thought that there should be IT jobs that will not or cannot be offshored or maybe it is less likely. </p>\n\n<p>Is there offshoring happening in data engineering too? If yes, how offshoring is/will affect data engineering?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eel3ln', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Alex_df_300'), 'discussion_type': None, 'num_comments': 63, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eel3ln/is_there_offshoring_happening_in_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eel3ln/is_there_offshoring_happening_in_data_engineering/', 'subreddit_subscribers': 201126, 'created_utc': 1722209765.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.779+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I was a DBA (for a SaaS then a major bank--not mickey mouse) for 6 yrs and now I\'ve done DE for about the same. Something that keeps happening over and over is recruiters will completely disregard my DBA experience as not even remotely relevant to a DE position. They\'ll say something like "so you\'ve only been a build role for 6 yrs then"? making a point to basically say essentially, so that\'s all you got? I\'m probably one of the top valued people on our team because I\'ve become the de-facto SME go-to guy for Redshift, MySQL, DMS and SQL query tuning. You wouldn\'t want someone like that on a DE team (assuming that\'s the stack your team uses daily) ?? I think devs view any non-dev as a gorilla and especially old-school IT side roles (DBA, sysadmin..) as basically completely useless.', 'author_fullname': 't2_10kwn7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBA to DE', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ec56kd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 57, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 57, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1721939664.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721939476.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was a DBA (for a SaaS then a major bank--not mickey mouse) for 6 yrs and now I&#39;ve done DE for about the same. Something that keeps happening over and over is recruiters will completely disregard my DBA experience as not even remotely relevant to a DE position. They&#39;ll say something like &quot;so you&#39;ve only been a build role for 6 yrs then&quot;? making a point to basically say essentially, so that&#39;s all you got? I&#39;m probably one of the top valued people on our team because I&#39;ve become the de-facto SME go-to guy for Redshift, MySQL, DMS and SQL query tuning. You wouldn&#39;t want someone like that on a DE team (assuming that&#39;s the stack your team uses daily) ?? I think devs view any non-dev as a gorilla and especially old-school IT side roles (DBA, sysadmin..) as basically completely useless.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ec56kd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bcsamsquanch'), 'discussion_type': None, 'num_comments': 53, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ec56kd/dba_to_de/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ec56kd/dba_to_de/', 'subreddit_subscribers': 201126, 'created_utc': 1721939476.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.779+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Iâ€™m a fresh grad and Iâ€™ve joined the company as the only data engineer for the past 4 months. I was previously there for an internship where I developed a POC dashboard, mainly using Kedro and Streamlit for my pipelines and data visualisation. With more focus put on data, I thought it would be a good idea to join them but even 6 months later they are lacking experienced hires in this area.\n\nInterestingly, we have an ML team who have been focused on building one external-facing product and multiple internal ones with all their data stored on one (1) considerably powerful workstation where model training is also done (that takes 4-6 weeks on average). The data retrieval method from the production system is somewhat hacky, so with me joining the company, there is support from multiple functions to make the data more centralised via a single-source-of-truth infrastructure. My degree is in data science, so I have more experience with data preparation and model training / evaluation, but setting up this infrastructure is really new to me. Everything has been on-prem for this company (been around for >30 years) and weâ€™re looking at going into Cloud, so data retrieval can be more standardised in a single repository. Now I feel like I'm wearing many hats - product manager, data engineer, UX researcher, and I am wondering what my next steps should be to move things forward while ensuring good practices down the road.\n\nThe first use case would be a dashboard product for our customers and Iâ€™m currently debating how the dashboard frontend and backend should be built. Do you all have any advice on the tech stack to adopt? I was considering the below, but I am really open to ideas:\n\n1. Azure Data Lake Storage (still thinking about how data retrieval here is better but a colleague who was an ex-CTO of a startup is always pushing for it)\n2. Azure Databricks (whatâ€™s the industry best practice to get data from ADLS here?)\n3. Custom built Webapp w Angular & Nextjs\n4. Azure App Service (or should we host it elsewhere?)\n\nI'm looking at Azure as a colleague who was an ex-CTO pointed me in this direction and that we use Microsoft Office but it felt like using it for the sake of using it, so I'm thinking if this is the way to go. Right now Iâ€™m explore how to connect the data from point A to B and which orchestrator to use to orchestrate everything, is this the right way to go? How would someone with multiple YOE go about this? I read about many stories about how the tech stack used serves use cases in the short term, but breaks along with time - how can I prevent that? Iâ€™m trying to document past data-related projects the company has taken on and how data flows for those. What are some tech stacks I should look into to better take on this responsibility?", 'author_fullname': 't2_rnmfq77m3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "I'm the ONLY Data Engineer in the company as a Fresh-grad", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efny7v', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 56, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 56, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722327271.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Iâ€™m a fresh grad and Iâ€™ve joined the company as the only data engineer for the past 4 months. I was previously there for an internship where I developed a POC dashboard, mainly using Kedro and Streamlit for my pipelines and data visualisation. With more focus put on data, I thought it would be a good idea to join them but even 6 months later they are lacking experienced hires in this area.</p>\n\n<p>Interestingly, we have an ML team who have been focused on building one external-facing product and multiple internal ones with all their data stored on one (1) considerably powerful workstation where model training is also done (that takes 4-6 weeks on average). The data retrieval method from the production system is somewhat hacky, so with me joining the company, there is support from multiple functions to make the data more centralised via a single-source-of-truth infrastructure. My degree is in data science, so I have more experience with data preparation and model training / evaluation, but setting up this infrastructure is really new to me. Everything has been on-prem for this company (been around for &gt;30 years) and weâ€™re looking at going into Cloud, so data retrieval can be more standardised in a single repository. Now I feel like I&#39;m wearing many hats - product manager, data engineer, UX researcher, and I am wondering what my next steps should be to move things forward while ensuring good practices down the road.</p>\n\n<p>The first use case would be a dashboard product for our customers and Iâ€™m currently debating how the dashboard frontend and backend should be built. Do you all have any advice on the tech stack to adopt? I was considering the below, but I am really open to ideas:</p>\n\n<ol>\n<li>Azure Data Lake Storage (still thinking about how data retrieval here is better but a colleague who was an ex-CTO of a startup is always pushing for it)</li>\n<li>Azure Databricks (whatâ€™s the industry best practice to get data from ADLS here?)</li>\n<li>Custom built Webapp w Angular &amp; Nextjs</li>\n<li>Azure App Service (or should we host it elsewhere?)</li>\n</ol>\n\n<p>I&#39;m looking at Azure as a colleague who was an ex-CTO pointed me in this direction and that we use Microsoft Office but it felt like using it for the sake of using it, so I&#39;m thinking if this is the way to go. Right now Iâ€™m explore how to connect the data from point A to B and which orchestrator to use to orchestrate everything, is this the right way to go? How would someone with multiple YOE go about this? I read about many stories about how the tech stack used serves use cases in the short term, but breaks along with time - how can I prevent that? Iâ€™m trying to document past data-related projects the company has taken on and how data flows for those. What are some tech stacks I should look into to better take on this responsibility?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1efny7v', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Internal-Calendar-92'), 'discussion_type': None, 'num_comments': 33, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efny7v/im_the_only_data_engineer_in_the_company_as_a/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1efny7v/im_the_only_data_engineer_in_the_company_as_a/', 'subreddit_subscribers': 201126, 'created_utc': 1722327271.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.780+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm learning about data modeling and dimensional modeling technique. I chose to read The Datawarehouse Toolkit by Raph Kimball and came across his comparison between Star Schemas and OLAP Cubes as below.\n\nhttps://preview.redd.it/84p0cj0emffd1.png?width=593&format=png&auto=webp&s=118457b04c1d64610e5b51ef957acc51563fe74f\n\n  \nAfter reading it over and over again, I still can't not understand the main differences between them. Can you explain the author's meaning? Are they in the same logical model but just in 2 separated physical model because of the different types of storage?", 'author_fullname': 't2_eio8j1l59', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What is the difference between Star Schemas and  OLAP cubes?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'84p0cj0emffd1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 154, 'x': 108, 'u': 'https://preview.redd.it/84p0cj0emffd1.png?width=108&crop=smart&auto=webp&s=1da13c6197bc28c6ec0a53c1189b94ab5d7bec67'}, {'y': 308, 'x': 216, 'u': 'https://preview.redd.it/84p0cj0emffd1.png?width=216&crop=smart&auto=webp&s=29bd5e516def2a3f9458821e368005fd9b044808'}, {'y': 457, 'x': 320, 'u': 'https://preview.redd.it/84p0cj0emffd1.png?width=320&crop=smart&auto=webp&s=d80544a1a1b867bcf1f6cfe02b26c94a0eb703a7'}], 's': {'y': 848, 'x': 593, 'u': 'https://preview.redd.it/84p0cj0emffd1.png?width=593&format=png&auto=webp&s=118457b04c1d64610e5b51ef957acc51563fe74f'}, 'id': '84p0cj0emffd1'}}, 'name': 't3_1eevtwi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 51, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 51, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/X22IyNta_HqGfiBAdGeOku719f95pMrtESVLddKBEF0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722247802.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m learning about data modeling and dimensional modeling technique. I chose to read The Datawarehouse Toolkit by Raph Kimball and came across his comparison between Star Schemas and OLAP Cubes as below.</p>\n\n<p><a href="https://preview.redd.it/84p0cj0emffd1.png?width=593&amp;format=png&amp;auto=webp&amp;s=118457b04c1d64610e5b51ef957acc51563fe74f">https://preview.redd.it/84p0cj0emffd1.png?width=593&amp;format=png&amp;auto=webp&amp;s=118457b04c1d64610e5b51ef957acc51563fe74f</a></p>\n\n<p>After reading it over and over again, I still can&#39;t not understand the main differences between them. Can you explain the author&#39;s meaning? Are they in the same logical model but just in 2 separated physical model because of the different types of storage?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eevtwi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='chimeyrock'), 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eevtwi/what_is_the_difference_between_star_schemas_and/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eevtwi/what_is_the_difference_between_star_schemas_and/', 'subreddit_subscribers': 201126, 'created_utc': 1722247802.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.780+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Iâ€™m new here still learning de currently using aws, and i need help.\n\nMy ETL job runs for the files that have been processed in my raw bucket. How do i handle this? I need the job to run only for files that has not be processed yet. Any tips or recommendation industry practice would be very helpfulðŸ™ðŸ»ðŸ™ðŸ»', 'author_fullname': 't2_fta9agm0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How you guys handle incremental files for ETL/ELT?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egskf5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 50, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 50, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722445806.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Iâ€™m new here still learning de currently using aws, and i need help.</p>\n\n<p>My ETL job runs for the files that have been processed in my raw bucket. How do i handle this? I need the job to run only for files that has not be processed yet. Any tips or recommendation industry practice would be very helpfulðŸ™ðŸ»ðŸ™ðŸ»</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egskf5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='luqmancrit69'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egskf5/how_you_guys_handle_incremental_files_for_etlelt/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egskf5/how_you_guys_handle_incremental_files_for_etlelt/', 'subreddit_subscribers': 201126, 'created_utc': 1722445806.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.781+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '10gb large csv file, read with pandas "low\\_memory=False" argument. took a while!\n\nexported as parquet with the compression methods below.\n\n* Snappy ( default, requires no argument)\n* gzip\n* brotli\n* zstd\n\nhttps://preview.redd.it/4ax24idvk2fd1.png?width=1380&format=png&auto=webp&s=f5593756255bf8fce4918509b1df042e310b4a8a\n\n**Result: BROTLI Compression is the Winner!  ZSTD being the fastest though!** \n\nhttps://preview.redd.it/9cfvneu2l2fd1.png?width=1272&format=png&auto=webp&s=685e1b1451c7ae3345fdf8c0d421d4ee83775c95\n\n', 'author_fullname': 't2_9eonutls', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '10gb large Csv File, Export as parquet, compression comparison!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 66, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'9cfvneu2l2fd1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 34, 'x': 108, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=108&crop=smart&auto=webp&s=15a6a5cbf9b87d225eaa8fd64d5bd844aed07e24'}, {'y': 69, 'x': 216, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=216&crop=smart&auto=webp&s=3a48c223592f9fca61285d10fc819780d28d12ee'}, {'y': 102, 'x': 320, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=320&crop=smart&auto=webp&s=53d8c660c52313d76a9416b6d618144b8a2d134e'}, {'y': 205, 'x': 640, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=640&crop=smart&auto=webp&s=f69aaf13041c097ba916cf6e7b650402d95be254'}, {'y': 307, 'x': 960, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=960&crop=smart&auto=webp&s=dc51f320e609407880400db1274f5a9a9a11c130'}, {'y': 346, 'x': 1080, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=1080&crop=smart&auto=webp&s=544c154957b09ce786389247c33e7ef1123560bb'}], 's': {'y': 408, 'x': 1272, 'u': 'https://preview.redd.it/9cfvneu2l2fd1.png?width=1272&format=png&auto=webp&s=685e1b1451c7ae3345fdf8c0d421d4ee83775c95'}, 'id': '9cfvneu2l2fd1'}, '4ax24idvk2fd1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 51, 'x': 108, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=108&crop=smart&auto=webp&s=f8ee641612722a9805f02b4e50648812efb0a77c'}, {'y': 103, 'x': 216, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=216&crop=smart&auto=webp&s=86362de731d3ca3186800b1d5b5a42348f5b4072'}, {'y': 153, 'x': 320, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=320&crop=smart&auto=webp&s=18a0831e15cbedfcdd3b87410b296f1341ec3e82'}, {'y': 306, 'x': 640, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=640&crop=smart&auto=webp&s=53e7eb2321494cd713af996abe469a38ee44e8f1'}, {'y': 459, 'x': 960, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=960&crop=smart&auto=webp&s=82a4bdfe6f18655978d046743abbf7b1638edb3f'}, {'y': 516, 'x': 1080, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=1080&crop=smart&auto=webp&s=bbfb4cddbae0e61e1e2c98ad0bee6c7e4eb30034'}], 's': {'y': 660, 'x': 1380, 'u': 'https://preview.redd.it/4ax24idvk2fd1.png?width=1380&format=png&auto=webp&s=f5593756255bf8fce4918509b1df042e310b4a8a'}, 'id': '4ax24idvk2fd1'}}, 'name': 't3_1ed1gtw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 49, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 49, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/S16WXgw-c-2kvDaiESHS9dNNGxtohy37zbOdubyePGY.jpg', 'edited': 1722089827.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722034606.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>10gb large csv file, read with pandas &quot;low_memory=False&quot; argument. took a while!</p>\n\n<p>exported as parquet with the compression methods below.</p>\n\n<ul>\n<li>Snappy ( default, requires no argument)</li>\n<li>gzip</li>\n<li>brotli</li>\n<li>zstd</li>\n</ul>\n\n<p><a href="https://preview.redd.it/4ax24idvk2fd1.png?width=1380&amp;format=png&amp;auto=webp&amp;s=f5593756255bf8fce4918509b1df042e310b4a8a">https://preview.redd.it/4ax24idvk2fd1.png?width=1380&amp;format=png&amp;auto=webp&amp;s=f5593756255bf8fce4918509b1df042e310b4a8a</a></p>\n\n<p><strong>Result: BROTLI Compression is the Winner!  ZSTD being the fastest though!</strong> </p>\n\n<p><a href="https://preview.redd.it/9cfvneu2l2fd1.png?width=1272&amp;format=png&amp;auto=webp&amp;s=685e1b1451c7ae3345fdf8c0d421d4ee83775c95">https://preview.redd.it/9cfvneu2l2fd1.png?width=1272&amp;format=png&amp;auto=webp&amp;s=685e1b1451c7ae3345fdf8c0d421d4ee83775c95</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1ed1gtw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BrianDeFlorida'), 'discussion_type': None, 'num_comments': 18, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ed1gtw/10gb_large_csv_file_export_as_parquet_compression/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ed1gtw/10gb_large_csv_file_export_as_parquet_compression/', 'subreddit_subscribers': 201126, 'created_utc': 1722034606.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.781+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_19f2veuo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The rise of the analytics pretendgineer', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efio46', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'ups': 47, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 47, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Z5HSD4bIRTApXZhRAjn0lGp-yu0y6qEv5tBxp1-W2-s.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722307970.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'benn.substack.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://benn.substack.com/p/the-rise-of-the-analytics-pretendgineer', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?auto=webp&s=67414c8c9850db2b87b41b92d8ca4d508558bc2b', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?width=108&crop=smart&auto=webp&s=07706d2759b4b0c4d80ebe2b99012b4b2bb48ac7', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?width=216&crop=smart&auto=webp&s=e7ff09674e0e1b87d7e3f43e1b3f9794417d36f0', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?width=320&crop=smart&auto=webp&s=d95f99a9706be536f69b196f9b5606beacd483da', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?width=640&crop=smart&auto=webp&s=98488672f1eda8d39152229cbf4decaff88b0914', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?width=960&crop=smart&auto=webp&s=a1fda4f438891061c025d3449f38b009311e243f', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/s_fU4BsONJ0EsT2pA6A8zoBSH56lKkDdw8IA_L6KbGk.jpg?width=1080&crop=smart&auto=webp&s=0ecd4c68792e7647934c621ba634700f68e818aa', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'I0evmNtkv4cEW7yMmWHLZafx4b5BhwhfFnz1gIwrUGo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1efio46', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='m3-bs'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efio46/the_rise_of_the_analytics_pretendgineer/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://benn.substack.com/p/the-rise-of-the-analytics-pretendgineer', 'subreddit_subscribers': 201126, 'created_utc': 1722307970.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.782+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "ARM chips enhance Kafkaâ€™s speed, here are the benchmarks\n\nHey everyone, if you're using Kafka in your data stack, the results of our latest tests at DoubleCloud might catch your interest. We've been evaluating various environments to determine the most cost-effective setup for Kafka.\n\nTo evaluate different architectures, we quantified how many millions of rows could be ingested into the Kafka broker per cent spent. You can view these results in the attached image. For a more in-depth look, feel free to explore our detailed research findings in our blog post:\n\nhttps://double.cloud/blog/posts/2024/06/benchmarking-apache-kafka-performance-per-price/\n\nSome of our key findings include:\n\n- The m7g family, powered by Graviton 3, outperforms the m6a and m6i families by up to 39% in certain scenarios.\n\n- While each new AMD or Intel processor shows improvement, the efficiency gains in the newer generations seem to have plateaued.\n\n- There is no significant improvement with newer JVM versions on ARM architecture. However, it appears that OpenJDK-11 and Corretto-11 are already quite optimized for ARM.\n\n- Discounts can make even older tech, like Ampere Alta, competitive; at least in terms of cost-efficiency.\n\nI'd love to hear about your experiences with benchmarking Kafka or get your thoughts on our approach. Your insights would be greatly appreciated!\n\n\n\n", 'author_fullname': 't2_13u9dhsros', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ARM chips enhance Kafkaâ€™s speed, here are the benchmarks', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 100, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ee6gte', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.99, 'author_flair_background_color': None, 'ups': 51, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 51, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/vWUC1uR_SeWcUeZpYX_knbyrlgAj88GSlmIJLM64dxI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722170186.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>ARM chips enhance Kafkaâ€™s speed, here are the benchmarks</p>\n\n<p>Hey everyone, if you&#39;re using Kafka in your data stack, the results of our latest tests at DoubleCloud might catch your interest. We&#39;ve been evaluating various environments to determine the most cost-effective setup for Kafka.</p>\n\n<p>To evaluate different architectures, we quantified how many millions of rows could be ingested into the Kafka broker per cent spent. You can view these results in the attached image. For a more in-depth look, feel free to explore our detailed research findings in our blog post:</p>\n\n<p><a href="https://double.cloud/blog/posts/2024/06/benchmarking-apache-kafka-performance-per-price/">https://double.cloud/blog/posts/2024/06/benchmarking-apache-kafka-performance-per-price/</a></p>\n\n<p>Some of our key findings include:</p>\n\n<ul>\n<li><p>The m7g family, powered by Graviton 3, outperforms the m6a and m6i families by up to 39% in certain scenarios.</p></li>\n<li><p>While each new AMD or Intel processor shows improvement, the efficiency gains in the newer generations seem to have plateaued.</p></li>\n<li><p>There is no significant improvement with newer JVM versions on ARM architecture. However, it appears that OpenJDK-11 and Corretto-11 are already quite optimized for ARM.</p></li>\n<li><p>Discounts can make even older tech, like Ampere Alta, competitive; at least in terms of cost-efficiency.</p></li>\n</ul>\n\n<p>I&#39;d love to hear about your experiences with benchmarking Kafka or get your thoughts on our approach. Your insights would be greatly appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/c31y37i589fd1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?auto=webp&s=b054fe116d0399bff9fcd2f0b38c424f80ea8ab3', 'width': 3264, 'height': 2340}, 'resolutions': [{'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?width=108&crop=smart&auto=webp&s=a78717f9efa53f7fa96ab323e70af77d313e7b24', 'width': 108, 'height': 77}, {'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?width=216&crop=smart&auto=webp&s=cba9a2cc85ada6fcfa39d9c74772257f3cee9e70', 'width': 216, 'height': 154}, {'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?width=320&crop=smart&auto=webp&s=3009ec32d1a49031b09a2046f905b828e221c6dd', 'width': 320, 'height': 229}, {'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?width=640&crop=smart&auto=webp&s=206bc11e8d2d2dd43f186d1dcb92501fb083fafa', 'width': 640, 'height': 458}, {'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?width=960&crop=smart&auto=webp&s=24bf081ee342406bc1f16525c9ceda4bf7f773c6', 'width': 960, 'height': 688}, {'url': 'https://preview.redd.it/c31y37i589fd1.jpeg?width=1080&crop=smart&auto=webp&s=967a9b4390c9b0443515d6ca5b536ae82ae7aa17', 'width': 1080, 'height': 774}], 'variants': {}, 'id': 'F3l3C0xnbHqNDWAP_b_OlpJOlp01B3NGjDGUhdrbhzI'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ee6gte', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Zestyclose-Editor563'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ee6gte/arm_chips_enhance_kafkas_speed_here_are_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/c31y37i589fd1.jpeg', 'subreddit_subscribers': 201126, 'created_utc': 1722170186.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.782+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone. Currently, I am facing some difficulties in choosing a database. I work at a small company, and we have a project to create a database where molecular biologists can upload data and query other users' data. Due to the nature of molecular biology data, we need a high write throughput (each upload contains about 4 million rows). Therefore, we chose Cassandra because of its fast write speed (tested on our server at 10 million rows / 140s).\n\nHowever, the current issue is that Cassandra does not have an open-source solution for exporting an API for the frontend to query. If we have to code the backend REST API ourselves, it will be very tiring and time-consuming. I am looking for another database that can do this. I am considering HBase as an alternative solution. Is it really stable? Is there any combo like Directus + Postgres? Please give me your opinions.", 'author_fullname': 't2_wdxjgcmct', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Which database should I choose for a large database?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh7aux', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 45, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 45, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722484782.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone. Currently, I am facing some difficulties in choosing a database. I work at a small company, and we have a project to create a database where molecular biologists can upload data and query other users&#39; data. Due to the nature of molecular biology data, we need a high write throughput (each upload contains about 4 million rows). Therefore, we chose Cassandra because of its fast write speed (tested on our server at 10 million rows / 140s).</p>\n\n<p>However, the current issue is that Cassandra does not have an open-source solution for exporting an API for the frontend to query. If we have to code the backend REST API ourselves, it will be very tiring and time-consuming. I am looking for another database that can do this. I am considering HBase as an alternative solution. Is it really stable? Is there any combo like Directus + Postgres? Please give me your opinions.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eh7aux', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Practical_Slip6791'), 'discussion_type': None, 'num_comments': 54, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh7aux/which_database_should_i_choose_for_a_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh7aux/which_database_should_i_choose_for_a_large/', 'subreddit_subscribers': 201126, 'created_utc': 1722484782.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.783+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have been hired in a company to modernize their data architecture. \nSaid company manages A LOT of pipelines with just stored procedures and it is having problems anyone expects (data quality, no clear data lineage, debugging difficultiesâ€¦).\n\nHow would you change that? \nIn my previous role I always managed pipelines through superclassic dbt+airflow combination, and it worked fine. \nMy issue/doubt here is that the number of pipelines here is far bigger than before.\n\nDid this challenge occur to you? How did you manage it?', 'author_fullname': 't2_kro1n8ih', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do you scale 100+ pipelines?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edg33t', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.99, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 46, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 46, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722085995.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have been hired in a company to modernize their data architecture. \nSaid company manages A LOT of pipelines with just stored procedures and it is having problems anyone expects (data quality, no clear data lineage, debugging difficultiesâ€¦).</p>\n\n<p>How would you change that? \nIn my previous role I always managed pipelines through superclassic dbt+airflow combination, and it worked fine. \nMy issue/doubt here is that the number of pipelines here is far bigger than before.</p>\n\n<p>Did this challenge occur to you? How did you manage it?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1edg33t', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AtLeast3Characters92'), 'discussion_type': None, 'num_comments': 36, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edg33t/how_do_you_scale_100_pipelines/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edg33t/how_do_you_scale_100_pipelines/', 'subreddit_subscribers': 201126, 'created_utc': 1722085995.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.783+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Databricks will start charging for egress traffic on the use of all serverless products and features. The detail what the egress charges will cost, but it's unclear which services are going to trigger the charges and how much traffic they're going to generate. It comes at a weird time in the industry when members of the Bandwidth Alliance like Google, Azure, and Alibaba are moving to reduce or eliminate egress fees altogether. \n\n[https://www.databricks.com/product/pricing/platform-addons](https://www.databricks.com/product/pricing/platform-addons)", 'author_fullname': 't2_ez8d4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Databricks Egress Charges', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecevdk', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 38, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 38, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721966734.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Databricks will start charging for egress traffic on the use of all serverless products and features. The detail what the egress charges will cost, but it&#39;s unclear which services are going to trigger the charges and how much traffic they&#39;re going to generate. It comes at a weird time in the industry when members of the Bandwidth Alliance like Google, Azure, and Alibaba are moving to reduce or eliminate egress fees altogether. </p>\n\n<p><a href="https://www.databricks.com/product/pricing/platform-addons">https://www.databricks.com/product/pricing/platform-addons</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?auto=webp&s=8df3d11f9844b03cdc17bfe3edfce197765641a5', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?width=108&crop=smart&auto=webp&s=d9b5f2cfdd33acac6e8923f6127406e64b7374a3', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?width=216&crop=smart&auto=webp&s=4f3c5f1d6cdb061f3d5fc3ad9683cd801f7e2f91', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?width=320&crop=smart&auto=webp&s=735183d0d11f1ce0750f769c9266bb3a4715f282', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?width=640&crop=smart&auto=webp&s=c7f7200e19defa8b9381205f7454cc272af7b768', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?width=960&crop=smart&auto=webp&s=8ae5a439591fa5c1c46ee6b92dcdc26b25998c01', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/Jr2T8Tt_ht36NeIn3Wg6ghSVditZAd1XWc1I6s3Y2Mg.jpg?width=1080&crop=smart&auto=webp&s=ebd94bce2d752ef4db9c327b596584a41739c8ee', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'GFLv56MS7kAfWqj81AhKW-SLNBz25fRxc4Z6_qdewOg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecevdk', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Bazencourt'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecevdk/databricks_egress_charges/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecevdk/databricks_egress_charges/', 'subreddit_subscribers': 201126, 'created_utc': 1721966734.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.783+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_uf0vdzs2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is this book worth reading ? pls give some reviews if anyone of you has read it', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehcd6q', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'ups': 38, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 38, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/5tKAaAqTzscuVmSjd5AfoUEgpmwP-6kx7INs3RhX9-g.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722504485.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/yy1fq9i2u0gd1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?auto=webp&s=39f9efa35d898b970780f80dd027f791f156db2c', 'width': 400, 'height': 525}, 'resolutions': [{'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?width=108&crop=smart&auto=webp&s=c54ed9477564b6b2e27a487358db7e29798fd477', 'width': 108, 'height': 141}, {'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?width=216&crop=smart&auto=webp&s=84cfabe3a6dc19440f6e38c8d24fd1aa3887da0b', 'width': 216, 'height': 283}, {'url': 'https://preview.redd.it/yy1fq9i2u0gd1.jpeg?width=320&crop=smart&auto=webp&s=1d9fac121b7d3701f58816d38310320d9f78bf9f', 'width': 320, 'height': 420}], 'variants': {}, 'id': 'tJVjpuGR-P7USSNJa0AIrWRXK9CsJSxZpFgLD63emDs'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehcd6q', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='OpenWeb5282'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehcd6q/is_this_book_worth_reading_pls_give_some_reviews/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/yy1fq9i2u0gd1.jpeg', 'subreddit_subscribers': 201126, 'created_utc': 1722504485.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.784+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_uamr9xer', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Platform Engineers: The Game-Changers of the data team', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 123, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ec3lrv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'ups': 34, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 34, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/VOHR0atN1tporHaLcK1Dv9Nz2nZrWIdeRJBxcIveLPQ.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721935638.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'dlthub.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://dlthub.com/blog/data-platform-engineers', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?auto=webp&s=bcde9c277a3a6d0f99a3dfd574b1ea70c7b61815', 'width': 1374, 'height': 1210}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?width=108&crop=smart&auto=webp&s=104b0d3966a1397bc19e70dad50c1d95bb899382', 'width': 108, 'height': 95}, {'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?width=216&crop=smart&auto=webp&s=33366f725e31214f3968d3b1ffbc06937f66a7f2', 'width': 216, 'height': 190}, {'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?width=320&crop=smart&auto=webp&s=f3f3bb9cea5a054595178de62a0f2709e503d5b4', 'width': 320, 'height': 281}, {'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?width=640&crop=smart&auto=webp&s=36620e92eeffcf73897dfc4fb4dce54d512c8def', 'width': 640, 'height': 563}, {'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?width=960&crop=smart&auto=webp&s=7d2fffacb3ae91e00a4c6a68c1a5fd8a6b817763', 'width': 960, 'height': 845}, {'url': 'https://external-preview.redd.it/ZGsVCzqjLLtw9ExGZWQ6PVM61T2LiCRqSdwff081tS4.jpg?width=1080&crop=smart&auto=webp&s=94522a1ca1fd982465d8874bc8e8dff369d592db', 'width': 1080, 'height': 951}], 'variants': {}, 'id': '3G_bIzwPta26sEGkkWbtdTv5DW8yOrhnpKE0o4z4fSA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ec3lrv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Thinker_Assignment'), 'discussion_type': None, 'num_comments': 16, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ec3lrv/data_platform_engineers_the_gamechangers_of_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://dlthub.com/blog/data-platform-engineers', 'subreddit_subscribers': 201126, 'created_utc': 1721935638.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.784+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I know AI is not my cup of tea (with whatever tutorial videos I have watched). \n\nI am more interested in DA, DE, DS but this FOMO is killing me and want to know how much AI is needed for these field. \n\nI dont want to invest time in learning AI just because there is buzz around this word (also i find it a bi difficult to learn) and evryone is reacting as if there wil be job loss who dont kno AI. \n\n', 'author_fullname': 't2_qhfnnc9e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I have AI FOMO', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egl0v0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 33, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 33, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722426146.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I know AI is not my cup of tea (with whatever tutorial videos I have watched). </p>\n\n<p>I am more interested in DA, DE, DS but this FOMO is killing me and want to know how much AI is needed for these field. </p>\n\n<p>I dont want to invest time in learning AI just because there is buzz around this word (also i find it a bi difficult to learn) and evryone is reacting as if there wil be job loss who dont kno AI. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1egl0v0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Guilty-Direction-808'), 'discussion_type': None, 'num_comments': 42, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egl0v0/i_have_ai_fomo/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egl0v0/i_have_ai_fomo/', 'subreddit_subscribers': 201126, 'created_utc': 1722426146.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.784+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Can you recommend any services (free or paid) which can be used to create a couple of small tables and run SQL queries in the browser?\n\nSomething like Leetcode, but with the option to create my own problems. No need for verifying solutions.', 'author_fullname': 't2_s51xk2tyi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Can you recommend SQL playground?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecnr67', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 30, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 30, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721999547.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Can you recommend any services (free or paid) which can be used to create a couple of small tables and run SQL queries in the browser?</p>\n\n<p>Something like Leetcode, but with the option to create my own problems. No need for verifying solutions.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecnr67', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LoudScreamingGoat'), 'discussion_type': None, 'num_comments': 34, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecnr67/can_you_recommend_sql_playground/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecnr67/can_you_recommend_sql_playground/', 'subreddit_subscribers': 201126, 'created_utc': 1721999547.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.785+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Curious to hear if anyone is using any no-code tools for data engineering. I guess Airbyte is the only one Ive used myself.', 'author_fullname': 't2_rtwek', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is anyone using no-code tools?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecr8dt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 27, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 27, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722008402.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Curious to hear if anyone is using any no-code tools for data engineering. I guess Airbyte is the only one Ive used myself.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecr8dt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ecz-'), 'discussion_type': None, 'num_comments': 69, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecr8dt/is_anyone_using_nocode_tools/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecr8dt/is_anyone_using_nocode_tools/', 'subreddit_subscribers': 201126, 'created_utc': 1722008402.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.785+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'As engineers building a BI tool,\xa0we thought, why not treat analytics like software development? We believe\xa0**dashboards should be as-code because this allows you to:**\n\n* Treat dashboards as programmable objects and build reusable components/functions.\n* Dynamically generate charts or dashboards.\n* Version control of every dashboard in a Git repository.\n* Automate and integrate your dashboards with your workflow.\n\nSo, we built one, still a work in progress, but you can play with it here:\xa0[https://playground.holistics.io/play](https://playground.holistics.io/play).\n\nFeedback is very welcome and appreciated.', 'author_fullname': 't2_tipblvmq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Programmable, Dashboards as Code with better governance & reusability', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egq0ky', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 28, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 28, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722487842.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722439645.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As engineers building a BI tool,\xa0we thought, why not treat analytics like software development? We believe\xa0<strong>dashboards should be as-code because this allows you to:</strong></p>\n\n<ul>\n<li>Treat dashboards as programmable objects and build reusable components/functions.</li>\n<li>Dynamically generate charts or dashboards.</li>\n<li>Version control of every dashboard in a Git repository.</li>\n<li>Automate and integrate your dashboards with your workflow.</li>\n</ul>\n\n<p>So, we built one, still a work in progress, but you can play with it here:\xa0<a href="https://playground.holistics.io/play">https://playground.holistics.io/play</a>.</p>\n\n<p>Feedback is very welcome and appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?auto=webp&s=3514fd791b3c9cf51e69e68dc911a5ed4a1f0539', 'width': 1270, 'height': 760}, 'resolutions': [{'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?width=108&crop=smart&auto=webp&s=9d028478615dd387b93fadb66f6b8a7cb2c36242', 'width': 108, 'height': 64}, {'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?width=216&crop=smart&auto=webp&s=c5addd9ca4bcaa7acddce93c96035392e23ef00e', 'width': 216, 'height': 129}, {'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?width=320&crop=smart&auto=webp&s=cd7b44ba972bb5438e58477ea32e08fb45ec4a91', 'width': 320, 'height': 191}, {'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?width=640&crop=smart&auto=webp&s=305e0bf712e29458224ddb0f3bb48d4c3730e246', 'width': 640, 'height': 382}, {'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?width=960&crop=smart&auto=webp&s=b2c170e9b1f50f86b3ddf9a5e9829d4fb4e2205d', 'width': 960, 'height': 574}, {'url': 'https://external-preview.redd.it/eajnzHP3mE8d19V4_oLKdXerqRzcz14AJpsu5lrhMYI.jpg?width=1080&crop=smart&auto=webp&s=b3e02925c3b998fb7f49734923bb06e4528bb6e8', 'width': 1080, 'height': 646}], 'variants': {}, 'id': 'KVUciP8nrhBxNXW8nzgLtZBF91Qv7IIuKIAnM35L9bA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1egq0ky', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='InitiativeOk6728'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egq0ky/programmable_dashboards_as_code_with_better/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egq0ky/programmable_dashboards_as_code_with_better/', 'subreddit_subscribers': 201126, 'created_utc': 1722439645.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.786+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Can anyone please help me to get one full structured data engineering course?', 'author_fullname': 't2_8q4ttq1x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering Full Structured Course', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eghkii', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 25, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 25, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722412739.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Can anyone please help me to get one full structured data engineering course?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eghkii', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pollymorfism'), 'discussion_type': None, 'num_comments': 33, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eghkii/data_engineering_full_structured_course/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eghkii/data_engineering_full_structured_course/', 'subreddit_subscribers': 201126, 'created_utc': 1722412739.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.786+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, how many of you run Airbyte in production? And how do you run it? (with Kubernetes? Their cloud offer?). \n\n1. I am interested for 2 reasons: to use it for our company\n2. because one of our product users [asked for advice](https://discuss.qovery.com/t/help-setting-up-airbyte-and-using-kubernetes-secrets/2848) running Airbyte in production\n\n  \nThank you', 'author_fullname': 't2_1noqay5b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Do you run Airbyte in production?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eclb9x', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 24, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 24, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721991875.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, how many of you run Airbyte in production? And how do you run it? (with Kubernetes? Their cloud offer?). </p>\n\n<ol>\n<li>I am interested for 2 reasons: to use it for our company</li>\n<li>because one of our product users <a href="https://discuss.qovery.com/t/help-setting-up-airbyte-and-using-kubernetes-secrets/2848">asked for advice</a> running Airbyte in production</li>\n</ol>\n\n<p>Thank you</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eclb9x', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ev0xmusic'), 'discussion_type': None, 'num_comments': 32, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eclb9x/do_you_run_airbyte_in_production/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eclb9x/do_you_run_airbyte_in_production/', 'subreddit_subscribers': 201126, 'created_utc': 1721991875.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.786+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all, \n\nI have a CEO meeting with a startup I am really interested in, however, I am in the middle of a major data migration project with my current company, they highly depend on me and I canâ€™t let them down by leaving now. \n\nHow can I highlight this in a way that doesnâ€™t make me lose this potential new role Iâ€™ve been trying to get into since top of year? \n\nThereâ€™re no restrictions to leaving my current company, I can submit resignation anytime, it just feels wrong.\n\nAlso, Iâ€™d be transitioning from a large corporate to a startup that has about 10 employees (though the role is much more interesting than my current), so staying with my current company at least to the end of this project (expected top of next fiscal year) would be lower risk.', 'author_fullname': 't2_6or8m0hm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How can I discuss working with two companies? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ee7y1k', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 23, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722174841.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all, </p>\n\n<p>I have a CEO meeting with a startup I am really interested in, however, I am in the middle of a major data migration project with my current company, they highly depend on me and I canâ€™t let them down by leaving now. </p>\n\n<p>How can I highlight this in a way that doesnâ€™t make me lose this potential new role Iâ€™ve been trying to get into since top of year? </p>\n\n<p>Thereâ€™re no restrictions to leaving my current company, I can submit resignation anytime, it just feels wrong.</p>\n\n<p>Also, Iâ€™d be transitioning from a large corporate to a startup that has about 10 employees (though the role is much more interesting than my current), so staying with my current company at least to the end of this project (expected top of next fiscal year) would be lower risk.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ee7y1k', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Judessaa'), 'discussion_type': None, 'num_comments': 45, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ee7y1k/how_can_i_discuss_working_with_two_companies/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ee7y1k/how_can_i_discuss_working_with_two_companies/', 'subreddit_subscribers': 201126, 'created_utc': 1722174841.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.787+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone, i have a question about what is the best way to load data from S3 to snowflake and truncate the data after the ingestion is successful.. \nIt will be a continues job runs every 4 hours and data is staged to s3 using lambda function \n\nI don't want to use snowpipe because it's not truncating the data so what is the best approach? ", 'author_fullname': 't2_dm09aec1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'S3 to snowflake', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egxl1n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722457932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone, i have a question about what is the best way to load data from S3 to snowflake and truncate the data after the ingestion is successful.. \nIt will be a continues job runs every 4 hours and data is staged to s3 using lambda function </p>\n\n<p>I don&#39;t want to use snowpipe because it&#39;s not truncating the data so what is the best approach? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egxl1n', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mountain-Luck7673'), 'discussion_type': None, 'num_comments': 20, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egxl1n/s3_to_snowflake/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egxl1n/s3_to_snowflake/', 'subreddit_subscribers': 201126, 'created_utc': 1722457932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.787+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'll start: Blob", 'author_fullname': 't2_xwr1wuedn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What are data engineering tools/systems that have a funny name?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh4dyn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722475832.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ll start: Blob</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1eh4dyn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='inner-musician-5457'), 'discussion_type': None, 'num_comments': 36, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh4dyn/what_are_data_engineering_toolssystems_that_have/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh4dyn/what_are_data_engineering_toolssystems_that_have/', 'subreddit_subscribers': 201126, 'created_utc': 1722475832.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.787+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello, I have a design question about a current project of mine. First of all: I am unfortunately not an architect, which is why I find it somewhat difficult to develop a system that is scalable and does not collapse under load. That's why I just wanted to ask here, as I'm sure there are others who have more experience in this area than I do.\n\nAbout my project. I want to build a scalable rule engine. I have various services that publish events. These events range from messages to simple numerical values that change over time and thus trigger an event. Users can now create their own alerts based on such events, based on a json rule engine. The only sticking point. Additional data modules can be added to these alarms, adding data to the events, such as general aggregations over a certain period of time, etc. This means that, in the worst case, each alarm created by a user is unique and must be processed separately. The rule engine then checks the rules against the assembled json input. The bottleneck of the whole application lies in the processing of the individual alarms and the enrichment of the events with the respective data modules.\n\nDoes anyone have any ideas on how to make this performant and scalable? The system should not take longer with an increasing number of alarms created. This means that millions of alarms should be processed. Of course, you can't do this with one server, but with several and load balancing.\n\nMy idea for the whole thing would be that events arrive in the Event Service via Pub/Sub. This service first stores the event in the Enrichment Service and performs any previous aggregations so that it does not have to perform repeated calculations. Subsequently, the alarm rules for an event type are loaded from the database in the Event Service and distributed to the rule engine workers, which then process the individual alarms. The Rule Engine Services retrieve the additional information defined by the user from the Enrichment Service which has caching via Redis and then evaluate the input based on the rules they have created.\nIf the rule is correct, an email, SMS, etc. is sent.", 'author_fullname': 't2_jxkiu4d9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Building a scalable alarm rule engine', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 81, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egly1c', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tSYQT1X8hEhrrFmQ6tBmH1QuKl2s6GA6cb4dnlKAiwY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722429007.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello, I have a design question about a current project of mine. First of all: I am unfortunately not an architect, which is why I find it somewhat difficult to develop a system that is scalable and does not collapse under load. That&#39;s why I just wanted to ask here, as I&#39;m sure there are others who have more experience in this area than I do.</p>\n\n<p>About my project. I want to build a scalable rule engine. I have various services that publish events. These events range from messages to simple numerical values that change over time and thus trigger an event. Users can now create their own alerts based on such events, based on a json rule engine. The only sticking point. Additional data modules can be added to these alarms, adding data to the events, such as general aggregations over a certain period of time, etc. This means that, in the worst case, each alarm created by a user is unique and must be processed separately. The rule engine then checks the rules against the assembled json input. The bottleneck of the whole application lies in the processing of the individual alarms and the enrichment of the events with the respective data modules.</p>\n\n<p>Does anyone have any ideas on how to make this performant and scalable? The system should not take longer with an increasing number of alarms created. This means that millions of alarms should be processed. Of course, you can&#39;t do this with one server, but with several and load balancing.</p>\n\n<p>My idea for the whole thing would be that events arrive in the Event Service via Pub/Sub. This service first stores the event in the Enrichment Service and performs any previous aggregations so that it does not have to perform repeated calculations. Subsequently, the alarm rules for an event type are loaded from the database in the Event Service and distributed to the rule engine workers, which then process the individual alarms. The Rule Engine Services retrieve the additional information defined by the user from the Enrichment Service which has caching via Redis and then evaluate the input based on the rules they have created.\nIf the rule is correct, an email, SMS, etc. is sent.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/vmlh50hrlufd1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/vmlh50hrlufd1.png?auto=webp&s=0fdc27832e3fbd17aff5392e7ca589d55e45ccb6', 'width': 1207, 'height': 701}, 'resolutions': [{'url': 'https://preview.redd.it/vmlh50hrlufd1.png?width=108&crop=smart&auto=webp&s=3c12c1b54e89349032688b39ea6bb9bd026b3321', 'width': 108, 'height': 62}, {'url': 'https://preview.redd.it/vmlh50hrlufd1.png?width=216&crop=smart&auto=webp&s=b8eed416f26ad503880d70d2b301b99b744631bf', 'width': 216, 'height': 125}, {'url': 'https://preview.redd.it/vmlh50hrlufd1.png?width=320&crop=smart&auto=webp&s=f70742dc4f7a01efe1d0c33b8c4eac10470c2b88', 'width': 320, 'height': 185}, {'url': 'https://preview.redd.it/vmlh50hrlufd1.png?width=640&crop=smart&auto=webp&s=24a2455f0f8de6e85de1006be554064238ed6e0a', 'width': 640, 'height': 371}, {'url': 'https://preview.redd.it/vmlh50hrlufd1.png?width=960&crop=smart&auto=webp&s=7dd2e55bd866311cba31ae34d7fdad3e2ab7a227', 'width': 960, 'height': 557}, {'url': 'https://preview.redd.it/vmlh50hrlufd1.png?width=1080&crop=smart&auto=webp&s=ad7beca84a575a5ee0c33e7a44c8a27764db830e', 'width': 1080, 'height': 627}], 'variants': {}, 'id': 'gV1GZlktW_o4bVl2dnBk1ZMXWnSVkKSeIYuAGuH2mco'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1egly1c', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='_noctera_'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egly1c/building_a_scalable_alarm_rule_engine/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/vmlh50hrlufd1.png', 'subreddit_subscribers': 201126, 'created_utc': 1722429007.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.788+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I keep getting asked to produce this.  They want to ***see*** more than what DBT docs can provide.\n\nThe particular hop they want to see has about 100 tables but all combined 1300 relationships.  Reciprocating relationships have been removed, so these are unique.  And this is where every diagraming tool I've tried just falls apart.  The relationship connectors end up making the diagram overloaded and useless.  There are future conversations about reducing this into groups of tables but not until I exhaust all resources on their big want.", 'author_fullname': 't2_33sjvqg2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you use to create very large ER diagrams?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ege8i0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722399904.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I keep getting asked to produce this.  They want to <strong><em>see</em></strong> more than what DBT docs can provide.</p>\n\n<p>The particular hop they want to see has about 100 tables but all combined 1300 relationships.  Reciprocating relationships have been removed, so these are unique.  And this is where every diagraming tool I&#39;ve tried just falls apart.  The relationship connectors end up making the diagram overloaded and useless.  There are future conversations about reducing this into groups of tables but not until I exhaust all resources on their big want.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ege8i0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Captain_Coffee_III'), 'discussion_type': None, 'num_comments': 25, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ege8i0/what_do_you_use_to_create_very_large_er_diagrams/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ege8i0/what_do_you_use_to_create_very_large_er_diagrams/', 'subreddit_subscribers': 201126, 'created_utc': 1722399904.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.788+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello all and Happy Friday!\n\n  \nI have been working on spark as a junior for about a year now. I am fairly well versed with the dataframe API and understand the theory on, for example, shuffles during wide transformations thought its all very basic in my opinion.\n\nI read existing code and have read spark architecture related blogs/articles and can recite out explanations of shuffle, partitions, repartitioning, etc but I don't have seem to have developed intuition regarding when to use Spark's features to properly optimise my code.\n\n  \nHow did you all go about increasing your working knowledge? Any advice would be greatly appreciated. Many thanks!", 'author_fullname': 't2_chptq8qwh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do you develop intuition regarding HOW to use Spark/Pyspark?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ech7fq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 19, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 19, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721975278.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello all and Happy Friday!</p>\n\n<p>I have been working on spark as a junior for about a year now. I am fairly well versed with the dataframe API and understand the theory on, for example, shuffles during wide transformations thought its all very basic in my opinion.</p>\n\n<p>I read existing code and have read spark architecture related blogs/articles and can recite out explanations of shuffle, partitions, repartitioning, etc but I don&#39;t have seem to have developed intuition regarding when to use Spark&#39;s features to properly optimise my code.</p>\n\n<p>How did you all go about increasing your working knowledge? Any advice would be greatly appreciated. Many thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ech7fq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='jnrdataengineer2023'), 'discussion_type': None, 'num_comments': 27, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ech7fq/how_do_you_develop_intuition_regarding_how_to_use/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ech7fq/how_do_you_develop_intuition_regarding_how_to_use/', 'subreddit_subscribers': 201126, 'created_utc': 1721975278.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.789+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_c1uwm29g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How top data teams are structured', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efs0bp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 18, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/gV-FOrVPcXRpDUMFQvP_svvEX-DEvVN8T7vehmrd5VU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722342317.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'mikkeldengsoe.substack.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://mikkeldengsoe.substack.com/p/how-top-data-teams-are-structured', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?auto=webp&s=b12953aaa819834c242798328a0418a4246b6cc4', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?width=108&crop=smart&auto=webp&s=a119dcf0e3f6c254daa20fbb88989fe1f75a56f5', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?width=216&crop=smart&auto=webp&s=5b6cec8c3a14a8d952bbd745aa11c0b257fbe144', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?width=320&crop=smart&auto=webp&s=e2ffbad4e08a43ecfdf3de398041fe767d82d0f6', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?width=640&crop=smart&auto=webp&s=d4e80f0cefa4a60088551b38474998bf955dc2d7', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?width=960&crop=smart&auto=webp&s=1994a80004e3c4eb6e5897851af0454f4546d923', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/SLb59fxe_IjbFlNUgv_2wNzq_r1LYprWXtxkaz9rFvM.jpg?width=1080&crop=smart&auto=webp&s=4ad6cea283970d29f41b3c8021cec16daa095411', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Gh4mhqitINhdKVMVeQopi8_PdYARVKn-Nol8zW4fHPY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1efs0bp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Different_Eggplant97'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efs0bp/how_top_data_teams_are_structured/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://mikkeldengsoe.substack.com/p/how-top-data-teams-are-structured', 'subreddit_subscribers': 201126, 'created_utc': 1722342317.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.790+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hiho!\n\nI am responsible for the E2E analytics flow at my current company, so I get to choose tooling, build the ETL and perform the load testing for our analytics use cases. We've been having performance issues with Redshift and I'll try to detail where we currently are and what our use case is.\n\n&#x200B;\n\nBasically we have a Cube.js semantics layer that provides an API endpoint to which we can send requests. These requests have a payload which is converted to raw SQL and ran on whatever compute backend configured (currently Redshift is the data warehouse), then the results are returned. This works flawlessly with our React app frontend dashboards: we just use highcharts and fetch data from this Cube.js API, which in turn fetches data from the warehouse.\n\nAgain, this works fine for most use cases, but we now have a filter panel in the React app which is used  by users to well... filter the data. This also WOULD work fine as all tables are pretty well optimized and rather small, BUT... Redshift has this [query compilation](https://aws.amazon.com/blogs/big-data/fast-and-predictable-performance-with-serverless-compilation-using-amazon-redshift/) feature in which unique queries are compiled first before saved on a long-lived cache, so the first time you run an unique query (a query that never ran before) it's performance will be SLOW. [Here](https://www.redshiftresearchproject.org/white_papers/downloads/query_compilation.pdf)'s another whitepaper reference. And here's a sample of such behavior:\n\n[This one query was compiled into 7 segments, and one of them took 6.8s to compile. Total query execution time was 7.6s, so it spent 90&#37; of time runtime compiling, and only 10&#37; \\(less than a second\\) actually calculating results](https://preview.redd.it/d3rxz57hdved1.png?width=1477&format=png&auto=webp&s=cfa04598fdd88ee7755ac86f55e9536d8b03154d)\n\nAs you might guess, this is AWFUL for UX, and since we have the filter panel, there is a massive combination of unique queries each user can make (just do combinatorial analysis for 6 filter dimensions and tens of options each, the number is massive!). Further executions of the same user with a given filter combinations work fine because we have the Cube.js cache, the Redshift results cache, and the query will already be compile in the query cache, it's just the first execution that is awful. Some queries take upwards of 40s to compile (and less than a second to then run)!\n\nIf you're curious, this query above was something like this:\n\n    /* RQEV2-ZiBe1KuqlJ */\n    SELECT\n      dt1.name AS name,\n      AVG(\n        ROUND(\n          CASE\n            WHEN f.some_field > 0.0000 THEN 1.0000\n            ELSE 0.0000\n          END,\n          4\n        )\n      ) AS metric_percentage\n    FROM\n      fact_tbl AS f\n      LEFT JOIN dim_tbl1 AS dt1 ON dt1.b_id = f.b_id\n      LEFT JOIN dim_tbl2 AS dt2 ON dt2.tenant_id = f.tenant_id\n      LEFT JOIN dim_tbl3 AS dt3 ON dt3.l_id = f.l_id \n    WHERE 1 = 1\n      AND (dt2.tenant_id IN ('00000000-0000-0000-0000-000000000000', 'e6df549b-d704c6c-8848-679ab8a4b87d'))\n      AND (dt3.l_id IN ('A', 'B', 'C'))\n    GROUP BY\n      1\n    ORDER BY\n      2 ASC\n    LIMIT\n      1000\n\nwhile the fact\\_tbl is around 10M rows, dt1 is 5k rows, dt2 is 150 rows and dt3 is 5 rows. All IDs are proper DISTKEYs and/or SORTKEYs. Again this took 7.6s, of which 6.8s was compiling and less than a second executing.\n\n&#x200B;\n\nAs such, I think there is no solution other than migrate tooling. Redshift just wasn't built for this. I tried devising some workaround to avoid the query compile, and it kind of works but it is absolute jank. So I turn to the community to see if anyone has any real world experience building this kind of application. Pretty much:\n\n* Hundreds of concurrent users, each user visits a dashboard page which can trigger \\~20 queries each time the page is visited;\n* Queries are SELECTs with restrictive filters (such as tenant\\_id) and JOINs between them with further aggregations with GROUP BY. All tables are properly DISTKEYd and SORTKEYd in Redshift (basically well indexed on the necessary fields);\n* As such, all queries run are very performant, but there are many queries being executed at a time, so I need high concurrency;\n* The saving grace is: **our data volume is small, the largest fact tables are up to 20 million rows at most**, and they get immediately filtered down to <500k due to the tenant\\_id filter. Dimensional tables are also small, with at most 5k rows.\n* I need to return results for these queries in <3s. Ideally <2s.\n* New data is NOT updated in real time. Maybe doing a MERGE twice or 3 times a day in the fact and dims would suffice. Currently we have a replica of the transactional DB in Redshift and build materialized views as fact and dimension tables on top of the replicas. Seems to work really well if it wasn't for the compilation issue: when a source table is updated, Redshift auto-refreshes the matviews as soon as possible!\n\n&#x200B;\n\nRedshift would be a perfect match for this if it wasn't for the compilation feature. Recently we had a concurrent load of 60 users and the Redshift cluster (1 leader/3 compute nodes *dc2.large*) behaved like this:\n\n* Compute nodes were at <30% CPU usage, so compute wasn't the issue;\n* Leader node was capped at 100% CPU usage. The leader node is the one that receives queries and handles compilation offload/inload to and from the separate compilation farm mentioned in the [docs](https://aws.amazon.com/blogs/big-data/fast-and-predictable-performance-with-serverless-compilation-using-amazon-redshift/) (and I think the leader can only retrieve one segment at a time!);\n* Dashboards felt REALLY unresponsive.\n\n&#x200B;\n\nAnyways, based on these requirements, I tried migrating the fact and dimensional tables to a 2vCPU, 4GB RAM RDS Postgres instance (data volume is small, maybe don't even need OLAP) and the performance seems great: no more 40s compilation times and the queries are executed pretty quickly, so the dashboards feel very responsive. However I am afraid of how well will PG scale: data volume I think it would be fine, but concurrency could become an issue, right? Running thousands of\n\n    SELECT\n      f.field1,\n      f.field2,\n      f.field3,\n      AVG(\n        ROUND(\n          CASE\n            WHEN field4 > 0.0000 THEN 1.0000\n            ELSE 0.0000\n          END,\n          4\n        )\n      ) AS some_metric\n    FROM\n      fact_table f\n      LEFT JOIN dim_tbl  ON fact_table.indexed_field  = dim_tbl.indexed_field\n      LEFT JOIN dim_tbl2 ON fact_table.tenant_id      = dim_tbl2.tenant_id \n    WHERE 1 = 1\n      AND dim_tbl2.tenant_id = 'some_indexed_uuid'\n    GROUP BY\n      1,\n      2,\n      3\n    ORDER BY\n      2 ASC\n    LIMIT\n      5000\n\neach with different filters and joins and tables concurrently could be an issue, me thinks. I'm building a load test framework at the moment to assess how well it will perform, and in the meantime discuss other possibilities.\n\n&#x200B;\n\nDo you have any thoughts or ideas? My ultimate attempt would be to migrate from Redshift or the PG instance to something like BigQuery, but we're primarily an AWS shop (using AWS Glue for ETL, EC2, Redshift, etc.), so I'd like to avoid BQ if there are other options. Would MotherDuck fit this use case? Snowflake is very expensive so that is out of the question...\n\n&#x200B;\n\nThanks!", 'author_fullname': 't2_bask6zu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Suggestions on a backend datastore/compute engine for analytical dashboard with ~100 concurrent users', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 91, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'d3rxz57hdved1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=108&crop=smart&auto=webp&s=360f021c5d2a1a6a7653b49ec818d5a2f281194f'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=216&crop=smart&auto=webp&s=6785f05862d9eca9d11846b877bd25d6e8d6efef'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=320&crop=smart&auto=webp&s=bffab2dd3f87c8a96c1a1b7a9cc947af1a4e9001'}, {'y': 481, 'x': 640, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=640&crop=smart&auto=webp&s=a27e8b3c070c8f7d8330deeeab54e22bdc7f7b50'}, {'y': 722, 'x': 960, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=960&crop=smart&auto=webp&s=ba2b9bd5fd38e8102bd6cb7fafe19b6bd5188611'}, {'y': 812, 'x': 1080, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=1080&crop=smart&auto=webp&s=f177c395cfcec39ebf420de6b3a2a6f31c609147'}], 's': {'y': 1111, 'x': 1477, 'u': 'https://preview.redd.it/d3rxz57hdved1.png?width=1477&format=png&auto=webp&s=cfa04598fdd88ee7755ac86f55e9536d8b03154d'}, 'id': 'd3rxz57hdved1'}}, 'name': 't3_1ecpfd7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 18, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/dctqxDwLbWD3QmfkjzxTihkiNOajOVmXNWLdtiMN9Wc.jpg', 'edited': 1722006342.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'subreddit_type': 'public', 'created': 1722003983.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hiho!</p>\n\n<p>I am responsible for the E2E analytics flow at my current company, so I get to choose tooling, build the ETL and perform the load testing for our analytics use cases. We&#39;ve been having performance issues with Redshift and I&#39;ll try to detail where we currently are and what our use case is.</p>\n\n<p>&#x200B;</p>\n\n<p>Basically we have a Cube.js semantics layer that provides an API endpoint to which we can send requests. These requests have a payload which is converted to raw SQL and ran on whatever compute backend configured (currently Redshift is the data warehouse), then the results are returned. This works flawlessly with our React app frontend dashboards: we just use highcharts and fetch data from this Cube.js API, which in turn fetches data from the warehouse.</p>\n\n<p>Again, this works fine for most use cases, but we now have a filter panel in the React app which is used  by users to well... filter the data. This also WOULD work fine as all tables are pretty well optimized and rather small, BUT... Redshift has this <a href="https://aws.amazon.com/blogs/big-data/fast-and-predictable-performance-with-serverless-compilation-using-amazon-redshift/">query compilation</a> feature in which unique queries are compiled first before saved on a long-lived cache, so the first time you run an unique query (a query that never ran before) it&#39;s performance will be SLOW. <a href="https://www.redshiftresearchproject.org/white_papers/downloads/query_compilation.pdf">Here</a>&#39;s another whitepaper reference. And here&#39;s a sample of such behavior:</p>\n\n<p><a href="https://preview.redd.it/d3rxz57hdved1.png?width=1477&amp;format=png&amp;auto=webp&amp;s=cfa04598fdd88ee7755ac86f55e9536d8b03154d">This one query was compiled into 7 segments, and one of them took 6.8s to compile. Total query execution time was 7.6s, so it spent 90&#37; of time runtime compiling, and only 10&#37; (less than a second) actually calculating results</a></p>\n\n<p>As you might guess, this is AWFUL for UX, and since we have the filter panel, there is a massive combination of unique queries each user can make (just do combinatorial analysis for 6 filter dimensions and tens of options each, the number is massive!). Further executions of the same user with a given filter combinations work fine because we have the Cube.js cache, the Redshift results cache, and the query will already be compile in the query cache, it&#39;s just the first execution that is awful. Some queries take upwards of 40s to compile (and less than a second to then run)!</p>\n\n<p>If you&#39;re curious, this query above was something like this:</p>\n\n<pre><code>/* RQEV2-ZiBe1KuqlJ */\nSELECT\n  dt1.name AS name,\n  AVG(\n    ROUND(\n      CASE\n        WHEN f.some_field &gt; 0.0000 THEN 1.0000\n        ELSE 0.0000\n      END,\n      4\n    )\n  ) AS metric_percentage\nFROM\n  fact_tbl AS f\n  LEFT JOIN dim_tbl1 AS dt1 ON dt1.b_id = f.b_id\n  LEFT JOIN dim_tbl2 AS dt2 ON dt2.tenant_id = f.tenant_id\n  LEFT JOIN dim_tbl3 AS dt3 ON dt3.l_id = f.l_id \nWHERE 1 = 1\n  AND (dt2.tenant_id IN (&#39;00000000-0000-0000-0000-000000000000&#39;, &#39;e6df549b-d704c6c-8848-679ab8a4b87d&#39;))\n  AND (dt3.l_id IN (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;))\nGROUP BY\n  1\nORDER BY\n  2 ASC\nLIMIT\n  1000\n</code></pre>\n\n<p>while the fact_tbl is around 10M rows, dt1 is 5k rows, dt2 is 150 rows and dt3 is 5 rows. All IDs are proper DISTKEYs and/or SORTKEYs. Again this took 7.6s, of which 6.8s was compiling and less than a second executing.</p>\n\n<p>&#x200B;</p>\n\n<p>As such, I think there is no solution other than migrate tooling. Redshift just wasn&#39;t built for this. I tried devising some workaround to avoid the query compile, and it kind of works but it is absolute jank. So I turn to the community to see if anyone has any real world experience building this kind of application. Pretty much:</p>\n\n<ul>\n<li>Hundreds of concurrent users, each user visits a dashboard page which can trigger ~20 queries each time the page is visited;</li>\n<li>Queries are SELECTs with restrictive filters (such as tenant_id) and JOINs between them with further aggregations with GROUP BY. All tables are properly DISTKEYd and SORTKEYd in Redshift (basically well indexed on the necessary fields);</li>\n<li>As such, all queries run are very performant, but there are many queries being executed at a time, so I need high concurrency;</li>\n<li>The saving grace is: <strong>our data volume is small, the largest fact tables are up to 20 million rows at most</strong>, and they get immediately filtered down to &lt;500k due to the tenant_id filter. Dimensional tables are also small, with at most 5k rows.</li>\n<li>I need to return results for these queries in &lt;3s. Ideally &lt;2s.</li>\n<li>New data is NOT updated in real time. Maybe doing a MERGE twice or 3 times a day in the fact and dims would suffice. Currently we have a replica of the transactional DB in Redshift and build materialized views as fact and dimension tables on top of the replicas. Seems to work really well if it wasn&#39;t for the compilation issue: when a source table is updated, Redshift auto-refreshes the matviews as soon as possible!</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p>Redshift would be a perfect match for this if it wasn&#39;t for the compilation feature. Recently we had a concurrent load of 60 users and the Redshift cluster (1 leader/3 compute nodes <em>dc2.large</em>) behaved like this:</p>\n\n<ul>\n<li>Compute nodes were at &lt;30% CPU usage, so compute wasn&#39;t the issue;</li>\n<li>Leader node was capped at 100% CPU usage. The leader node is the one that receives queries and handles compilation offload/inload to and from the separate compilation farm mentioned in the <a href="https://aws.amazon.com/blogs/big-data/fast-and-predictable-performance-with-serverless-compilation-using-amazon-redshift/">docs</a> (and I think the leader can only retrieve one segment at a time!);</li>\n<li>Dashboards felt REALLY unresponsive.</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p>Anyways, based on these requirements, I tried migrating the fact and dimensional tables to a 2vCPU, 4GB RAM RDS Postgres instance (data volume is small, maybe don&#39;t even need OLAP) and the performance seems great: no more 40s compilation times and the queries are executed pretty quickly, so the dashboards feel very responsive. However I am afraid of how well will PG scale: data volume I think it would be fine, but concurrency could become an issue, right? Running thousands of</p>\n\n<pre><code>SELECT\n  f.field1,\n  f.field2,\n  f.field3,\n  AVG(\n    ROUND(\n      CASE\n        WHEN field4 &gt; 0.0000 THEN 1.0000\n        ELSE 0.0000\n      END,\n      4\n    )\n  ) AS some_metric\nFROM\n  fact_table f\n  LEFT JOIN dim_tbl  ON fact_table.indexed_field  = dim_tbl.indexed_field\n  LEFT JOIN dim_tbl2 ON fact_table.tenant_id      = dim_tbl2.tenant_id \nWHERE 1 = 1\n  AND dim_tbl2.tenant_id = &#39;some_indexed_uuid&#39;\nGROUP BY\n  1,\n  2,\n  3\nORDER BY\n  2 ASC\nLIMIT\n  5000\n</code></pre>\n\n<p>each with different filters and joins and tables concurrently could be an issue, me thinks. I&#39;m building a load test framework at the moment to assess how well it will perform, and in the meantime discuss other possibilities.</p>\n\n<p>&#x200B;</p>\n\n<p>Do you have any thoughts or ideas? My ultimate attempt would be to migrate from Redshift or the PG instance to something like BigQuery, but we&#39;re primarily an AWS shop (using AWS Glue for ETL, EC2, Redshift, etc.), so I&#39;d like to avoid BQ if there are other options. Would MotherDuck fit this use case? Snowflake is very expensive so that is out of the question...</p>\n\n<p>&#x200B;</p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/OjU-b_ch447l4asp1G0bRYyDXUhezA0Uh_-vc2xDHKM.jpg?auto=webp&s=72d62b120e0361eb3b031c24ef8662fbfc831d01', 'width': 800, 'height': 523}, 'resolutions': [{'url': 'https://external-preview.redd.it/OjU-b_ch447l4asp1G0bRYyDXUhezA0Uh_-vc2xDHKM.jpg?width=108&crop=smart&auto=webp&s=c0f3014efe611d68a87637376ef9c15dc20c23d0', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/OjU-b_ch447l4asp1G0bRYyDXUhezA0Uh_-vc2xDHKM.jpg?width=216&crop=smart&auto=webp&s=5569c0cbb911ab416779d929ecc0754d0b0c85dc', 'width': 216, 'height': 141}, {'url': 'https://external-preview.redd.it/OjU-b_ch447l4asp1G0bRYyDXUhezA0Uh_-vc2xDHKM.jpg?width=320&crop=smart&auto=webp&s=05dec4aaf8199f12a00f5d20808eaf0fbde10665', 'width': 320, 'height': 209}, {'url': 'https://external-preview.redd.it/OjU-b_ch447l4asp1G0bRYyDXUhezA0Uh_-vc2xDHKM.jpg?width=640&crop=smart&auto=webp&s=c834813d528e08eb778cdbfea31768c3b2fbe7b0', 'width': 640, 'height': 418}], 'variants': {}, 'id': 'IUnVFDRB6oeHGKKDUfEym-RPQ5BGp_p8X1zeRTdUnkE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecpfd7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Beauty_Fades'), 'discussion_type': None, 'num_comments': 26, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecpfd7/suggestions_on_a_backend_datastorecompute_engine/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecpfd7/suggestions_on_a_backend_datastorecompute_engine/', 'subreddit_subscribers': 201126, 'created_utc': 1722003983.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.791+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Is it common for people to use it in production? Iâ€™ve seen most people mention it for local processing but I think it would be interesting to set it up in a server but Iâ€™m not sure if this a normal thing people do. \nIf you happen to do so, please enlighten me', 'author_fullname': 't2_6b3yqctf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DuckDB in production ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh4hrh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 16, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722476145.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Is it common for people to use it in production? Iâ€™ve seen most people mention it for local processing but I think it would be interesting to set it up in a server but Iâ€™m not sure if this a normal thing people do. \nIf you happen to do so, please enlighten me</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eh4hrh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Snoo_70708'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh4hrh/duckdb_in_production/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh4hrh/duckdb_in_production/', 'subreddit_subscribers': 201126, 'created_utc': 1722476145.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.791+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In the era of big data, efficient data preparation and analytics are essential for deriving actionable insights. This tutorial demonstrates using Pathway for the ETL process, Delta Lake for efficient data storage, and Apache Spark for data analytics, making it highly relevant for data engineers looking to integrate data from various new sources and efficiently process it within the Spark ecosystem.\n\n**Comprehensive guide with code:** [https://pathway.com/developers/templates/delta\\_lake\\_etl](https://pathway.com/developers/templates/delta_lake_etl)\n\n**Why This Approach Works:**\n\n* **Versatile Data Integration:** Pathwayâ€™s Airbyte connector allows you to ingest data from any data system, be it GitHub or Salesforce, and store it in Delta Lake.\n* **Seamless Pipeline Integration:** Expand your data pipeline effortlessly by adding new data sources without significantly changing them. \n* **Optimized Data Storage:** Querying over data organized in Delta Lake is faster, enabling efficient data processing with Spark. Delta Lakeâ€™s scalable metadata handling and time travel support make it easy to access and query previous versions of data.\n\nUsing Pathway for Delta ETL simplifies these tasks significantly:\n\n* **Extract:** Use Airbyte to gather data from sources like GitHub, configuring it to specify exactly what data you need, such as commit history from a repository.\n* **Transform:** Pathway helps remove sensitive information and prepare data for analysis. Additionally, you can add useful information, such as the username of the person who made changes and the time of the changes.\n* **Load:** The cleaned data is then saved into Delta Lake, which can be stored on your local system or in the cloud (e.g., S3) for efficient storage and analysis with Spark.\n\nWould love to hear your experiences with these tools in your data engineering workflows!', 'author_fullname': 't2_11kkqkf962', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Implementation Guide for Delta ETL for Spark Analytics', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egnmxy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722433702.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In the era of big data, efficient data preparation and analytics are essential for deriving actionable insights. This tutorial demonstrates using Pathway for the ETL process, Delta Lake for efficient data storage, and Apache Spark for data analytics, making it highly relevant for data engineers looking to integrate data from various new sources and efficiently process it within the Spark ecosystem.</p>\n\n<p><strong>Comprehensive guide with code:</strong> <a href="https://pathway.com/developers/templates/delta_lake_etl">https://pathway.com/developers/templates/delta_lake_etl</a></p>\n\n<p><strong>Why This Approach Works:</strong></p>\n\n<ul>\n<li><strong>Versatile Data Integration:</strong> Pathwayâ€™s Airbyte connector allows you to ingest data from any data system, be it GitHub or Salesforce, and store it in Delta Lake.</li>\n<li><strong>Seamless Pipeline Integration:</strong> Expand your data pipeline effortlessly by adding new data sources without significantly changing them. </li>\n<li><strong>Optimized Data Storage:</strong> Querying over data organized in Delta Lake is faster, enabling efficient data processing with Spark. Delta Lakeâ€™s scalable metadata handling and time travel support make it easy to access and query previous versions of data.</li>\n</ul>\n\n<p>Using Pathway for Delta ETL simplifies these tasks significantly:</p>\n\n<ul>\n<li><strong>Extract:</strong> Use Airbyte to gather data from sources like GitHub, configuring it to specify exactly what data you need, such as commit history from a repository.</li>\n<li><strong>Transform:</strong> Pathway helps remove sensitive information and prepare data for analysis. Additionally, you can add useful information, such as the username of the person who made changes and the time of the changes.</li>\n<li><strong>Load:</strong> The cleaned data is then saved into Delta Lake, which can be stored on your local system or in the cloud (e.g., S3) for efficient storage and analysis with Spark.</li>\n</ul>\n\n<p>Would love to hear your experiences with these tools in your data engineering workflows!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?auto=webp&s=0728304b83e1a234eb9576307dde7be8ad6cfe14', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?width=108&crop=smart&auto=webp&s=b486b2b81da3dc9064e42285c7321ec6e89ea5bc', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?width=216&crop=smart&auto=webp&s=a3896c1aab983c132ea404f011b6f97f072e3228', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?width=320&crop=smart&auto=webp&s=3341fc4c7ac0d01af2442b86ec8866916c2b3a68', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?width=640&crop=smart&auto=webp&s=56d8107ef10c06ad2e887080ebdab19c52079d1c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?width=960&crop=smart&auto=webp&s=ea97070df16386973060c87702a4cf28ab729d71', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/Zc62crkM_IupXM6jMrjxulQqHciIm3pE7KXm5n1C7pg.jpg?width=1080&crop=smart&auto=webp&s=02e960331b7c2e45c271e2f384bad74424baaa3e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '70SuKf2MGtEV_tx6g4QLxJ1u43ogfJMjCjNI10eJgfQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1egnmxy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Typical-Scene-5794'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egnmxy/implementation_guide_for_delta_etl_for_spark/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egnmxy/implementation_guide_for_delta_etl_for_spark/', 'subreddit_subscribers': 201126, 'created_utc': 1722433702.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.792+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'How many stages were involved in the process? What sort of questions were you asked?', 'author_fullname': 't2_2zfz8duf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "What is the easiest experience you've had applying for a data engineer job?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehhr08', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722521674.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>How many stages were involved in the process? What sort of questions were you asked?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehhr08', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Novel-Tree7557'), 'discussion_type': None, 'num_comments': 34, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehhr08/what_is_the_easiest_experience_youve_had_applying/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehhr08/what_is_the_easiest_experience_youve_had_applying/', 'subreddit_subscribers': 201126, 'created_utc': 1722521674.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.792+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am a junior big data engineer in Poland, in a few days i will be assigned to a project with the following stack (Nifi, Spark, Hive-impala, ozzie) but the nature of the project that is Nifi take the most weight of the project and there is a third party developing spark and i have concerns regarding that i might be tool oriented rather than developer, i need an advice that shall i try to join project that i develop spark myself or reviewing those third party scripts and optimize them would be enough. ', 'author_fullname': 't2_v0uh4ksy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is Nifi that Important ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehc2rt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 16, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722503274.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am a junior big data engineer in Poland, in a few days i will be assigned to a project with the following stack (Nifi, Spark, Hive-impala, ozzie) but the nature of the project that is Nifi take the most weight of the project and there is a third party developing spark and i have concerns regarding that i might be tool oriented rather than developer, i need an advice that shall i try to join project that i develop spark myself or reviewing those third party scripts and optimize them would be enough. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehc2rt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ancient_Quiet_790'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehc2rt/is_nifi_that_important/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehc2rt/is_nifi_that_important/', 'subreddit_subscribers': 201126, 'created_utc': 1722503274.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.793+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have a small project running in a docker container in my local machine. I got airflow orchestrating a script which dumps data into *** and then some dbt models going on after that. I eventually want to host this somewhere and keep it running. The script is light, all I have is a few API calls, a few insert and select queries. Less than 10k rows.\n\nIâ€™m trying to do the same thing Dagster, just to get some experience with it. And if I did have everything inside Dagster, would I be able to run this in a cloud instance with 1 gb memory? I tried with airflow and it wouldnâ€™t start up because it doesnâ€™t meet the minimum requirements. I canâ€™t find anything about dagsterâ€™s minimum reqs.\n', 'author_fullname': 't2_7l5s31ok', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dagster - Postgres - DBT in Docker? Anybody running this setup?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efjzkt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722312152.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a small project running in a docker container in my local machine. I got airflow orchestrating a script which dumps data into *** and then some dbt models going on after that. I eventually want to host this somewhere and keep it running. The script is light, all I have is a few API calls, a few insert and select queries. Less than 10k rows.</p>\n\n<p>Iâ€™m trying to do the same thing Dagster, just to get some experience with it. And if I did have everything inside Dagster, would I be able to run this in a cloud instance with 1 gb memory? I tried with airflow and it wouldnâ€™t start up because it doesnâ€™t meet the minimum requirements. I canâ€™t find anything about dagsterâ€™s minimum reqs.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1efjzkt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='HarvesterOfReveries'), 'discussion_type': None, 'num_comments': 20, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efjzkt/dagster_***_dbt_in_docker_anybody_running/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1efjzkt/dagster_***_dbt_in_docker_anybody_running/', 'subreddit_subscribers': 201126, 'created_utc': 1722312152.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.793+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Anyone here running heavy load/processing on airflow with kubernetes executor, how has your experience been and tips or advice?\n\nCurrently we run airflow with localexecutor but no heavy load, we run all dbt processes and we donâ€™t allow running any heavy processing on airflow. But wanting to move to k8s executor so that we can expand our orchestration capabilities.', 'author_fullname': 't2_5cjr5v2c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Airflow with K8s executor experience', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edwudn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722132770.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Anyone here running heavy load/processing on airflow with kubernetes executor, how has your experience been and tips or advice?</p>\n\n<p>Currently we run airflow with localexecutor but no heavy load, we run all dbt processes and we donâ€™t allow running any heavy processing on airflow. But wanting to move to k8s executor so that we can expand our orchestration capabilities.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1edwudn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='captut'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edwudn/airflow_with_k8s_executor_experience/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edwudn/airflow_with_k8s_executor_experience/', 'subreddit_subscribers': 201126, 'created_utc': 1722132770.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.794+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I gave Azure data Engineering exam last week and failed. I  learned from Microsoft learn, did hands on lab as mentioned on Microsoft Learn. I did practice exams from Microsoft and measure Up test bank and scored above 95% on both.The questions were really tough.\n\nI have passed Azure data fundamentals exam last year and this motivated me to learn and prepare for  Azure data engineering. I have never worked in Azure environment just have some years of experience in RDBMS, SSIS, and some exposure of HDFs and HQL. \n\nAny suggestions how can I pass the exam? ', 'author_fullname': 't2_8uim76no', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Scored 633 only in DP-203', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edno4o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722106515.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I gave Azure data Engineering exam last week and failed. I  learned from Microsoft learn, did hands on lab as mentioned on Microsoft Learn. I did practice exams from Microsoft and measure Up test bank and scored above 95% on both.The questions were really tough.</p>\n\n<p>I have passed Azure data fundamentals exam last year and this motivated me to learn and prepare for  Azure data engineering. I have never worked in Azure environment just have some years of experience in RDBMS, SSIS, and some exposure of HDFs and HQL. </p>\n\n<p>Any suggestions how can I pass the exam? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1edno4o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ParticularBook4372'), 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edno4o/scored_633_only_in_dp203/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edno4o/scored_633_only_in_dp203/', 'subreddit_subscribers': 201126, 'created_utc': 1722106515.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.794+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all, dataneuron is an mini open-source command-line application that allows to quickly create a semantic (context) layer and then query the db (sqlite, mysql, mssql, ***) and continually increase the accuracy. [https://github.com/databrainhq/dataneuron](https://github.com/databrainhq/dataneuron)\n\n  \nIt does few things that is different,\n\n- when there is a question like "orders for coff mkr" without any RAG or cosine similarity with just some db queries it can make it work.\n\n- you can bring your own model (compatible with OpenAI spec/package), right now it works by default with Claude 3.5 Sonnet (by default), OpenAI, LLAMA Mistral etc (through groq, together, nvidia), Ollama support (not fully tested)\n\n  \nThis is meant to be simple. And it loads the entire semantic layer into the LLM context so it only works with 10-15 tables. And in chat mode it can work till some messages. \n\n  \nI built this because there are several approaches in text2sql that is happening, and I believe the semantic layer (context layer) is the key, as the models become more intelligent, given the right context it will answer questions accurately and the missing piece is the continuous iteration on the semantic layer. \n\nI\'d love to hear your feedback, and make sure to give us a star on GitHub if you like it! ðŸš€ [https://github.com/databrainhq/dataneuron](https://github.com/databrainhq/dataneuron)', 'author_fullname': 't2_fbuupahh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I built an open-source CLI tool to maintain semantic layer and to query a db (text2sql) more accurately', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecfzna', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721970709.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all, dataneuron is an mini open-source command-line application that allows to quickly create a semantic (context) layer and then query the db (sqlite, mysql, mssql, ***) and continually increase the accuracy. <a href="https://github.com/databrainhq/dataneuron">https://github.com/databrainhq/dataneuron</a></p>\n\n<p>It does few things that is different,</p>\n\n<ul>\n<li><p>when there is a question like &quot;orders for coff mkr&quot; without any RAG or cosine similarity with just some db queries it can make it work.</p></li>\n<li><p>you can bring your own model (compatible with OpenAI spec/package), right now it works by default with Claude 3.5 Sonnet (by default), OpenAI, LLAMA Mistral etc (through groq, together, nvidia), Ollama support (not fully tested)</p></li>\n</ul>\n\n<p>This is meant to be simple. And it loads the entire semantic layer into the LLM context so it only works with 10-15 tables. And in chat mode it can work till some messages. </p>\n\n<p>I built this because there are several approaches in text2sql that is happening, and I believe the semantic layer (context layer) is the key, as the models become more intelligent, given the right context it will answer questions accurately and the missing piece is the continuous iteration on the semantic layer. </p>\n\n<p>I&#39;d love to hear your feedback, and make sure to give us a star on GitHub if you like it! ðŸš€ <a href="https://github.com/databrainhq/dataneuron">https://github.com/databrainhq/dataneuron</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?auto=webp&s=1733d67c8a5d753e1b60a5c7000ada15b17e9a02', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?width=108&crop=smart&auto=webp&s=e79a1a3b5fe973851ec4f253bdd3585cdf7df7cd', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?width=216&crop=smart&auto=webp&s=83394031be0db09c8e5151deb4ab2bba96c1092d', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?width=320&crop=smart&auto=webp&s=5512ed46b82dd21349218e6c97f1bdba7290efcd', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?width=640&crop=smart&auto=webp&s=70c6ae14368b5848f90606d9de2407b52320be34', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?width=960&crop=smart&auto=webp&s=7d0b343d101e470c8a3ffae0c887f87807c9b7a6', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/G_OBT5K-9UwvNPLnAkDsEswfROl5soW_CN512-Lo0Gg.jpg?width=1080&crop=smart&auto=webp&s=883f33c1bf5387c3536bbe81b33678739d31eac1', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Y2UHqpAgnoaTJuyUXuzRvdtN7XXsbyp9sCGC9kIVEAY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1ecfzna', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BackgroundDig441'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecfzna/i_built_an_opensource_cli_tool_to_maintain/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecfzna/i_built_an_opensource_cli_tool_to_maintain/', 'subreddit_subscribers': 201126, 'created_utc': 1721970709.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.795+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Would like to get peopleâ€™s opinions on this but whatâ€™s your course of action if you find something that was wrong about a report that was created (bad join, wrong formula, etc.) and it has an impact on data the has already been reported to stakeholders.\n\nWorking a data model right now where the temp query that an analyst created had a bunch of problems in it. Itâ€™s not vastly significant but it will overall shift the numbers of someone were to look back at it. Business has been telling me that they donâ€™t want to make a big change like that.\n\nDo you keep the â€œincorrectâ€ data and moving forward have new data be correct?', 'author_fullname': 't2_5ukitegd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you do with historical data if it was wrong but already reported on?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg1idp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722365892.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Would like to get peopleâ€™s opinions on this but whatâ€™s your course of action if you find something that was wrong about a report that was created (bad join, wrong formula, etc.) and it has an impact on data the has already been reported to stakeholders.</p>\n\n<p>Working a data model right now where the temp query that an analyst created had a bunch of problems in it. Itâ€™s not vastly significant but it will overall shift the numbers of someone were to look back at it. Business has been telling me that they donâ€™t want to make a big change like that.</p>\n\n<p>Do you keep the â€œincorrectâ€ data and moving forward have new data be correct?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eg1idp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='burningburnerbern'), 'discussion_type': None, 'num_comments': 18, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg1idp/what_do_you_do_with_historical_data_if_it_was/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg1idp/what_do_you_do_with_historical_data_if_it_was/', 'subreddit_subscribers': 201126, 'created_utc': 1722365892.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.795+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi, I am re-reading Kimball's Data Warehouse Toolkit. In Chapter 6 he talks about how to model Order header and Order Line data. In his ideal solution, there is one fact table on the Order Line grain that absorbs the Order Header attributes into it.\n\nNow, in that solution, he either didn't have any facts existing on the Order Header grain, or he was able to allocate them to the Order Line level.\n\nIn my case, I have facts on both Order and Order Line level and those on the Order level cannot be allocated to the Order Line level. **He says two fact tables (one for each grain) should be created in those cases.** \n\n**My question here is** - does that mean, that we'll also need a new Order Header dim (which is something he initially called out as anti-pattern) to be created and shared between the two fact tables, or should the attributes from the Order Header be replicated on both fact tables? Basically, is his guidance to go with Option 1 or Option 2 in these situations?\n\nhttps://preview.redd.it/13z867i7dlfd1.png?width=1676&format=png&auto=webp&s=0327c57633ada9ea684c6d07c37628c115af236f\n\nOption 1 means replicating Order-level attributes on two different fact tables. Option 2 creates a conformed dimension, resulting in a large join to the Order dim when slicing/filtering. I'd expect Option 1 to perform quite a bit better (btw I am using Snowflake), but any time a new fact table, that is supposed to be referencing Order, needs to be added, the Order attributes will need to be replicated on it.\n\nI assume there could also be option 3, which would still use Order Dim, but would also replicate some of the most commonly used slicers/filters directly onto the fact tables...\n\n  \nAny opinions on which option Kimball is advocating for, or just opinions on the preferred approach in these cases based on experience are welcomed. Thank you!", 'author_fullname': 't2_gx2hs6l34', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Header and detail - dimensional model', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 69, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'13z867i7dlfd1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 53, 'x': 108, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=108&crop=smart&auto=webp&s=58fca3b1a3af8fddbafb3e7d1b925021e7aa5351'}, {'y': 107, 'x': 216, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=216&crop=smart&auto=webp&s=89b156a204c73c9bc352c7c1c3379a2827c3f32f'}, {'y': 159, 'x': 320, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=320&crop=smart&auto=webp&s=6b41d446a7a18a27127370ff0daae462efa444da'}, {'y': 318, 'x': 640, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=640&crop=smart&auto=webp&s=476383e432d8cb29aaebd566dea272ad82d63cf1'}, {'y': 477, 'x': 960, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=960&crop=smart&auto=webp&s=86dc938b1f1e72e2e356a3973d111e082d09f15f'}, {'y': 537, 'x': 1080, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=1080&crop=smart&auto=webp&s=f88645e6d8a6de7aafacccc85afde70ba4493e48'}], 's': {'y': 834, 'x': 1676, 'u': 'https://preview.redd.it/13z867i7dlfd1.png?width=1676&format=png&auto=webp&s=0327c57633ada9ea684c6d07c37628c115af236f'}, 'id': '13z867i7dlfd1'}}, 'name': 't3_1eflkgq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/s6zeT_6dI8V7pF_4Wcssx_B6CLoOKhyTkHYTvCrD58g.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722317650.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, I am re-reading Kimball&#39;s Data Warehouse Toolkit. In Chapter 6 he talks about how to model Order header and Order Line data. In his ideal solution, there is one fact table on the Order Line grain that absorbs the Order Header attributes into it.</p>\n\n<p>Now, in that solution, he either didn&#39;t have any facts existing on the Order Header grain, or he was able to allocate them to the Order Line level.</p>\n\n<p>In my case, I have facts on both Order and Order Line level and those on the Order level cannot be allocated to the Order Line level. <strong>He says two fact tables (one for each grain) should be created in those cases.</strong> </p>\n\n<p><strong>My question here is</strong> - does that mean, that we&#39;ll also need a new Order Header dim (which is something he initially called out as anti-pattern) to be created and shared between the two fact tables, or should the attributes from the Order Header be replicated on both fact tables? Basically, is his guidance to go with Option 1 or Option 2 in these situations?</p>\n\n<p><a href="https://preview.redd.it/13z867i7dlfd1.png?width=1676&amp;format=png&amp;auto=webp&amp;s=0327c57633ada9ea684c6d07c37628c115af236f">https://preview.redd.it/13z867i7dlfd1.png?width=1676&amp;format=png&amp;auto=webp&amp;s=0327c57633ada9ea684c6d07c37628c115af236f</a></p>\n\n<p>Option 1 means replicating Order-level attributes on two different fact tables. Option 2 creates a conformed dimension, resulting in a large join to the Order dim when slicing/filtering. I&#39;d expect Option 1 to perform quite a bit better (btw I am using Snowflake), but any time a new fact table, that is supposed to be referencing Order, needs to be added, the Order attributes will need to be replicated on it.</p>\n\n<p>I assume there could also be option 3, which would still use Order Dim, but would also replicate some of the most commonly used slicers/filters directly onto the fact tables...</p>\n\n<p>Any opinions on which option Kimball is advocating for, or just opinions on the preferred approach in these cases based on experience are welcomed. Thank you!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eflkgq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='InsightInk'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eflkgq/header_and_detail_dimensional_model/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eflkgq/header_and_detail_dimensional_model/', 'subreddit_subscribers': 201126, 'created_utc': 1722317650.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.796+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi, I'm starting to dip my toes into the data engineering world and have been working a little with Spark. I see that the polars library is quite popular for speed and had some questions. Is it a good idea to use Polars on Spark or will it interfere with Spark's parallelism? From what I've been reading Polars seems excellent for parrellelsing stuff on your machine, but I'm unclear how it works with the distributed side of Spark? Any insight on this and related issues would be super helpful.\n\nFor context, I'm working on a project where a bunch of parquet files are read into spark, the data then undergoes some cleaning up and transfroming before being merged into a bunch of delta tables.", 'author_fullname': 't2_3mfqrrfi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it a good idea to use Polars with Pyspark?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehbgfl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722500700.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, I&#39;m starting to dip my toes into the data engineering world and have been working a little with Spark. I see that the polars library is quite popular for speed and had some questions. Is it a good idea to use Polars on Spark or will it interfere with Spark&#39;s parallelism? From what I&#39;ve been reading Polars seems excellent for parrellelsing stuff on your machine, but I&#39;m unclear how it works with the distributed side of Spark? Any insight on this and related issues would be super helpful.</p>\n\n<p>For context, I&#39;m working on a project where a bunch of parquet files are read into spark, the data then undergoes some cleaning up and transfroming before being merged into a bunch of delta tables.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehbgfl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Schlooooompy'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehbgfl/is_it_a_good_idea_to_use_polars_with_pyspark/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehbgfl/is_it_a_good_idea_to_use_polars_with_pyspark/', 'subreddit_subscribers': 201126, 'created_utc': 1722500700.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.796+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_nwnnprhup', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Amazonâ€™s Exabyte-Scale Migration from Apache Spark to Ray on Amazon EC2', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 69, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egxnsu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/aTbLuIA4dHP37zZy3uWN2Dm52xB4VI0JxXq85gYhWNg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722458117.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'aws.amazon.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?auto=webp&s=c091425bd8a7d95084f2541baf5f04b1dd8b9bf0', 'width': 1260, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=108&crop=smart&auto=webp&s=8a26f3d3fd2aeea9de73c0c7220835232d5976b6', 'width': 108, 'height': 53}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=216&crop=smart&auto=webp&s=e39b4aba108b51327eb053f885afe27cb111f431', 'width': 216, 'height': 107}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=320&crop=smart&auto=webp&s=56eb833818025a22de90be17152afee64113dc77', 'width': 320, 'height': 159}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=640&crop=smart&auto=webp&s=79bcfb494edffafb5688d0200121f9d16b447ecd', 'width': 640, 'height': 318}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=960&crop=smart&auto=webp&s=b66a98643aa46e923827085af5551a0998db1a91', 'width': 960, 'height': 478}, {'url': 'https://external-preview.redd.it/kJUQZ7tZ8hrT0pl1P9-L8k6XQ9ogrAQb3Agf8XEqXog.jpg?width=1080&crop=smart&auto=webp&s=77f6646519ede21b9ebf076fe6dc25c82bbbe3b6', 'width': 1080, 'height': 538}], 'variants': {}, 'id': 'DYcK-LguRnr1f1_eAOPF3C2IotmgYet-W6HqkdAt5dU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1egxnsu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='saaggy_peneer'), 'discussion_type': None, 'num_comments': 2, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egxnsu/amazons_exabytescale_migration_from_apache_spark/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/', 'subreddit_subscribers': 201126, 'created_utc': 1722458117.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.797+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello All,\n\nCurrently at a very small tech startup that is using managed Airflow in AWS. Managing Airflow in prod is not difficult as we have a full time devops eng (with not too much to manage), but our local development is so painful in Airflow.\n\nAirflow is dockererized and it typically takes many ups/downs or re-runs of a job to get the code to update; the airflow errors are sometimes cryptic when airflow itself takes a shit; the logging isn\'t great; sometimes you have to click into the job "see more" to see the full output. In general, in our setup it does not promote anything even close to "fast iterative development".\n\nOur workflows are dead simple - they are not "big data" pipelines with 20-30 steps. They are at most a few steps; pickup a file from S3, do dataframe stuff, put back in S3. Sometimes call an API for the LLM. In general pretty simple.\n\nSo my options are something like Prefect, Argo (we use Kubernetes but I don\'t run K8\'s locally for dev) or something else like Lambdas (what the team wants...) or just write a simple python library that has workers that listen on a queue and runs them.\n\nI haven\'t worked with Lambda much recently, but I remember version control was an issue, just local dev in general was a pain in the ass.\n\nWe don\'t have volume of any kind - MAU is like 15 people (maybe scales to 100s) our data is just excel/csv files (small, like megabytes) and our pipelines don\'t take long to run.\n\nIn this scenario, I\'m sort of leaning towards just vanilla python repo with a library for picking up messages from the queue, running the correct worker for the topic and calling it a day.\n\nThe team is small (<10 devs) and the prior EM tended to over engineer (self imposed SOC 2 type 2 for example, Okta, no local dev/source code on local boxes (we we don\'t really obey), etc).\n\nThoughts? Appreciate the feedback!', 'author_fullname': 't2_enexo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'AWS Shop: Current Airflow (difficult for us devs) - Move to Prefect, or a simple Worker/Queue system?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef60fy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722275568.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello All,</p>\n\n<p>Currently at a very small tech startup that is using managed Airflow in AWS. Managing Airflow in prod is not difficult as we have a full time devops eng (with not too much to manage), but our local development is so painful in Airflow.</p>\n\n<p>Airflow is dockererized and it typically takes many ups/downs or re-runs of a job to get the code to update; the airflow errors are sometimes cryptic when airflow itself takes a shit; the logging isn&#39;t great; sometimes you have to click into the job &quot;see more&quot; to see the full output. In general, in our setup it does not promote anything even close to &quot;fast iterative development&quot;.</p>\n\n<p>Our workflows are dead simple - they are not &quot;big data&quot; pipelines with 20-30 steps. They are at most a few steps; pickup a file from S3, do dataframe stuff, put back in S3. Sometimes call an API for the LLM. In general pretty simple.</p>\n\n<p>So my options are something like Prefect, Argo (we use Kubernetes but I don&#39;t run K8&#39;s locally for dev) or something else like Lambdas (what the team wants...) or just write a simple python library that has workers that listen on a queue and runs them.</p>\n\n<p>I haven&#39;t worked with Lambda much recently, but I remember version control was an issue, just local dev in general was a pain in the ass.</p>\n\n<p>We don&#39;t have volume of any kind - MAU is like 15 people (maybe scales to 100s) our data is just excel/csv files (small, like megabytes) and our pipelines don&#39;t take long to run.</p>\n\n<p>In this scenario, I&#39;m sort of leaning towards just vanilla python repo with a library for picking up messages from the queue, running the correct worker for the topic and calling it a day.</p>\n\n<p>The team is small (&lt;10 devs) and the prior EM tended to over engineer (self imposed SOC 2 type 2 for example, Okta, no local dev/source code on local boxes (we we don&#39;t really obey), etc).</p>\n\n<p>Thoughts? Appreciate the feedback!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ef60fy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='stupidadult'), 'discussion_type': None, 'num_comments': 18, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef60fy/aws_shop_current_airflow_difficult_for_us_devs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef60fy/aws_shop_current_airflow_difficult_for_us_devs/', 'subreddit_subscribers': 201126, 'created_utc': 1722275568.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.797+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello,\n\nIn my job i am using Scala for data engineering stuff and java for API development.  \nI have 4 years of experiences.\n\nWhen i am looking at job offers, i realize that the majority are asking for Python.\n\nSo my question is : Should I continue learning and improving my skills in Java/Scala or starting learning python ?', 'author_fullname': 't2_9iw7pa3b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Learn python as a Java/Scala Data Engineer ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edorg6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722109439.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,</p>\n\n<p>In my job i am using Scala for data engineering stuff and java for API development.<br/>\nI have 4 years of experiences.</p>\n\n<p>When i am looking at job offers, i realize that the majority are asking for Python.</p>\n\n<p>So my question is : Should I continue learning and improving my skills in Java/Scala or starting learning python ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1edorg6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='_Marwan02'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edorg6/learn_python_as_a_javascala_data_engineer/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edorg6/learn_python_as_a_javascala_data_engineer/', 'subreddit_subscribers': 201126, 'created_utc': 1722109439.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.797+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Snowflake has released their open source Iceberg catalog, Polaris. The catalog works with open source compute engines such as Doris, Flink, Trino, and of course Spark. The [release documentation](http://polaris.io) is pretty good and there are multiple deployment options including docker and Kubernetes. Will be interesting to see if they attract additional contributors or remain a majority Snowflake project.\n\n[https://github.com/polaris-catalog/polaris](https://github.com/polaris-catalog/polaris)', 'author_fullname': 't2_ez8d4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Snowflake Polaris Catalog', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg0bzm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722363025.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Snowflake has released their open source Iceberg catalog, Polaris. The catalog works with open source compute engines such as Doris, Flink, Trino, and of course Spark. The <a href="http://polaris.io">release documentation</a> is pretty good and there are multiple deployment options including docker and Kubernetes. Will be interesting to see if they attract additional contributors or remain a majority Snowflake project.</p>\n\n<p><a href="https://github.com/polaris-catalog/polaris">https://github.com/polaris-catalog/polaris</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eg0bzm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Bazencourt'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg0bzm/snowflake_polaris_catalog/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg0bzm/snowflake_polaris_catalog/', 'subreddit_subscribers': 201126, 'created_utc': 1722363025.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.798+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all. First time posting here. A couple of friends and I are looking into starting a DE business aimed at small and medium business in our area. How do we start/approach businesses in the field. \n\nAny advice is appreciated.  ', 'author_fullname': 't2_xfxv1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Starting a DE Business', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eelkqv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722211148.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all. First time posting here. A couple of friends and I are looking into starting a DE business aimed at small and medium business in our area. How do we start/approach businesses in the field. </p>\n\n<p>Any advice is appreciated.  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eelkqv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Businessofgames'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eelkqv/starting_a_de_business/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eelkqv/starting_a_de_business/', 'subreddit_subscribers': 201126, 'created_utc': 1722211148.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.798+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone,\n\nI'll be joining a small company as a data engineer soon. They use a dedicated server to host and deploy their solutions, which costs them around $200 a month. They store and manage IoT data for vehicles in real time, generating approximately 30GB of data daily.\n\nIâ€™ve noticed that many data engineering jobs use cloud solutions like GCP and AWS. However, due to the high costs (engineers in my country earn an average of $250 monthly, and cloud solutions seem much more expensive), so switching to these providers isnâ€™t an option for the company.\n\nMy questions are:\n\n1. Is it practical to use open-source tools like Kafka, Cassandra, and Spark on a dedicated server, or will it be too much of an overhead to manage them?\n\n2. Does anyone know how much it  costs approximately  to use data engineering tools on the cloud for a company that generate approximately 30gb of data daily and need to store and analyze it (also they plan to use ML)?\n\nAny advice or insights would be greatly appreciated!\n\nThanks!\n", 'author_fullname': 't2_czqa011c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Cloud Solutions vs. Dedicated Server for Data Engineering in a Small Company', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecy0b4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722025516.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;ll be joining a small company as a data engineer soon. They use a dedicated server to host and deploy their solutions, which costs them around $200 a month. They store and manage IoT data for vehicles in real time, generating approximately 30GB of data daily.</p>\n\n<p>Iâ€™ve noticed that many data engineering jobs use cloud solutions like GCP and AWS. However, due to the high costs (engineers in my country earn an average of $250 monthly, and cloud solutions seem much more expensive), so switching to these providers isnâ€™t an option for the company.</p>\n\n<p>My questions are:</p>\n\n<ol>\n<li><p>Is it practical to use open-source tools like Kafka, Cassandra, and Spark on a dedicated server, or will it be too much of an overhead to manage them?</p></li>\n<li><p>Does anyone know how much it  costs approximately  to use data engineering tools on the cloud for a company that generate approximately 30gb of data daily and need to store and analyze it (also they plan to use ML)?</p></li>\n</ol>\n\n<p>Any advice or insights would be greatly appreciated!</p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecy0b4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Anxious_Culture_2901'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecy0b4/cloud_solutions_vs_dedicated_server_for_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecy0b4/cloud_solutions_vs_dedicated_server_for_data/', 'subreddit_subscribers': 201126, 'created_utc': 1722025516.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.798+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'From what little understanding I have around these two, I thought one of their big features was the elimination for needing to be a multi-cloud environment? For example, if a retail company has vendors using different clouds, a big reason you would pick Snowflake/Databricks was because it essentially "translated" the data you were receiving/sending out. I\'m likely misremembering this, but I could have sworn this gave them the ability to free themselves from needing multiple clouds to accommodate to their vendors...your help/insight is greatly appreciated.', 'author_fullname': 't2_3jryrs5n', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dumb question about Databricks/Snowflake', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egw5ns', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722454432.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>From what little understanding I have around these two, I thought one of their big features was the elimination for needing to be a multi-cloud environment? For example, if a retail company has vendors using different clouds, a big reason you would pick Snowflake/Databricks was because it essentially &quot;translated&quot; the data you were receiving/sending out. I&#39;m likely misremembering this, but I could have sworn this gave them the ability to free themselves from needing multiple clouds to accommodate to their vendors...your help/insight is greatly appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egw5ns', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pickmeup0103'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egw5ns/dumb_question_about_databrickssnowflake/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egw5ns/dumb_question_about_databrickssnowflake/', 'subreddit_subscribers': 201126, 'created_utc': 1722454432.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.799+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi Guys, \n\nI was just curious how you justify your costs to the business?\n\nInternally do you speak about cost per query?  Or how do you derive if something is running expensive, is it simply down to your budget and operating within that?\n\nI was asked to understand the cost per query of our system, which is on-premise MPP.    \n\nIm fearful one of the execs is talking to GCP or something like that as I believe they charge by the query.   \n\nIs this common?   If not how do you get your cost per query, all costs of infra, employees divided by the annual queries collected from the platform statisitcs?', 'author_fullname': 't2_z77sr36st', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Datawarehouse/Lakhouse costs', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egti9q', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722448074.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Guys, </p>\n\n<p>I was just curious how you justify your costs to the business?</p>\n\n<p>Internally do you speak about cost per query?  Or how do you derive if something is running expensive, is it simply down to your budget and operating within that?</p>\n\n<p>I was asked to understand the cost per query of our system, which is on-premise MPP.    </p>\n\n<p>Im fearful one of the execs is talking to GCP or something like that as I believe they charge by the query.   </p>\n\n<p>Is this common?   If not how do you get your cost per query, all costs of infra, employees divided by the annual queries collected from the platform statisitcs?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1egti9q', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='TheDataguy83'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egti9q/datawarehouselakhouse_costs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egti9q/datawarehouselakhouse_costs/', 'subreddit_subscribers': 201126, 'created_utc': 1722448074.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.799+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello,\n\nI'm currently working as a Data Analyst for more than 3 years now and want to switch to Data Engineering jobs. I have strong analytical skills but I'm not coming from cs background, I'm an engineer and self learned before I got my first job.\n\nIn general, I have proficieny using in Python, Tableau, SQL, Pyspark, Databricks. I generally want to learn more about Docker, CI/CD, Airflow, Kafka, data pipelines, dbt, etc.\n\nI would prefer something with hands-on projects. Why I prefer a structured course is that, since I need to dedicate some time outside work hours, I generally fail to do otherwise. This kind of materials generally keeps me more focused.\n\nIt can be self-paces or instructor-led, and I have around 1500USD budget from my company for self-learning and development.\n\nWhat are your recommendations?", 'author_fullname': 't2_dmb3o0qj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DA to DE Courses & Projects', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg5n7l', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722375753.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,</p>\n\n<p>I&#39;m currently working as a Data Analyst for more than 3 years now and want to switch to Data Engineering jobs. I have strong analytical skills but I&#39;m not coming from cs background, I&#39;m an engineer and self learned before I got my first job.</p>\n\n<p>In general, I have proficieny using in Python, Tableau, SQL, Pyspark, Databricks. I generally want to learn more about Docker, CI/CD, Airflow, Kafka, data pipelines, dbt, etc.</p>\n\n<p>I would prefer something with hands-on projects. Why I prefer a structured course is that, since I need to dedicate some time outside work hours, I generally fail to do otherwise. This kind of materials generally keeps me more focused.</p>\n\n<p>It can be self-paces or instructor-led, and I have around 1500USD budget from my company for self-learning and development.</p>\n\n<p>What are your recommendations?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1eg5n7l', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Frosty-Joke-5880'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg5n7l/da_to_de_courses_projects/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg5n7l/da_to_de_courses_projects/', 'subreddit_subscribers': 201126, 'created_utc': 1722375753.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.800+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\n  \nI have a requirement in my current role where I need to design a data warehouse from scratch. This is for a new application we are building. I have built the data model for the application which is Azure SQL Server based OLTP database.\n\nOur organization is using Databricks for all their warehouse needs in other project and my manager wants me to use the same for creating the warehouse. This layer on Databricks will be used for analytical purpose from which PowerBI will connect to fetch data and refresh reports. Other projects use multi-hop/Medallion architecture and I would want to stick to same for this warehouse as well.\n\n  \nMy queries: \n\n1. I have never modelled a warehouse in the past using Kimball/Dimensional Modelling. How should I go about it and design the data model? Any guidance to start on it would be helpful.\n\n2. Are data warehouses on databricks similar to data warehouses on traditional RDBMS like Netezza or SQL Server? What are the key differences that I should keep in mind while designing this warehouse?\n\n3. I enjoyed when I was creating the data model for the OLTP and it is working perfect at the moment behind the application bar few blockers which we are working to remove. My main stress is designing the data model for DW mainly because it is different than the OLTP Data model and secondly it is supposed to be in the databricks.\n\n  \nAny help or pointers on this would be appreciable and will help me control my nerves.\n\n  \nTIA', 'author_fullname': 't2_73q1nsnt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Create a Data warehouse from scratch', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg35ck', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722369763.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>I have a requirement in my current role where I need to design a data warehouse from scratch. This is for a new application we are building. I have built the data model for the application which is Azure SQL Server based OLTP database.</p>\n\n<p>Our organization is using Databricks for all their warehouse needs in other project and my manager wants me to use the same for creating the warehouse. This layer on Databricks will be used for analytical purpose from which PowerBI will connect to fetch data and refresh reports. Other projects use multi-hop/Medallion architecture and I would want to stick to same for this warehouse as well.</p>\n\n<p>My queries: </p>\n\n<ol>\n<li><p>I have never modelled a warehouse in the past using Kimball/Dimensional Modelling. How should I go about it and design the data model? Any guidance to start on it would be helpful.</p></li>\n<li><p>Are data warehouses on databricks similar to data warehouses on traditional RDBMS like Netezza or SQL Server? What are the key differences that I should keep in mind while designing this warehouse?</p></li>\n<li><p>I enjoyed when I was creating the data model for the OLTP and it is working perfect at the moment behind the application bar few blockers which we are working to remove. My main stress is designing the data model for DW mainly because it is different than the OLTP Data model and secondly it is supposed to be in the databricks.</p></li>\n</ol>\n\n<p>Any help or pointers on this would be appreciable and will help me control my nerves.</p>\n\n<p>TIA</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eg35ck', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='EatDoughnut'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg35ck/create_a_data_warehouse_from_scratch/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg35ck/create_a_data_warehouse_from_scratch/', 'subreddit_subscribers': 201126, 'created_utc': 1722369763.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.800+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey fellow data engineers! I\'m new to dbt and could use some help with a tricky situation.\n\n# The Problem:\n\nI\'m trying to implement a model that tracks hard deletes in my source tables. These tables are HUGE, so using snapshots isn\'t really an option (too much time and compute power?).\n\n# What I\'ve Considered:\n\n1. An incremental strategy seems like it could save time and resources.\n2. However, the default incremental model in dbt doesn\'t track hard deletes in the source.\n3. Using a custom materialization built on top of the incremental strategy, but I\'m not sure where to start.\n4. Using a post-hook to find deletes by using a minus operation and dropping them, but that would likely break idempotence.\n\n# What I Need Help With:\n\nHow would you implement this in dbt? Remember, I\'m a dbt newbie, so please explain like I\'m five.\n\n# Questions:\n\n1. Is there a way to modify the incremental strategy to track hard deletes?\n2. How can I create a custom materialization that handles hard deletes? Any resources or starting points?\n3. Are there any dbt packages or patterns that could help with this?\n4. Is there a way to use post-hooks for delete detection without breaking idempotence?\n5. Any other approaches I should consider that I haven\'t mentioned?\n\nThanks in advance for any help or guidance!\n\n*Edit*:\n\nDB: Snowflake\n\nSource Table Tracking: The root node upstream does track deletes and soft deletes them, but the intermediate tables that the current model is built from don\'t track hard deletes, and I have no control over them.\n\nTable Size: Huge was just to emphasize that we have hundreds of millions of rows, and I am tasked with finding an "efficient" solution.\n\nData Changes: Records are both updated and inserted as is default with dbt incremental model, but the extra requirement is to also drop hard deletes.', 'author_fullname': 't2_3njxv4qv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[dbt] Tracking hard deletes incrementally in large tables - Need advice!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efsg0h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722352808.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722343638.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey fellow data engineers! I&#39;m new to dbt and could use some help with a tricky situation.</p>\n\n<h1>The Problem:</h1>\n\n<p>I&#39;m trying to implement a model that tracks hard deletes in my source tables. These tables are HUGE, so using snapshots isn&#39;t really an option (too much time and compute power?).</p>\n\n<h1>What I&#39;ve Considered:</h1>\n\n<ol>\n<li>An incremental strategy seems like it could save time and resources.</li>\n<li>However, the default incremental model in dbt doesn&#39;t track hard deletes in the source.</li>\n<li>Using a custom materialization built on top of the incremental strategy, but I&#39;m not sure where to start.</li>\n<li>Using a post-hook to find deletes by using a minus operation and dropping them, but that would likely break idempotence.</li>\n</ol>\n\n<h1>What I Need Help With:</h1>\n\n<p>How would you implement this in dbt? Remember, I&#39;m a dbt newbie, so please explain like I&#39;m five.</p>\n\n<h1>Questions:</h1>\n\n<ol>\n<li>Is there a way to modify the incremental strategy to track hard deletes?</li>\n<li>How can I create a custom materialization that handles hard deletes? Any resources or starting points?</li>\n<li>Are there any dbt packages or patterns that could help with this?</li>\n<li>Is there a way to use post-hooks for delete detection without breaking idempotence?</li>\n<li>Any other approaches I should consider that I haven&#39;t mentioned?</li>\n</ol>\n\n<p>Thanks in advance for any help or guidance!</p>\n\n<p><em>Edit</em>:</p>\n\n<p>DB: Snowflake</p>\n\n<p>Source Table Tracking: The root node upstream does track deletes and soft deletes them, but the intermediate tables that the current model is built from don&#39;t track hard deletes, and I have no control over them.</p>\n\n<p>Table Size: Huge was just to emphasize that we have hundreds of millions of rows, and I am tasked with finding an &quot;efficient&quot; solution.</p>\n\n<p>Data Changes: Records are both updated and inserted as is default with dbt incremental model, but the extra requirement is to also drop hard deletes.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1efsg0h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='lucidparadigm'), 'discussion_type': None, 'num_comments': 22, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efsg0h/dbt_tracking_hard_deletes_incrementally_in_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1efsg0h/dbt_tracking_hard_deletes_incrementally_in_large/', 'subreddit_subscribers': 201126, 'created_utc': 1722343638.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.801+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Currently doing a 4 year cs degree with honours and I have the option of picking up a double in statistics or a masters in data science. I can probably get some acknowledgement of prior learning for the masters given that I'm doing a math minor in my cs degree right now, so both options should both take me 1 year. Any idea as to which would be better?", 'author_fullname': 't2_ral5936f4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Masters in DS vs Bachelor in Stats?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef0jz8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.74, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722262346.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Currently doing a 4 year cs degree with honours and I have the option of picking up a double in statistics or a masters in data science. I can probably get some acknowledgement of prior learning for the masters given that I&#39;m doing a math minor in my cs degree right now, so both options should both take me 1 year. Any idea as to which would be better?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ef0jz8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BriefBit4360'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef0jz8/masters_in_ds_vs_bachelor_in_stats/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef0jz8/masters_in_ds_vs_bachelor_in_stats/', 'subreddit_subscribers': 201126, 'created_utc': 1722262346.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.802+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I started my new job as a data engineer a few months ago but have had experience in data analytics/data science for the past 7 years, some of which included tasks that could roughly be considered data engineering.  My job thus far has been more on the analytics engineering side since my primary task is to transform source data to eventually load to our data warehouse and use data in our warehouse to develop cleaned datasets to be used by data science and other teams.\n\nI changed from analytics to data engineering b/c I like programming a lot more than analyzing data. I would ideally like to have my role progress to a mix of analytics engineering + backend software engineering skills. Before getting my job, I took some intro to CS courses that covered data structures and algorithms and learned some full-stack dev skills. My goal is to continue honing my CS skills and learn more backend dev in my own time. \n\nMy job uses AWS, so I'm trying to get good at learning AWS and hope that maybe my job can provide certification opportunities, though I haven't seen any training opportunities advertised at my job. It's a pretty big company and so far I feel like my team has a lot of growth potential b/c the project just kickstarted after being in the works for just a few years. Just curious what learning opportunities your job provides? ", 'author_fullname': 't2_2prckadt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Does your job provide learning opportunities? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef8r2e', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722282138.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I started my new job as a data engineer a few months ago but have had experience in data analytics/data science for the past 7 years, some of which included tasks that could roughly be considered data engineering.  My job thus far has been more on the analytics engineering side since my primary task is to transform source data to eventually load to our data warehouse and use data in our warehouse to develop cleaned datasets to be used by data science and other teams.</p>\n\n<p>I changed from analytics to data engineering b/c I like programming a lot more than analyzing data. I would ideally like to have my role progress to a mix of analytics engineering + backend software engineering skills. Before getting my job, I took some intro to CS courses that covered data structures and algorithms and learned some full-stack dev skills. My goal is to continue honing my CS skills and learn more backend dev in my own time. </p>\n\n<p>My job uses AWS, so I&#39;m trying to get good at learning AWS and hope that maybe my job can provide certification opportunities, though I haven&#39;t seen any training opportunities advertised at my job. It&#39;s a pretty big company and so far I feel like my team has a lot of growth potential b/c the project just kickstarted after being in the works for just a few years. Just curious what learning opportunities your job provides? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ef8r2e', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='thro0away12'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef8r2e/does_your_job_provide_learning_opportunities/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef8r2e/does_your_job_provide_learning_opportunities/', 'subreddit_subscribers': 201126, 'created_utc': 1722282138.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.802+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi!\n\nI run a very small team that has implemented Databricks in our organization, and we have set up a solid system (CI/CD, jobs, pipelines etc). But we are lacking the â€œintegrationâ€ to the rest of the organization. We have an on premise structure that we cannot reach yet (small network team, so not priority), so we cannot really reach on premise databases. In addition, we have to land data manually in the raw storage for Databricks to consume. \n\nTo counter this, Iâ€™m running Prefect using Prefect Cloud  (free) and a local agent that has FW access. This agent runs scripts that uploads to azure storage or writes to Postgres databases in azure. But I canâ€™t really stand the Prefect UI, and I have to make a choice to go paid to get proper RBAC.\n\nSo I am looking for recommendations for the following:\n\n* Databricks as the main analytics and processing tool\n* ??? as an orchestrator/agent that can pick up files on premise or externally, and either dump raw data for Databricks to consume, or write clean data from on premise databases to Postgres, but that also gives some sort of overview, scheduling, metadata etc.\n* Data catalog tool to allow owners of datasets maintain the metadata for their datasets.\n* Limit tools to what a two-three person team can manage while still making pipelines.\n* We are semi-good at Terraform, if applicable.\n\n  \nI am looking at Dagster, but Iâ€™d love to hear some recommendations. Like I said, we are a small team, so Iâ€™m skeptical to hosting OS versions of orchestration software since we really donâ€™t have anyone to implement and maintain it, so happily pay a small price for a hosted version with hybrid deployments.', 'author_fullname': 't2_jg0yo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for advice - orchestrator/data integration tool on top of Databrick', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ee1d9u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722149358.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi!</p>\n\n<p>I run a very small team that has implemented Databricks in our organization, and we have set up a solid system (CI/CD, jobs, pipelines etc). But we are lacking the â€œintegrationâ€ to the rest of the organization. We have an on premise structure that we cannot reach yet (small network team, so not priority), so we cannot really reach on premise databases. In addition, we have to land data manually in the raw storage for Databricks to consume. </p>\n\n<p>To counter this, Iâ€™m running Prefect using Prefect Cloud  (free) and a local agent that has FW access. This agent runs scripts that uploads to azure storage or writes to Postgres databases in azure. But I canâ€™t really stand the Prefect UI, and I have to make a choice to go paid to get proper RBAC.</p>\n\n<p>So I am looking for recommendations for the following:</p>\n\n<ul>\n<li>Databricks as the main analytics and processing tool</li>\n<li>??? as an orchestrator/agent that can pick up files on premise or externally, and either dump raw data for Databricks to consume, or write clean data from on premise databases to Postgres, but that also gives some sort of overview, scheduling, metadata etc.</li>\n<li>Data catalog tool to allow owners of datasets maintain the metadata for their datasets.</li>\n<li>Limit tools to what a two-three person team can manage while still making pipelines.</li>\n<li>We are semi-good at Terraform, if applicable.</li>\n</ul>\n\n<p>I am looking at Dagster, but Iâ€™d love to hear some recommendations. Like I said, we are a small team, so Iâ€™m skeptical to hosting OS versions of orchestration software since we really donâ€™t have anyone to implement and maintain it, so happily pay a small price for a hosted version with hybrid deployments.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ee1d9u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='DeepFryEverything'), 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ee1d9u/looking_for_advice_orchestratordata_integration/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ee1d9u/looking_for_advice_orchestratordata_integration/', 'subreddit_subscribers': 201126, 'created_utc': 1722149358.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.803+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have written a complete guide on [how to deploy Airbyte in Production on a Kubernetes](https://medium.com/p/64d3ac259474) (EKS) cluster with zero knowledge. I was wondering, how many of you would be interested in a 100% managed Airbyte + Kubernetes offer running on your own cloud account? \n\nThe main benefits would be the following:\n\n1. No Kubernetes maintenance, no upgrade (it's on us)\n2. Airbyte maintenance and version upgrades on us\n3. Running on your own cloud and VPC\n\nIs that something that would make enough sense to you to find the offer appealing? If yes or no, what would be your concerns?", 'author_fullname': 't2_1noqay5b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Managed Airbyte on your own Kubernetes cluster', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edc93g', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.74, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722072006.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have written a complete guide on <a href="https://medium.com/p/64d3ac259474">how to deploy Airbyte in Production on a Kubernetes</a> (EKS) cluster with zero knowledge. I was wondering, how many of you would be interested in a 100% managed Airbyte + Kubernetes offer running on your own cloud account? </p>\n\n<p>The main benefits would be the following:</p>\n\n<ol>\n<li>No Kubernetes maintenance, no upgrade (it&#39;s on us)</li>\n<li>Airbyte maintenance and version upgrades on us</li>\n<li>Running on your own cloud and VPC</li>\n</ol>\n\n<p>Is that something that would make enough sense to you to find the offer appealing? If yes or no, what would be your concerns?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/8tw5-4Js4Q1qtSkYMfhYIRSvoZmd4IUsexRSBuEmtXI.jpg?auto=webp&s=af141fe2bd26c07a24be4737a80a7c47b25aff5c', 'width': 1000, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/8tw5-4Js4Q1qtSkYMfhYIRSvoZmd4IUsexRSBuEmtXI.jpg?width=108&crop=smart&auto=webp&s=cdf9252dbf13230ee52c854facc24811a83a41bd', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/8tw5-4Js4Q1qtSkYMfhYIRSvoZmd4IUsexRSBuEmtXI.jpg?width=216&crop=smart&auto=webp&s=7d3e453126c14f4be6720743dd02ed33f277f28b', 'width': 216, 'height': 86}, {'url': 'https://external-preview.redd.it/8tw5-4Js4Q1qtSkYMfhYIRSvoZmd4IUsexRSBuEmtXI.jpg?width=320&crop=smart&auto=webp&s=44b9600546f6579d0968ad37b37fc766f219f3b6', 'width': 320, 'height': 128}, {'url': 'https://external-preview.redd.it/8tw5-4Js4Q1qtSkYMfhYIRSvoZmd4IUsexRSBuEmtXI.jpg?width=640&crop=smart&auto=webp&s=95fb12137d08798ef11458509482d8e92fab984c', 'width': 640, 'height': 256}, {'url': 'https://external-preview.redd.it/8tw5-4Js4Q1qtSkYMfhYIRSvoZmd4IUsexRSBuEmtXI.jpg?width=960&crop=smart&auto=webp&s=e45b98035cf39628d75d31a447e8163fd01aa256', 'width': 960, 'height': 384}], 'variants': {}, 'id': 'R0SSUTO7qIty8fghfaAP139Ioejk2XTdYyiytggFuJw'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1edc93g', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ev0xmusic'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edc93g/managed_airbyte_on_your_own_kubernetes_cluster/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edc93g/managed_airbyte_on_your_own_kubernetes_cluster/', 'subreddit_subscribers': 201126, 'created_utc': 1722072006.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.803+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone,\n\nI'm working with a large MSSQL table that contains 80 billion records, and I've noticed that the records are not equally distributed across the partition index column. This has been causing performance issues when trying to read and process the data using AWS Glue for ETL operations.\n\nI've been considering various strategies to handle this data more efficiently:\n\n1. Using hashfield and hashpartitions: I understand that I can specify a column for partitioning and the number of partitions, but given the unequal distribution, I'm not sure how to choose the optimal hashfield and number of partitions.\n2. Using hashexpression: AWS Glue allows specifying a custom hash expression, but I'm unsure how to design an expression that would help balance the load across partitions.\n3. Alternative Approaches: Are there other strategies or best practices for efficiently processing such a massive and unevenly distributed dataset in AWS Glue?\n\nAny suggestions or experiences with handling similar scenarios would be greatly appreciated!", 'author_fullname': 't2_jerod9h7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to Efficiently Read an Unequally Distributed 80 Billion Record Table in AWS Glue?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecspgy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722012082.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m working with a large MSSQL table that contains 80 billion records, and I&#39;ve noticed that the records are not equally distributed across the partition index column. This has been causing performance issues when trying to read and process the data using AWS Glue for ETL operations.</p>\n\n<p>I&#39;ve been considering various strategies to handle this data more efficiently:</p>\n\n<ol>\n<li>Using hashfield and hashpartitions: I understand that I can specify a column for partitioning and the number of partitions, but given the unequal distribution, I&#39;m not sure how to choose the optimal hashfield and number of partitions.</li>\n<li>Using hashexpression: AWS Glue allows specifying a custom hash expression, but I&#39;m unsure how to design an expression that would help balance the load across partitions.</li>\n<li>Alternative Approaches: Are there other strategies or best practices for efficiently processing such a massive and unevenly distributed dataset in AWS Glue?</li>\n</ol>\n\n<p>Any suggestions or experiences with handling similar scenarios would be greatly appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ecspgy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='naruto_believe12'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecspgy/how_to_efficiently_read_an_unequally_distributed/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecspgy/how_to_efficiently_read_an_unequally_distributed/', 'subreddit_subscribers': 201126, 'created_utc': 1722012082.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.804+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm rather sick of the way data is collected, shaped and used in the corporate environment. Primarily the practice of shaping data to fit an expected outcome while not necessarily making it obvious that that's what's going on. I enjoyed my time in finance/banking because they actually cared but to be honest the fun work in that industry is reserved for people much much smarter then me lol. \n\nI've been lucky enough to get experience in a very wide range of skills so I'm capable of doing something entirely by myself or at least by myself until I really zero in on something. I just don't know what that something is. The non profit I worked for got started by simply collecting and organizing public data and then using that data to get leverage into private datasets. That sounds halfway interesting but is just one idea and I don't know what data id focus on anyway. \n\nI've also had the idea of getting data from sensors and working on building intelligent things but my hardware engineering skills are rather weak. I can do it but it will be slow ", 'author_fullname': 't2_13hrxymhim', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Experienced data engineer looking for independent project ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecs97t', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722010954.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m rather sick of the way data is collected, shaped and used in the corporate environment. Primarily the practice of shaping data to fit an expected outcome while not necessarily making it obvious that that&#39;s what&#39;s going on. I enjoyed my time in finance/banking because they actually cared but to be honest the fun work in that industry is reserved for people much much smarter then me lol. </p>\n\n<p>I&#39;ve been lucky enough to get experience in a very wide range of skills so I&#39;m capable of doing something entirely by myself or at least by myself until I really zero in on something. I just don&#39;t know what that something is. The non profit I worked for got started by simply collecting and organizing public data and then using that data to get leverage into private datasets. That sounds halfway interesting but is just one idea and I don&#39;t know what data id focus on anyway. </p>\n\n<p>I&#39;ve also had the idea of getting data from sensors and working on building intelligent things but my hardware engineering skills are rather weak. I can do it but it will be slow </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecs97t', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='soggyGreyDuck'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecs97t/experienced_data_engineer_looking_for_independent/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecs97t/experienced_data_engineer_looking_for_independent/', 'subreddit_subscribers': 201126, 'created_utc': 1722010954.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.804+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I learnt pyspark but it's all theory. I do not know any optimisation techniques and other things being asked in interviews. I do not even know anything about streaming and making data pipelines etc. \n\nI work on snowflake and python and make dashboards using dash and gradio/streamlit sometimes. \n\nI mentioned in a previous post in another sub that snowflake is making me bored and I want to do something better. \n\nI am appearing for jobs which needs spark experience. Can anyone help me on how to learn or prectice for interviews or just to be better in databricks? ", 'author_fullname': 't2_9peiwedk', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Need some help about being good in databricks...', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecnui9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721999809.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I learnt pyspark but it&#39;s all theory. I do not know any optimisation techniques and other things being asked in interviews. I do not even know anything about streaming and making data pipelines etc. </p>\n\n<p>I work on snowflake and python and make dashboards using dash and gradio/streamlit sometimes. </p>\n\n<p>I mentioned in a previous post in another sub that snowflake is making me bored and I want to do something better. </p>\n\n<p>I am appearing for jobs which needs spark experience. Can anyone help me on how to learn or prectice for interviews or just to be better in databricks? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ecnui9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LucaMarko'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecnui9/need_some_help_about_being_good_in_databricks/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecnui9/need_some_help_about_being_good_in_databricks/', 'subreddit_subscribers': 201126, 'created_utc': 1721999809.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.805+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Have you, what did you think?', 'author_fullname': 't2_lnwagoki', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Have you used data vault in production?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef8ge4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722281420.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Have you, what did you think?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ef8ge4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AMDataLake'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef8ge4/have_you_used_data_vault_in_production/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef8ge4/have_you_used_data_vault_in_production/', 'subreddit_subscribers': 201126, 'created_utc': 1722281420.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.805+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'My understanding is that flattening a JSON response just lands the data all in one table, whereas normalizing will break the data into a main table and sub tables.', 'author_fullname': 't2_msdl59aul', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there a difference between flattening or normalizing a JSON response and if so what is the value or use case?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef0dpy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722261920.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My understanding is that flattening a JSON response just lands the data all in one table, whereas normalizing will break the data into a main table and sub tables.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ef0dpy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='isyeetstillathing'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef0dpy/is_there_a_difference_between_flattening_or/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef0dpy/is_there_a_difference_between_flattening_or/', 'subreddit_subscribers': 201126, 'created_utc': 1722261920.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.806+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_jvff1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Running Iceberg + DuckDB on Google Cloud', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eex2au', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/mMrBp4qOoVEL6bstr4Yh2CAmGb4kCKPUu1J6QGaGXwA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722252326.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'definite.app', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.definite.app/blog/cloud-iceberg-duckdb', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/dwoXiSQYzdMov5iNKTQC5LFyyBwceB4TxVAuD61RktE.jpg?auto=webp&s=97d69d36f008561a3840c187bcdce0e1a191fbbd', 'width': 1024, 'height': 1024}, 'resolutions': [{'url': 'https://external-preview.redd.it/dwoXiSQYzdMov5iNKTQC5LFyyBwceB4TxVAuD61RktE.jpg?width=108&crop=smart&auto=webp&s=23a9f1c7cb2157810b5bf3e3f330f567eb64231d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/dwoXiSQYzdMov5iNKTQC5LFyyBwceB4TxVAuD61RktE.jpg?width=216&crop=smart&auto=webp&s=09acae0466c4195266800849eebe977cfea78927', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/dwoXiSQYzdMov5iNKTQC5LFyyBwceB4TxVAuD61RktE.jpg?width=320&crop=smart&auto=webp&s=d8224c72cd9bca5168777606f84b5714861699e6', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/dwoXiSQYzdMov5iNKTQC5LFyyBwceB4TxVAuD61RktE.jpg?width=640&crop=smart&auto=webp&s=406589c7b140a15f31e752eed179f28e07ea5bb9', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/dwoXiSQYzdMov5iNKTQC5LFyyBwceB4TxVAuD61RktE.jpg?width=960&crop=smart&auto=webp&s=7c4740c62a25aecb5e975afa4280bda6f090c660', 'width': 960, 'height': 960}], 'variants': {}, 'id': 'gOp3qSEhRf5vBAFYNnRM8ZukiPfb6XRV2asob-3JSwU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1eex2au', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='howMuchCheeseIs2Much'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eex2au/running_iceberg_duckdb_on_google_cloud/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.definite.app/blog/cloud-iceberg-duckdb', 'subreddit_subscribers': 201126, 'created_utc': 1722252326.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.807+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\n\nI\'m facing an issue and hope someone here has had similar experiences and can help me out.\n\nI initially created a PostgreSQL Flexible Server database with Entra ID authentication, which worked without any issues.\n\nNow, I want to deploy a Python script in Azure Functions that accesses this database. Locally, everything worked fine, but in Azure Functions, I can\'t get the authentication to work. The firewall settings are configured to allow Azure Functions to access the database.\n\nIn the Azure Function App, I have enabled the system-assigned managed identity under "Identity" and assigned this identity as an admin to the database. However, it still doesn\'t work.\n\nHas anyone tried something similar and can perhaps recommend a video or guide? Everything I\'ve tried so far hasn\'t worked or was outdated and used Active Directory. I\'m getting frustrated.\n\nLocally, it worked because I used my personal account.\n\nThanks in advance for your help!', 'author_fullname': 't2_arbz8km', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Not able to connect to Postgres with azure function and Entra Auth', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecp00o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722002889.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m facing an issue and hope someone here has had similar experiences and can help me out.</p>\n\n<p>I initially created a PostgreSQL Flexible Server database with Entra ID authentication, which worked without any issues.</p>\n\n<p>Now, I want to deploy a Python script in Azure Functions that accesses this database. Locally, everything worked fine, but in Azure Functions, I can&#39;t get the authentication to work. The firewall settings are configured to allow Azure Functions to access the database.</p>\n\n<p>In the Azure Function App, I have enabled the system-assigned managed identity under &quot;Identity&quot; and assigned this identity as an admin to the database. However, it still doesn&#39;t work.</p>\n\n<p>Has anyone tried something similar and can perhaps recommend a video or guide? Everything I&#39;ve tried so far hasn&#39;t worked or was outdated and used Active Directory. I&#39;m getting frustrated.</p>\n\n<p>Locally, it worked because I used my personal account.</p>\n\n<p>Thanks in advance for your help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ecp00o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Lagiol'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecp00o/not_able_to_connect_to_***_with_azure/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecp00o/not_able_to_connect_to_***_with_azure/', 'subreddit_subscribers': 201126, 'created_utc': 1722002889.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.808+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Im the only DE where i work but i have a bunch of analyst hitting my systems. I recently implemented dataform to provide a CI/CD pipeline for all ETL scripts in sql that are needed/being built for down stream reports. \n\nAll the scripts are version controlled and once pr'd to the main branch can be put into a release schedule. \n\nThe pipelines can then be constructed together how ever you please as all the individual sqlx files are available to orchestrate. Basically a SWD system for analysts, they love it! \n\nWondered if anyone else has had chane to use this yet and wondered what your thoughts are? ", 'author_fullname': 't2_4z4e8iqf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bigquery Dataform usage', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecn7f9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721997967.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Im the only DE where i work but i have a bunch of analyst hitting my systems. I recently implemented dataform to provide a CI/CD pipeline for all ETL scripts in sql that are needed/being built for down stream reports. </p>\n\n<p>All the scripts are version controlled and once pr&#39;d to the main branch can be put into a release schedule. </p>\n\n<p>The pipelines can then be constructed together how ever you please as all the individual sqlx files are available to orchestrate. Basically a SWD system for analysts, they love it! </p>\n\n<p>Wondered if anyone else has had chane to use this yet and wondered what your thoughts are? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecn7f9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Flashy_Ai'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecn7f9/bigquery_dataform_usage/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecn7f9/bigquery_dataform_usage/', 'subreddit_subscribers': 201126, 'created_utc': 1721997967.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.809+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am currently working in data analytics and am trying to move towards the data engineering role. \n\nMy current skillset:\nI use SQL heavily at work and am starting to learn python. I also have a homelab where I run multiple services so I am a little familiar with Linux as well as networking. At least enough to figure things out with google searches (usually). We use snowflake at work.\n\nMy project idea:\nI would like to do a project where I gather my bank transactions from an API and move that data through some kind of pipeline to eventually analyze and create dashboards in power bi. (I have successfully been able to pull this data with teller.io in python). The data here is not important, Iâ€™m more asking about the pipeline related stuff.\n\nMy question/ requested advice:\nWith this project I want to try to maximize my learning of real world skills that a data engineer might use, so I donâ€™t necessarily just want to drop the data in one database and call it good. It might not make sense logistically, but if it means I learn more skills I am happy pushing the data through multiple systems to get experience. For example Iâ€™ve thought about pushing the raw api response to an S3 bucket, grabbing it out of there and parsing it, then transforming and moving it to another DB for consumption. Or something like that. I just donâ€™t know all the possibilities or what the best possibilities are for learning the craft. Anyone have any advice on what I might do to maximize my learning from this project as a semi-beginner?\n\nNotes:\nI understand bank transactions are sensitive data, Iâ€™m not going to be using my real bank. \n\nIâ€™d like this to be cheap if possible, but if thereâ€™s a commonly used product in the field (like snowflake for example) I might be willing to spend some money if itâ€™s reasonable\n\nApologies for my ignorance on the data engineering lingo if I misused any words and thanks for your time in reading this\n\n', 'author_fullname': 't2_t9cvlrkt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Learning Data Engineering (Project Advice)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eh11bc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722466462.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am currently working in data analytics and am trying to move towards the data engineering role. </p>\n\n<p>My current skillset:\nI use SQL heavily at work and am starting to learn python. I also have a homelab where I run multiple services so I am a little familiar with Linux as well as networking. At least enough to figure things out with google searches (usually). We use snowflake at work.</p>\n\n<p>My project idea:\nI would like to do a project where I gather my bank transactions from an API and move that data through some kind of pipeline to eventually analyze and create dashboards in power bi. (I have successfully been able to pull this data with teller.io in python). The data here is not important, Iâ€™m more asking about the pipeline related stuff.</p>\n\n<p>My question/ requested advice:\nWith this project I want to try to maximize my learning of real world skills that a data engineer might use, so I donâ€™t necessarily just want to drop the data in one database and call it good. It might not make sense logistically, but if it means I learn more skills I am happy pushing the data through multiple systems to get experience. For example Iâ€™ve thought about pushing the raw api response to an S3 bucket, grabbing it out of there and parsing it, then transforming and moving it to another DB for consumption. Or something like that. I just donâ€™t know all the possibilities or what the best possibilities are for learning the craft. Anyone have any advice on what I might do to maximize my learning from this project as a semi-beginner?</p>\n\n<p>Notes:\nI understand bank transactions are sensitive data, Iâ€™m not going to be using my real bank. </p>\n\n<p>Iâ€™d like this to be cheap if possible, but if thereâ€™s a commonly used product in the field (like snowflake for example) I might be willing to spend some money if itâ€™s reasonable</p>\n\n<p>Apologies for my ignorance on the data engineering lingo if I misused any words and thanks for your time in reading this</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eh11bc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Spiritual-Battle5723'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eh11bc/learning_data_engineering_project_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eh11bc/learning_data_engineering_project_advice/', 'subreddit_subscribers': 201126, 'created_utc': 1722466462.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.809+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\nIâ€™d love some inputs on how you are approaching data transformations in cloud data stores such as snowflake - are you writing long and complex procedures to temp/persisted table to end with a result? Or are you storing it in views for transparancy?\n\nI know that traditionally, stacking views on top of each other with complex logic creates quite a performance overhead in say SQL Server - is that your experience in e.g. Snowflake as well?\n\nBest\n', 'author_fullname': 't2_k29ghut6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Stacking views vs writing procedures', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eexjkp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722253918.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>Iâ€™d love some inputs on how you are approaching data transformations in cloud data stores such as snowflake - are you writing long and complex procedures to temp/persisted table to end with a result? Or are you storing it in views for transparancy?</p>\n\n<p>I know that traditionally, stacking views on top of each other with complex logic creates quite a performance overhead in say SQL Server - is that your experience in e.g. Snowflake as well?</p>\n\n<p>Best</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eexjkp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Responsible_Roof_253'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eexjkp/stacking_views_vs_writing_procedures/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eexjkp/stacking_views_vs_writing_procedures/', 'subreddit_subscribers': 201126, 'created_utc': 1722253918.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.809+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'This article is for folks looking to understand a bit more about DE and transition into DE.\n\nLearn different types of focus areas for a Data Engineer in the modern industry with future articles covering the specific roadmaps. \n\nIn future will dive into roadmaps of the following transitions: \n- Software Engineer to Data Engineer \n- Data Scientist to Data Engineer \n- Data Analyst to Data Engineer\n\nLet me know you thoughts if you think I should add more areas and transition for future.\n\n', 'author_fullname': 't2_dhgy4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Types of Data Engineers', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edkty9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/C77CFBHvj_auEfR9B3Z8NevW4GKBQrno_Po4PsZqalQ.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722099034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'junaideffendi.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>This article is for folks looking to understand a bit more about DE and transition into DE.</p>\n\n<p>Learn different types of focus areas for a Data Engineer in the modern industry with future articles covering the specific roadmaps. </p>\n\n<p>In future will dive into roadmaps of the following transitions: \n- Software Engineer to Data Engineer \n- Data Scientist to Data Engineer \n- Data Analyst to Data Engineer</p>\n\n<p>Let me know you thoughts if you think I should add more areas and transition for future.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&utm_campaign=post&utm_medium=web', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?auto=webp&s=ee64b6b78ce376a08056b2817e0627e0b93080a3', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?width=108&crop=smart&auto=webp&s=749fbb51c15bf46a32f9e537894930b0162a291c', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?width=216&crop=smart&auto=webp&s=6d6dafa018b475b5387c21a520d6d43f85ae5193', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?width=320&crop=smart&auto=webp&s=02a5f6aec34d1f5f4f5a8c096a971daf72714db6', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?width=640&crop=smart&auto=webp&s=8b7a6f5cbd9521eed3d70cc602546269666f8976', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?width=960&crop=smart&auto=webp&s=6e605041a9b5ef3551c6cf89f3b42219500bce02', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/nAqWUCUdXwDBqrOPFPpt5Cf9metSeBSdEB1PfI2DL6Y.jpg?width=1080&crop=smart&auto=webp&s=a645ebfe3ffd210e84018414974ac5352df1b062', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'eV9qRYXTptxHsPjPSAMu6aQUvTYXoSLtHq9iKObvA08'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1edkty9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mjfnd'), 'discussion_type': None, 'num_comments': 5, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edkty9/types_of_data_engineers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&utm_campaign=post&utm_medium=web', 'subreddit_subscribers': 201126, 'created_utc': 1722099034.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.810+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello guys,\n\nI have a project in which I have to collect aggregate data for each customer from one big table. In banking an example could be, a customer having an id, purchase\\_amount, money\\_conversion\\_amount columns and in table it is stored as  \nid,    purch.,   mon.,   date  \n100,   85,      200,     2024-07-26  \n100,   12,       0,        2024-07-25  \n101,   34,      10,       2024-07-26  \n100,   11,      56,       2024-07-24  \n101,  10,       0,        2024-07-25\n\nso aggregate data for each use stored in one big table  \nMy project aims to have one more aggregate table having this columns:  \nid, purchases\\_sum\\_last1day, purchases\\_sum\\_last3day, purchases\\_sum\\_1month, money\\_conversion\\_amount\\_sum\\_last1day .....  \naggregate functions are sum, min, max and avg  \nData is stored on data lake (hdfs) and we are using spark as well.  \nRight now I have a working application but I am not happy with the performance, it reads a config file and generated a very long sql query and executes it with spark.  \nI would like to get ideas about how efficiently I can handle the project (like having metadata table or using streaming somehow).  \n', 'author_fullname': 't2_hp6qa6mb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help with Data Catalog application architecture', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecj36s', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721982933.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello guys,</p>\n\n<p>I have a project in which I have to collect aggregate data for each customer from one big table. In banking an example could be, a customer having an id, purchase_amount, money_conversion_amount columns and in table it is stored as<br/>\nid,    purch.,   mon.,   date<br/>\n100,   85,      200,     2024-07-26<br/>\n100,   12,       0,        2024-07-25<br/>\n101,   34,      10,       2024-07-26<br/>\n100,   11,      56,       2024-07-24<br/>\n101,  10,       0,        2024-07-25</p>\n\n<p>so aggregate data for each use stored in one big table<br/>\nMy project aims to have one more aggregate table having this columns:<br/>\nid, purchases_sum_last1day, purchases_sum_last3day, purchases_sum_1month, money_conversion_amount_sum_last1day .....<br/>\naggregate functions are sum, min, max and avg<br/>\nData is stored on data lake (hdfs) and we are using spark as well.<br/>\nRight now I have a working application but I am not happy with the performance, it reads a config file and generated a very long sql query and executes it with spark.<br/>\nI would like to get ideas about how efficiently I can handle the project (like having metadata table or using streaming somehow).  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ecj36s', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='South-Hedgehog-6763'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecj36s/help_with_data_catalog_application_architecture/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecj36s/help_with_data_catalog_application_architecture/', 'subreddit_subscribers': 201126, 'created_utc': 1721982933.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.810+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Iâ€™ve been working as a data engineer for 3,5 years, working mostly with databricks and AWS, and various other tools depending on the need.\n\nI received an offer from another company with a significant pay rise and better working conditions but the title of the job is â€œfunctional analystâ€. I donâ€™t know a lot about this type of role other than itâ€™s some sort of manager position, if anyone has any insights, itâ€™s more than appreciated.\n\nAnyway, Iâ€™m just afraid that by accepting this offer, Iâ€™ll say goodbye to the technical aspect of the engineering/analysis process like coding, mathematics and exploring new possibilities.\n\nAre my concerns justified or totally not? ', 'author_fullname': 't2_fm0saa3wu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Move from a DE role to a Functional Analyst, but afraid to lose the technical/coding side of the job', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ecibxj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721979789.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Iâ€™ve been working as a data engineer for 3,5 years, working mostly with databricks and AWS, and various other tools depending on the need.</p>\n\n<p>I received an offer from another company with a significant pay rise and better working conditions but the title of the job is â€œfunctional analystâ€. I donâ€™t know a lot about this type of role other than itâ€™s some sort of manager position, if anyone has any insights, itâ€™s more than appreciated.</p>\n\n<p>Anyway, Iâ€™m just afraid that by accepting this offer, Iâ€™ll say goodbye to the technical aspect of the engineering/analysis process like coding, mathematics and exploring new possibilities.</p>\n\n<p>Are my concerns justified or totally not? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ecibxj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='spez_ass'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ecibxj/move_from_a_de_role_to_a_functional_analyst_but/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ecibxj/move_from_a_de_role_to_a_functional_analyst_but/', 'subreddit_subscribers': 201126, 'created_utc': 1721979789.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.811+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Got tasked with building a pipeline from MySQL to big query. \n\nMy plan was utilizing an ELT? I was planning on querying from MySQL, loading the data to big query and then transforming the data in dbt to create models. \n\nAm I going at it correctly? \n\nAny tips would be greatly appreciated. \n\nThank you! ', 'author_fullname': 't2_tl9jpgpxc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Batch of data from MySQL and Big  Query', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ec8x7v', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721949034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Got tasked with building a pipeline from MySQL to big query. </p>\n\n<p>My plan was utilizing an ELT? I was planning on querying from MySQL, loading the data to big query and then transforming the data in dbt to create models. </p>\n\n<p>Am I going at it correctly? </p>\n\n<p>Any tips would be greatly appreciated. </p>\n\n<p>Thank you! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ec8x7v', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mindless-Repair6475'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ec8x7v/batch_of_data_from_mysql_and_big_query/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ec8x7v/batch_of_data_from_mysql_and_big_query/', 'subreddit_subscribers': 201126, 'created_utc': 1721949034.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.811+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello thanks for taking the time.\n\nI\'m a solo developer for an employer who runs a small agency-- basically him and his industry expertise along with me and one other employee. He\'s brought me on full time because he wants to build out some data services he can offer to his clients with hopes of being able to move away from the traditional agency people / project hand holding strategy into something more scalable-- basically selling curated datasets and expert analysis with some light consulting on the side.\n\nI\'m building everything in GCP and pulling from two data sources. Right now the pipeline is running on airflow pulling data from the first API, storing that raw JSON in GCS, as well as performing some transformations on the data to get it to fit our desired schema for BigQuery, saving the transformed JSON, and then running a batch load operation to put the data into BigQuery. Then a request is built off of that data for the second data source (think "top ten" or something like that), and the same process is undergone on the second data source.\n\nThe process so far is relatively slow, given the API rate limits but also some slowness in processing. I am wary of over-optimizing or over-engineering since I am a single engineer team, and we have other projects ongoing-- however I am curious if someone can answer as to whether it would make sense to perform the data transformations from the raw JSON to the formatted JSON using Spark. Partly this is because I\'m interested in learning Spark for its own sake (using the Python API), so there is that. I have of course consulted ChatGPT but it tends to just tell me some version of what I might want to hear i.e. yes you could speed it up with Spark. But what I really want to ask from the human developer perspective is does it make any sense to do it this way or is this over-engineering or inelegant? I\'m ok making a bit of an effort to learn the new skill and jerry-rig it to work with my pipeline, like I said I do selfishly just want to apply the tool so that i can put it on my cv, but is this even a useful application of Spark or would I be better of just learning it on some kind of hobby project?\n\nEDIT: Reading the replies and reflecting, sounds like the answer is "no", Spark doesn\'t really make sense here. The main bottleneck is definitely all of the network-bound operations, and looking back the transformations I\'m making are minimal, and I\'ve optimized the code pretty much as well as I can using Python (list comprehension :\'-D). To be honest, the best way to speed this up will really just be to increase the worker count in Cloud Composer, still a long way for me to go there. I will however consider using an ELT approach next time for this kind of thing, as that was definitely where my mind was going with thinking of introducing Spark. Ultimately I\'m still thinking about a way to gain some experience using Spark without myself having to pay to host PB of data! I was hoping I could make it work for this but I\'m sure i\'ll find some opportunity. Thanks for the feedback everyone.', 'author_fullname': 't2_r0324by', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Will Spark speed up my pipeline, Airflow, BigQuery', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehlg5u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722538452.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722530795.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello thanks for taking the time.</p>\n\n<p>I&#39;m a solo developer for an employer who runs a small agency-- basically him and his industry expertise along with me and one other employee. He&#39;s brought me on full time because he wants to build out some data services he can offer to his clients with hopes of being able to move away from the traditional agency people / project hand holding strategy into something more scalable-- basically selling curated datasets and expert analysis with some light consulting on the side.</p>\n\n<p>I&#39;m building everything in GCP and pulling from two data sources. Right now the pipeline is running on airflow pulling data from the first API, storing that raw JSON in GCS, as well as performing some transformations on the data to get it to fit our desired schema for BigQuery, saving the transformed JSON, and then running a batch load operation to put the data into BigQuery. Then a request is built off of that data for the second data source (think &quot;top ten&quot; or something like that), and the same process is undergone on the second data source.</p>\n\n<p>The process so far is relatively slow, given the API rate limits but also some slowness in processing. I am wary of over-optimizing or over-engineering since I am a single engineer team, and we have other projects ongoing-- however I am curious if someone can answer as to whether it would make sense to perform the data transformations from the raw JSON to the formatted JSON using Spark. Partly this is because I&#39;m interested in learning Spark for its own sake (using the Python API), so there is that. I have of course consulted ChatGPT but it tends to just tell me some version of what I might want to hear i.e. yes you could speed it up with Spark. But what I really want to ask from the human developer perspective is does it make any sense to do it this way or is this over-engineering or inelegant? I&#39;m ok making a bit of an effort to learn the new skill and jerry-rig it to work with my pipeline, like I said I do selfishly just want to apply the tool so that i can put it on my cv, but is this even a useful application of Spark or would I be better of just learning it on some kind of hobby project?</p>\n\n<p>EDIT: Reading the replies and reflecting, sounds like the answer is &quot;no&quot;, Spark doesn&#39;t really make sense here. The main bottleneck is definitely all of the network-bound operations, and looking back the transformations I&#39;m making are minimal, and I&#39;ve optimized the code pretty much as well as I can using Python (list comprehension :&#39;-D). To be honest, the best way to speed this up will really just be to increase the worker count in Cloud Composer, still a long way for me to go there. I will however consider using an ELT approach next time for this kind of thing, as that was definitely where my mind was going with thinking of introducing Spark. Ultimately I&#39;m still thinking about a way to gain some experience using Spark without myself having to pay to host PB of data! I was hoping I could make it work for this but I&#39;m sure i&#39;ll find some opportunity. Thanks for the feedback everyone.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ehlg5u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nem03'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehlg5u/will_spark_speed_up_my_pipeline_airflow_bigquery/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehlg5u/will_spark_speed_up_my_pipeline_airflow_bigquery/', 'subreddit_subscribers': 201126, 'created_utc': 1722530795.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.812+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)', 'author_fullname': 't2_6l4z3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Monthly General Discussion - Aug 2024', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ehkasi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722528031.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.</p>\n\n<p>Examples:</p>\n\n<ul>\n<li>What are you working on this month?</li>\n<li>What was something you accomplished?</li>\n<li>What was something you learned recently?</li>\n<li>What is something frustrating you currently?</li>\n</ul>\n\n<p>As always, sub rules apply. Please be respectful and stay curious.</p>\n\n<p><strong>Community Links:</strong></p>\n\n<ul>\n<li><a href="https://dataengineeringcommunity.substack.com/">Monthly newsletter</a></li>\n<li><a href="https://dataengineering.wiki/Community/Events">Data Engineering Events</a></li>\n<li><a href="https://dataengineering.wiki/Community/Meetups">Data Engineering Meetups</a></li>\n<li><a href="https://dataengineering.wiki/Community/Get+Involved">Get involved in the community</a></li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?auto=webp&s=38495eca7177fe7952a28f7cb7ec78b3735d7ce7', 'width': 920, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=108&crop=smart&auto=webp&s=7182d4e8f2ab9018d01089cac5deea3bb6adee65', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=216&crop=smart&auto=webp&s=3b48371d41aeeaed44e3c9bd50975ea3612d6e21', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=320&crop=smart&auto=webp&s=01226e541c733f4f617393c51347c7276a330624', 'width': 320, 'height': 166}, {'url': 'https://external-preview.redd.it/QAz6_6X9UZnBeLhvhZTunbh7T_9BXorkRTA48bN-mQA.jpg?width=640&crop=smart&auto=webp&s=bdc1a1eebb35c952b90cf0f903aa3c71ca25c607', 'width': 640, 'height': 333}], 'variants': {}, 'id': 'xl1dl_PUt0lnmjNhPzrvZZDAwfJuW9dYKPmR729jqfA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ehkasi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AutoModerator'), 'discussion_type': None, 'num_comments': 3, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ehkasi/monthly_general_discussion_aug_2024/', 'parent_whitelist_status': 'all_ads', 'stickied': True, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ehkasi/monthly_general_discussion_aug_2024/', 'subreddit_subscribers': 201126, 'created_utc': 1722528031.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.812+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey folks, Iâ€™ve recently opened a subreddit specifically for streaming & real-time data discussions, would love for other fans of the domain to join!\n\nCheck it out: /r/streamingdata', 'author_fullname': 't2_cqao8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Community for Streaming Data Enthusiasts', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egq3xy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.73, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722439874.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey folks, Iâ€™ve recently opened a subreddit specifically for streaming &amp; real-time data discussions, would love for other fans of the domain to join!</p>\n\n<p>Check it out: <a href="/r/streamingdata">/r/streamingdata</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egq3xy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dan_the_lion'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egq3xy/community_for_streaming_data_enthusiasts/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egq3xy/community_for_streaming_data_enthusiasts/', 'subreddit_subscribers': 201126, 'created_utc': 1722439874.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.813+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Title ', 'author_fullname': 't2_7owm6ym1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Redshift serverless v/s Databricks - which is better/ more price performant?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg10dq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.68, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722364668.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Title </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eg10dq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok-Tradition-3450'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg10dq/redshift_serverless_vs_databricks_which_is_better/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg10dq/redshift_serverless_vs_databricks_which_is_better/', 'subreddit_subscribers': 201126, 'created_utc': 1722364668.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.813+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi all. \xa0\n\nI am relatively new to using Azure.\xa0 We have a cloud based ETL application which will on occasion require reading/consuming CSV, Excel and other data files and also want to write ETL log files.\xa0 Our company is currently using Azure and I have done some reading about Azure storage but I am confused as to what I should be using within Azure to store/consume flat files into / out of our ETL solution.\xa0 These files will typically not be large or that numerous (our may way of processing source data is via Fivetran/Snowflake).\xa0 These flat files would generally be certain data sets which don't come from an application, are usually small in size.\n\nI understand that there is Azure blob storage but also Azure provides Data lake storage Gen2 (support hierarchical namespace) which is a part of blob storage.\xa0 \xa0\n\nSo the question is:\xa0 If we want to occasionally read flat files into a cloud ETL solution and write text log files, should we be using Azure Blob storage or the Data lake Gen2?\xa0 \xa0\n\nApologies in advance if I'm not using the proper terms.\xa0 I'm still trying to learn what I need.\xa0\n\nThanks \xa0", 'author_fullname': 't2_cymxj8xi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure blob storage versus Data Lake Gen2.  Which do I need to be able to store/read misc csv and log files?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef6ulr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722277555.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all. \xa0</p>\n\n<p>I am relatively new to using Azure.\xa0 We have a cloud based ETL application which will on occasion require reading/consuming CSV, Excel and other data files and also want to write ETL log files.\xa0 Our company is currently using Azure and I have done some reading about Azure storage but I am confused as to what I should be using within Azure to store/consume flat files into / out of our ETL solution.\xa0 These files will typically not be large or that numerous (our may way of processing source data is via Fivetran/Snowflake).\xa0 These flat files would generally be certain data sets which don&#39;t come from an application, are usually small in size.</p>\n\n<p>I understand that there is Azure blob storage but also Azure provides Data lake storage Gen2 (support hierarchical namespace) which is a part of blob storage.\xa0 \xa0</p>\n\n<p>So the question is:\xa0 If we want to occasionally read flat files into a cloud ETL solution and write text log files, should we be using Azure Blob storage or the Data lake Gen2?\xa0 \xa0</p>\n\n<p>Apologies in advance if I&#39;m not using the proper terms.\xa0 I&#39;m still trying to learn what I need.\xa0</p>\n\n<p>Thanks \xa0</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ef6ulr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='GreyHairedDWGuy'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef6ulr/azure_blob_storage_versus_data_lake_gen2_which_do/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef6ulr/azure_blob_storage_versus_data_lake_gen2_which_do/', 'subreddit_subscribers': 201126, 'created_utc': 1722277555.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.814+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Iâ€™m looking to integrate DBT into my tech stack. Currently have around 150ish stored procedures in snowflake and looking for ways to add DBT without having to rewrite every single one. Any tips?', 'author_fullname': 't2_a3vkk6pj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBT migration', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ef43et', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722270963.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Iâ€™m looking to integrate DBT into my tech stack. Currently have around 150ish stored procedures in snowflake and looking for ways to add DBT without having to rewrite every single one. Any tips?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ef43et', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Equal_Piglet1234'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ef43et/dbt_migration/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ef43et/dbt_migration/', 'subreddit_subscribers': 201126, 'created_utc': 1722270963.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.814+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm a data analyst moving to data engineering and starting my first data engineering **PORTFOLIO PROJECT** using **Anime** dataset (I LOVE ANIME!)\n\n1. Is anime okay to choose as project center? I'm scared to be not taken seriously when it's time to share the project on LinkedIn\n\n2. In the data engineering field, does portfolio projects matter in hiring process? \xa0\n\n  \ndataset URL: [Jikan REST API v4 Docs](https://docs.api.jikan.moe/#section/Information)", 'author_fullname': 't2_mpil1bv3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '\n1st Portfolio DE PROJECT: ANIME ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1edmd7v', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722103118.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m a data analyst moving to data engineering and starting my first data engineering <strong>PORTFOLIO PROJECT</strong> using <strong>Anime</strong> dataset (I LOVE ANIME!)</p>\n\n<ol>\n<li><p>Is anime okay to choose as project center? I&#39;m scared to be not taken seriously when it&#39;s time to share the project on LinkedIn</p></li>\n<li><p>In the data engineering field, does portfolio projects matter in hiring process? \xa0</p></li>\n</ol>\n\n<p>dataset URL: <a href="https://docs.api.jikan.moe/#section/Information">Jikan REST API v4 Docs</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-rPplENymT-Ng2G1GF1umPVmn23BR1DHWq_yAPUzYVc.jpg?auto=webp&s=a57d7d38184c743539b59506a0b14fe01fa62ea5', 'width': 1000, 'height': 540}, 'resolutions': [{'url': 'https://external-preview.redd.it/-rPplENymT-Ng2G1GF1umPVmn23BR1DHWq_yAPUzYVc.jpg?width=108&crop=smart&auto=webp&s=49bad437f7ab8195d77725ca90670e8ae92ea0e7', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/-rPplENymT-Ng2G1GF1umPVmn23BR1DHWq_yAPUzYVc.jpg?width=216&crop=smart&auto=webp&s=51939dc3b7ba726cda636e0c9a7892bcceab79bc', 'width': 216, 'height': 116}, {'url': 'https://external-preview.redd.it/-rPplENymT-Ng2G1GF1umPVmn23BR1DHWq_yAPUzYVc.jpg?width=320&crop=smart&auto=webp&s=a933192fe2f845d1b56d4cc5467422cd99c1ff68', 'width': 320, 'height': 172}, {'url': 'https://external-preview.redd.it/-rPplENymT-Ng2G1GF1umPVmn23BR1DHWq_yAPUzYVc.jpg?width=640&crop=smart&auto=webp&s=83ebe1ed149dc745b502a1551c735568534adb89', 'width': 640, 'height': 345}, {'url': 'https://external-preview.redd.it/-rPplENymT-Ng2G1GF1umPVmn23BR1DHWq_yAPUzYVc.jpg?width=960&crop=smart&auto=webp&s=503ebcecb5c87451cd224b123aed67724b82eca0', 'width': 960, 'height': 518}], 'variants': {}, 'id': 'BxujkBUeHeLkS5WX2wV4NzG7VpzqrnurObeJwmsNyQU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1edmd7v', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Rude-Avocado-226'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1edmd7v/1st_portfolio_de_project_anime/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1edmd7v/1st_portfolio_de_project_anime/', 'subreddit_subscribers': 201126, 'created_utc': 1722103118.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.814+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone! Iâ€™m a data scientist but 50% of my job is also developing and owning dbt models. Genuine question for all you folks. Is it just me or are the current ways of exploring and productionizing sql models lackluster? Iâ€™ve tried using notebooks to help visualize the evolution of my data, opened multiple tabs in IDEs and yet bugs creep into my production code. I think the problem is having to refactor spaghetti code (which is a first necessary step to understand your data) and reviewing hundreds of lines of code is just not optimal. I think my reviewers and I struggle to keep track of the evolution of data especially when there are hundreds of lines of code. Any thoughts to this and workarounds from your guysâ€™ experiences? ', 'author_fullname': 't2_9zilrtf1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Analytics Code Dev Painpoints', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ec4wqf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721938792.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone! Iâ€™m a data scientist but 50% of my job is also developing and owning dbt models. Genuine question for all you folks. Is it just me or are the current ways of exploring and productionizing sql models lackluster? Iâ€™ve tried using notebooks to help visualize the evolution of my data, opened multiple tabs in IDEs and yet bugs creep into my production code. I think the problem is having to refactor spaghetti code (which is a first necessary step to understand your data) and reviewing hundreds of lines of code is just not optimal. I think my reviewers and I struggle to keep track of the evolution of data especially when there are hundreds of lines of code. Any thoughts to this and workarounds from your guysâ€™ experiences? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ec4wqf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ParfaitRude229'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ec4wqf/analytics_code_dev_painpoints/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ec4wqf/analytics_code_dev_painpoints/', 'subreddit_subscribers': 201126, 'created_utc': 1721938792.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.815+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Greetings from the data science subreddit. What are some good resources to help a data scientist learn best practices for database design and schemas? I donâ€™t necessarily need an absolutely beginners guide, but something I can reference so I donâ€™t make any glaring mistakes while developing a *** db for my team to use.\n', 'author_fullname': 't2_1nhqot00', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data engineering primer for a data scientist', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egzott', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722463078.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Greetings from the data science subreddit. What are some good resources to help a data scientist learn best practices for database design and schemas? I donâ€™t necessarily need an absolutely beginners guide, but something I can reference so I donâ€™t make any glaring mistakes while developing a *** db for my team to use.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1egzott', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AngryDuckling1'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egzott/data_engineering_primer_for_a_data_scientist/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egzott/data_engineering_primer_for_a_data_scientist/', 'subreddit_subscribers': 201126, 'created_utc': 1722463078.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.815+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey folks, dlt cofounder and data engineer here\n\nI'd like to invite you to a comprehensive python ELT workshop. We put together a course with principles first, implementation second about everything worth knowing in ELT with python, from how to build clean robust self healing pipelines, to advanced topics like parallelism, cdc, deployments.\n\nIt's a fit for python first data engineers and data platform builders.\n\nTo take this course, you should understand basic python, how to make web requests, what generators are and ideally also how decorators work, the rest will be taught.\n\nIt's in 2 weeks, you can find more details here  [https://dlthub.com/events](https://dlthub.com/events)\n\nWe will keep running this workshop so if you cannot make this one check back in a couple weeks for the next slot.\n\nWe already took community feedback for the topics of interest and we will keep improving the workshop based on feedback.\n\nThere will be homework and an associated certification. It's free and we will use colab notebook and duckdb as a common dev environment for those who want to code along, so you don't need a credit card for this.\n\nLooking forward to see you there!\n\nif the timeslot is unsuitable, please comment with what you want to see so we can make that happen", 'author_fullname': 't2_uamr9xer', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Invitation: OSS python ELT with dlt, 4 hours, 2 weeks, 1 certification.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egpt8j', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.61, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1722440021.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722439138.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey folks, dlt cofounder and data engineer here</p>\n\n<p>I&#39;d like to invite you to a comprehensive python ELT workshop. We put together a course with principles first, implementation second about everything worth knowing in ELT with python, from how to build clean robust self healing pipelines, to advanced topics like parallelism, cdc, deployments.</p>\n\n<p>It&#39;s a fit for python first data engineers and data platform builders.</p>\n\n<p>To take this course, you should understand basic python, how to make web requests, what generators are and ideally also how decorators work, the rest will be taught.</p>\n\n<p>It&#39;s in 2 weeks, you can find more details here  <a href="https://dlthub.com/events">https://dlthub.com/events</a></p>\n\n<p>We will keep running this workshop so if you cannot make this one check back in a couple weeks for the next slot.</p>\n\n<p>We already took community feedback for the topics of interest and we will keep improving the workshop based on feedback.</p>\n\n<p>There will be homework and an associated certification. It&#39;s free and we will use colab notebook and duckdb as a common dev environment for those who want to code along, so you don&#39;t need a credit card for this.</p>\n\n<p>Looking forward to see you there!</p>\n\n<p>if the timeslot is unsuitable, please comment with what you want to see so we can make that happen</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?auto=webp&s=0d531b981392bb4f1bffc35b3273fa3508e91e1d', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?width=108&crop=smart&auto=webp&s=f249e49bce4ca10f3cc6657b0a167cc214d4dd3b', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?width=216&crop=smart&auto=webp&s=defbac46580caee73b4c56b67c1770b2ae9c388d', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?width=320&crop=smart&auto=webp&s=b8447ccf668e29d7448f6ec6404353f5bf94a527', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?width=640&crop=smart&auto=webp&s=0249bdddf692545a2bd9c79f80e0abaf3ea31079', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?width=960&crop=smart&auto=webp&s=d6ec330cb76ae7f36a44f97e8715b13f154520f5', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/plvgZIQKkzrpC7TPAgmzq6ZxFFpcEuQ4_vFNax75Eoc.jpg?width=1080&crop=smart&auto=webp&s=5c97395cbf021614ae577d91090e3ff639e3b8e7', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '80XWh6vMWhyC9psC_7omSlm6OcBFEiXV7xCEesHMsiM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1egpt8j', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Thinker_Assignment'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egpt8j/invitation_oss_python_elt_with_dlt_4_hours_2/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egpt8j/invitation_oss_python_elt_with_dlt_4_hours_2/', 'subreddit_subscribers': 201126, 'created_utc': 1722439138.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.816+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello data people, Here is an overview: we have a source API, a PostgreSQL database, and a data lake. Currently, we process batch data once daily, but in the future, we plan to include streaming data as well. I need to know about the current scalable solutions that companies are using to ingest data from the source to PostgreSQL or the data lake.', 'author_fullname': 't2_84ztczxp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Ingesting data to postgre and data lake ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egp061', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722437154.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello data people, Here is an overview: we have a source API, a PostgreSQL database, and a data lake. Currently, we process batch data once daily, but in the future, we plan to include streaming data as well. I need to know about the current scalable solutions that companies are using to ingest data from the source to PostgreSQL or the data lake.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1egp061', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='chaachans'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egp061/ingesting_data_to_postgre_and_data_lake/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1egp061/ingesting_data_to_postgre_and_data_lake/', 'subreddit_subscribers': 201126, 'created_utc': 1722437154.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.816+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_2yrrce83', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1egk5wb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'ups': 3, 'total_awards_received': 0, 'media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/-wCzn9gKoUk?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen â€¢ GOTO 2024"></iframe>', 'width': 356, 'scrolling': False, 'height': 200}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen â€¢ GOTO 2024', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/-wCzn9gKoUk?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen â€¢ GOTO 2024"></iframe>', 'author_name': 'GOTO Conferences', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-wCzn9gKoUk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@GOTO-'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/-wCzn9gKoUk?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen â€¢ GOTO 2024"></iframe>', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/1egk5wb', 'height': 200}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/5fn2AJBgr6-cAXiV2c95trdQmM8wXdjXFktnd8CQTDs.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1722423189.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://youtu.be/-wCzn9gKoUk', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/BVQAqbOVFNWEH7hVe3U9VKdiNzdvKKUozrLMSyMTrqY.jpg?auto=webp&s=1aa6d8523efaec31f2f16694ac0d4e2a290f1260', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/BVQAqbOVFNWEH7hVe3U9VKdiNzdvKKUozrLMSyMTrqY.jpg?width=108&crop=smart&auto=webp&s=c97919c44defed96a6259ff1db764dcf10103d08', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/BVQAqbOVFNWEH7hVe3U9VKdiNzdvKKUozrLMSyMTrqY.jpg?width=216&crop=smart&auto=webp&s=be33c0a21bd793f1815a3ad6c09f70dd776b2258', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/BVQAqbOVFNWEH7hVe3U9VKdiNzdvKKUozrLMSyMTrqY.jpg?width=320&crop=smart&auto=webp&s=64edbc325312159af08f535ef51d6890e25dae92', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'VN2KpfcH-Vl5foSZP-1qerxIygFpqFYYpN2MHLjsDpI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1egk5wb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='goto-con'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1egk5wb/a_short_summary_of_the_last_decades_of_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://youtu.be/-wCzn9gKoUk', 'subreddit_subscribers': 201126, 'created_utc': 1722423189.0, 'num_crossposts': 0, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen â€¢ GOTO 2024', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/-wCzn9gKoUk?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen â€¢ GOTO 2024"></iframe>', 'author_name': 'GOTO Conferences', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-wCzn9gKoUk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@GOTO-'}}, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.817+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Receive datasets via email - currently run a job that queries my email account hourly to see if these data sets are received as a precondition to executing downstream ETL.  Does anyone run jobs where the email itself is a trigger and runs the ETL workflow(s)? Basically, in email terms, would like to be able to 'push' instead of 'fetch'.\n\nGoogled this and searched this forum and to my surprise did not find much specific to this question.  If I'm mistaken and this Q has been asked and answered, feel free to roast me.\n\nUsing MS Exchange for email, python for job run (Graph API to pull emails).", 'author_fullname': 't2_13cu3dcc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Email receipt as a job trigger', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg4mt9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722373278.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Receive datasets via email - currently run a job that queries my email account hourly to see if these data sets are received as a precondition to executing downstream ETL.  Does anyone run jobs where the email itself is a trigger and runs the ETL workflow(s)? Basically, in email terms, would like to be able to &#39;push&#39; instead of &#39;fetch&#39;.</p>\n\n<p>Googled this and searched this forum and to my surprise did not find much specific to this question.  If I&#39;m mistaken and this Q has been asked and answered, feel free to roast me.</p>\n\n<p>Using MS Exchange for email, python for job run (Graph API to pull emails).</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eg4mt9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cmc89645'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg4mt9/email_receipt_as_a_job_trigger/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg4mt9/email_receipt_as_a_job_trigger/', 'subreddit_subscribers': 201126, 'created_utc': 1722373278.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.817+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone, \n\nWe have huge onprem database probably with 3000 or more tables with mostly unwanted data - which I do not know ;).\n\nOur task is to build 5-10 microservices to facilitate as a backend to a webapp. These microservices are read only and have their own Azure SQL databases. No worries for data update/write as it will be handled by different system directly to the onprem database. \n\nNow, the main hurdle is performing ETL to those tables and synching the data to corresponding microservice's database. Each microservice might only need data from 4-5 tables and we won't need to go through all the tables in the Onprem db. So, we will basically need 50 tables at max for microservices to run. \n\nOur onprem database is being synced to Snowflake every hour or so. So, we have two options: \n\n1. get data from Snowflake incrementally-> perform ETL ->load  to microservice's database.\n\n2. get data from Onprem data incrementally -> perform ETL -> load to microservice's database.\n\n  \nWhat tools should we use?. My thought is ADF for orchestration and ADB for ETL as the transformations are heavy. \n\nshould we build one pipeline for each microservice? \n\nhow should we handle the incremental load to microservice database? We can have atmost 1 hr latency. \n\nhow can we handle PKs and FKs in the database? \n\nPlease advice. details would be greatly appreciated. I can elaborate more if needed. Thank you!", 'author_fullname': 't2_vorw34ea4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'best option for microservice based service ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eg32t4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722369613.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, </p>\n\n<p>We have huge onprem database probably with 3000 or more tables with mostly unwanted data - which I do not know ;).</p>\n\n<p>Our task is to build 5-10 microservices to facilitate as a backend to a webapp. These microservices are read only and have their own Azure SQL databases. No worries for data update/write as it will be handled by different system directly to the onprem database. </p>\n\n<p>Now, the main hurdle is performing ETL to those tables and synching the data to corresponding microservice&#39;s database. Each microservice might only need data from 4-5 tables and we won&#39;t need to go through all the tables in the Onprem db. So, we will basically need 50 tables at max for microservices to run. </p>\n\n<p>Our onprem database is being synced to Snowflake every hour or so. So, we have two options: </p>\n\n<ol>\n<li><p>get data from Snowflake incrementally-&gt; perform ETL -&gt;load  to microservice&#39;s database.</p></li>\n<li><p>get data from Onprem data incrementally -&gt; perform ETL -&gt; load to microservice&#39;s database.</p></li>\n</ol>\n\n<p>What tools should we use?. My thought is ADF for orchestration and ADB for ETL as the transformations are heavy. </p>\n\n<p>should we build one pipeline for each microservice? </p>\n\n<p>how should we handle the incremental load to microservice database? We can have atmost 1 hr latency. </p>\n\n<p>how can we handle PKs and FKs in the database? </p>\n\n<p>Please advice. details would be greatly appreciated. I can elaborate more if needed. Thank you!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eg32t4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='enthu-gen-ai'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eg32t4/best_option_for_microservice_based_service/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eg32t4/best_option_for_microservice_based_service/', 'subreddit_subscribers': 201126, 'created_utc': 1722369613.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.817+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey! Looking to get input from DEâ€™s.\n\nI work on a web platform, and we currently have our users input their data by uploading a file. We want to make it a bit more simple by having the user be able to select their data source on our platform and then importing the data.\n\nWhat I am looking for is a tool that helps our devs (reduces time) with the step where the user has selected the data from the source and we take it to snowflake. Weâ€™ve worked on two sources, but we want to do a lot more due to demand. Was wondering if there was a tool that we can use on our end to make this easier/scalable, rather than working with each API from each source directly? \n\nEdit: will be looking into airbyte as an option', 'author_fullname': 't2_lh8s5ycdc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any tool/company that can help us pull company data into Snowflake?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1efwcst', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.71, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722353485.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey! Looking to get input from DEâ€™s.</p>\n\n<p>I work on a web platform, and we currently have our users input their data by uploading a file. We want to make it a bit more simple by having the user be able to select their data source on our platform and then importing the data.</p>\n\n<p>What I am looking for is a tool that helps our devs (reduces time) with the step where the user has selected the data from the source and we take it to snowflake. Weâ€™ve worked on two sources, but we want to do a lot more due to demand. Was wondering if there was a tool that we can use on our end to make this easier/scalable, rather than working with each API from each source directly? </p>\n\n<p>Edit: will be looking into airbyte as an option</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1efwcst', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='imjusthereforPMstuff'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1efwcst/any_toolcompany_that_can_help_us_pull_company/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1efwcst/any_toolcompany_that_can_help_us_pull_company/', 'subreddit_subscribers': 201126, 'created_utc': 1722353485.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.818+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm in the process of building an Ai Analytics agent using OpenAI, Langchain and Streamlit. I could use some feedback on my current set up and was hoping some of you might be able to give me some tips.\n\nThe Goal:\nSo the goal is to provide the use with charts and graphs of data that is stored in our semantic layer on Snowflake. \n\nThe Data:\nWe are fortunate enough to have descriptions for every column and naming conventions for columns used in joins. I have created embeddings for all the table names and column descriptions and have put these behind an API that can use a semantic similarity search.\n\nThe Agent:\nI built some functions that can call the API endpoints to get either relevant table names or column names. I then added a function that can fetch a table schema, one that can fetch the data from specified columns from snowflake and one more that can filter the data using pandas. I have provided all these functions as tools to a Langchain agent with a manually written prompt with some guidelines on how to use the tools.\n\nThis set up has given mixed results. When it gets the right table name it can work like a charm, but it still struggles sometimes. For instance when a user is looking for revenue per week it puts daily sales into the search query, or it searches on the article level instead of per store. Sometimes it also looks up the schema of every table to find the right one, using up a lot of tokens.\n\nI feel like I'm moving in the right direction, but I wonder if there are maybe best practices I'm missing, causing me to use to many tokens. Furthermore I hear a lot about people using techniques like DSPy, Knowledge Graph and fine tuning, but I'm not sure whether these would offer (significant) benefits in my case.\n\nAny help/feedback on my approach would be much appreciated!\n", 'author_fullname': 't2_9szmjnv2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'GenAi Analytics Agent', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eeifu5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722202603.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m in the process of building an Ai Analytics agent using OpenAI, Langchain and Streamlit. I could use some feedback on my current set up and was hoping some of you might be able to give me some tips.</p>\n\n<p>The Goal:\nSo the goal is to provide the use with charts and graphs of data that is stored in our semantic layer on Snowflake. </p>\n\n<p>The Data:\nWe are fortunate enough to have descriptions for every column and naming conventions for columns used in joins. I have created embeddings for all the table names and column descriptions and have put these behind an API that can use a semantic similarity search.</p>\n\n<p>The Agent:\nI built some functions that can call the API endpoints to get either relevant table names or column names. I then added a function that can fetch a table schema, one that can fetch the data from specified columns from snowflake and one more that can filter the data using pandas. I have provided all these functions as tools to a Langchain agent with a manually written prompt with some guidelines on how to use the tools.</p>\n\n<p>This set up has given mixed results. When it gets the right table name it can work like a charm, but it still struggles sometimes. For instance when a user is looking for revenue per week it puts daily sales into the search query, or it searches on the article level instead of per store. Sometimes it also looks up the schema of every table to find the right one, using up a lot of tokens.</p>\n\n<p>I feel like I&#39;m moving in the right direction, but I wonder if there are maybe best practices I&#39;m missing, causing me to use to many tokens. Furthermore I hear a lot about people using techniques like DSPy, Knowledge Graph and fine tuning, but I&#39;m not sure whether these would offer (significant) benefits in my case.</p>\n\n<p>Any help/feedback on my approach would be much appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1eeifu5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AbbreviationsShot240'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eeifu5/genai_analytics_agent/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eeifu5/genai_analytics_agent/', 'subreddit_subscribers': 201126, 'created_utc': 1722202603.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.818+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello! I am currently a student in HS, and I have studied Python (numpy, pandas, matplotlib, seaborn), C++ (OOP, DSA), SQL (***ql), calculus, linear algebra, html, css, machine learning (a bit, took a course), apache hadoop, spark, kafka and nosql with mongodb, and some mini projects with Power BI. I know how to work with excel datasets with functions like vlookup, etc. I also have done assignments with Access in school, not sure how much further i can go with them but considering the never ending information in this world, i am sure there is so so so much more. \n\nI want to go down the data engineering path, but i know you either need 5+ years of experience or at least a masters degree to land a job in that domain (yes, I know technologies change in DE in the following years and no skills will remain the same in any industry).\n\n So I wanted to ask: would software development knowledge + experience help with landing data science jobs? I have noticed a trend where most data scientists on LinkedIn often either have a masters degree or have a lot of software engineering projects. Should I learn software development along with data science skills?', 'author_fullname': 't2_lclkn9hl6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Software development a plus or minus?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eed5ma', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.59, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722188771.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello! I am currently a student in HS, and I have studied Python (numpy, pandas, matplotlib, seaborn), C++ (OOP, DSA), SQL (***ql), calculus, linear algebra, html, css, machine learning (a bit, took a course), apache hadoop, spark, kafka and nosql with mongodb, and some mini projects with Power BI. I know how to work with excel datasets with functions like vlookup, etc. I also have done assignments with Access in school, not sure how much further i can go with them but considering the never ending information in this world, i am sure there is so so so much more. </p>\n\n<p>I want to go down the data engineering path, but i know you either need 5+ years of experience or at least a masters degree to land a job in that domain (yes, I know technologies change in DE in the following years and no skills will remain the same in any industry).</p>\n\n<p>So I wanted to ask: would software development knowledge + experience help with landing data science jobs? I have noticed a trend where most data scientists on LinkedIn often either have a masters degree or have a lot of software engineering projects. Should I learn software development along with data science skills?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1eed5ma', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ahyesthepirates'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eed5ma/software_development_a_plus_or_minus/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eed5ma/software_development_a_plus_or_minus/', 'subreddit_subscribers': 201126, 'created_utc': 1722188771.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.819+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi guys,\n\nWe want to migrate out of Singlestore due to the cost reason. What are some good alternatives which can provide MySQL driver (or close MySQL syntax)  to query the data?', 'author_fullname': 't2_77rljll2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What are some good alternative of Singlestore DB?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ee0jmi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722146027.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys,</p>\n\n<p>We want to migrate out of Singlestore due to the cost reason. What are some good alternatives which can provide MySQL driver (or close MySQL syntax)  to query the data?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ee0jmi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Specialist_Bird9619'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ee0jmi/what_are_some_good_alternative_of_singlestore_db/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ee0jmi/what_are_some_good_alternative_of_singlestore_db/', 'subreddit_subscribers': 201126, 'created_utc': 1722146027.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.819+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f4bf85ca970>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi guys\n\nWe are planning to migrant to Kafka for one of our data pipeline. We are planning to use Lambda but can you tell me what are other various tools/services which can be used as Kafka consumers?', 'author_fullname': 't2_77rljll2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What are various tools to be used as Kafka consumers?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ee0h0m', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1722145729.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys</p>\n\n<p>We are planning to migrant to Kafka for one of our data pipeline. We are planning to use Lambda but can you tell me what are other various tools/services which can be used as Kafka consumers?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ee0h0m', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Specialist_Bird9619'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ee0h0m/what_are_various_tools_to_be_used_as_kafka/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ee0h0m/what_are_various_tools_to_be_used_as_kafka/', 'subreddit_subscribers': 201126, 'created_utc': 1722145729.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-01T20:09:59.820+0000] {python.py:194} INFO - Done. Returned value was: None
[2024-08-01T20:09:59.831+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline2, task_id=reddit_extraction, execution_date=20240801T200954, start_date=20240801T200955, end_date=20240801T200959
[2024-08-01T20:09:59.904+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-08-01T20:09:59.924+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
